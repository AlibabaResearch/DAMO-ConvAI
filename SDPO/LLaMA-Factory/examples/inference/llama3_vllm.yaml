model_name_or_path: /Path-Meta-Llama-3.1-8B-Instruct 
template: llama3
infer_backend: vllm
vllm_enforce_eager: true
max_new_tokens: 4096
vllm_maxlen: 4096
flash_attn: fa2