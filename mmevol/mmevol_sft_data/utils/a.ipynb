{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多图round-i 数据提取和合并\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "generated_and_corrected_qa = \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/mosaic_imgs/data/multi_round_qa_v1_mosaic_imgs_3k/round_1/gen_qa_corrected\"\n",
    "\n",
    "mosaic_files = os.listdir(generated_and_corrected_qa)\n",
    "\n",
    "count = 0\n",
    "for mfs in tqdm(mosaic_files):\n",
    "    collection = {\n",
    "        \"top left\":    [],\n",
    "        \"top right\":   [],\n",
    "        \"lower left\":  [],\n",
    "        \"lower right\": []\n",
    "    }\n",
    "    try:\n",
    "        path = osp.join(generated_and_corrected_qa, mfs)\n",
    "        data = json.load(open(path, \"r\"))\n",
    "\n",
    "        for d in data:\n",
    "            collection[d[\"image_position\"]].append(d)\n",
    "        pos2id = json.load(open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/evolution/mosaic_imgs/data/meta_qa_det_cap/round1\", mfs), \"r\"))[\"id2loc\"]\n",
    "        \n",
    "        for pos, gen_qa in collection.items():\n",
    "            json.dump(gen_qa, open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-codeution/mosaic_imgs/data/multi_round_qa_v1_mosaic_imgs_3k/round_1/imgs_qa\", \"{}.json\".format(pos2id[pos])), \"w\"), indent=4)\n",
    "            count += 1\n",
    "    except:\n",
    "        continue\n",
    "print(\"{}/{} valid imgage qa\".format(count, len(mosaic_files)*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average improving ratio: 0.72, average score: 6.19, improved samples: 20654.0, total samples: 28654.0\n",
      "average improving ratio: 0.87, average score: 6.68, improved samples: 19789.0, total samples: 22790.0\n",
      "average improving ratio: 0.92, average score: 7.01, improved samples: 18873.0, total samples: 20527.0\n",
      "average improving ratio: 0.68, average score: 6.06, improved samples: 18110.0, total samples: 26647.0\n",
      "average improving ratio: 0.85, average score: 6.59, improved samples: 17117.0, total samples: 20219.0\n",
      "average improving ratio: 0.91, average score: 6.95, improved samples: 16399.0, total samples: 18075.0\n"
     ]
    }
   ],
   "source": [
    "# 演化前后打分\n",
    "import os.path as osp\n",
    "import os\n",
    "import json\n",
    "data_list = [\n",
    "             \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_single_imgs_6k/round1/score_gpt4_mini_corrected\",\n",
    "             \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_single_imgs_6k/round2/score_gpt4_mini_corrected\",\n",
    "             \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_single_imgs_6k/round3/score_gpt4_mini_corrected\",\n",
    "             \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_single_imgs_6k_mini/round1/score_gpt4_mini_corrected\",\n",
    "             \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_single_imgs_6k_mini/round2/score_gpt4_mini_corrected\",\n",
    "             \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_single_imgs_6k_mini/round3/score_gpt4_mini_corrected\",\n",
    "             ]\n",
    "# data_list = [\"/mnt/workspace/workgroup/haonan/evolution-code/evolution/mosaic_imgs/data/multi_round_qa_v1_mosaic_imgs_3k/round_1/score_corrected\"]\n",
    "for dl in data_list:\n",
    "    score_file = os.listdir(dl)\n",
    "    improved_num = 0.\n",
    "    total_number = 0.\n",
    "    total_score = 0.\n",
    "\n",
    "    for sf in score_file:\n",
    "        file = osp.join(dl, sf)\n",
    "        try:\n",
    "            # print(file)\n",
    "            data = json.load(open(file, \"r\"))\n",
    "            total_number += len(data)\n",
    "            for i in data:\n",
    "                total_score += i['score'] if i[\"improved\"] == \"yes\" else 0\n",
    "                improved_num += 1 if i[\"improved\"] == \"yes\" else 0\n",
    "        except:\n",
    "            continue\n",
    "    # print(improved_num, total_number, total_score)\n",
    "    print(\"average improving ratio: {:.2f}, average score: {:.2f}, improved samples: {}, total samples: {}\".format(improved_num / total_number, total_score / improved_num, improved_num, total_number))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'bf_in_breath': 1299, 'df_cot': 962, 'df_format': 546}, 2807)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个方向演化的样本数量\n",
    "import json\n",
    "import os.path as osp\n",
    "count = {\n",
    "    \"bf_in_breath\": 0,\n",
    "    \"df_cot\": 0, \n",
    "    \"df_format\": 0\n",
    "}\n",
    "g = 0\n",
    "evo = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/evo_path.json\", \"r\"))\n",
    "# print(len(evo)) # 484\n",
    "for k, v in evo.items():\n",
    "    file = json.load(open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_persona/round3/filtered_qa\", k + \".json\"), \"r\"))\n",
    "    g += len(file[\"conversations_v0\"])\n",
    "    for index, vv in enumerate(v):\n",
    "        count[vv] += len(file[\"conversations_v{}\".format(index+1)])\n",
    "count, sum(count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score 和 演化qa数量不一致统计\n",
    "import json\n",
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "a = os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_persona/round1/gen_qa_corrected\")\n",
    "b = os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_persona/round1/score_gpt4_mini_corrected\")\n",
    "print(len(a), len(b))\n",
    "c = 0\n",
    "n1 = 0\n",
    "n2 = 0\n",
    "v = 0\n",
    "for i in a:\n",
    "    try:\n",
    "        # g = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_persona/round1/gen_qa_corrected/\"+i, \"r\"))\n",
    "        y = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_persona/round1/score_gpt4_mini_corrected/\"+i, \"r\"))\n",
    "        # if len(g) != len(y):\n",
    "        #     c+=1\n",
    "        # n1 += len(g)\n",
    "        # n2 += len(y)\n",
    "    except:\n",
    "        v+=1\n",
    "        print(i)\n",
    "        continue\n",
    "n1, n2, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29032/29032 [00:01<00:00, 20360.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# coco_1k 数据重构\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "data = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_30k_addition_science.json\", \"r\"))\n",
    "\n",
    "# datasets_path = \"/mnt/workspace/workgroup/haonan/evolution-code/datasets\"\n",
    "# if os.path.exists(osp.join(datasets_path, \"meta_data\")):\n",
    "#     shutil.rmtree(osp.join(datasets_path, \"meta_data\"))\n",
    "#     os.mkdir(osp.join(datasets_path, \"meta_data\"))\n",
    "\n",
    "data_v2 = []\n",
    "for d in tqdm(data):\n",
    "    qa = []\n",
    "    for qa_index in range(len(d['conversations_v0'])//2):\n",
    "        qa.append(\n",
    "            {\n",
    "                \"image_position\": \"single\",\n",
    "                \"objects\": [],\n",
    "                \"skills\": [],\n",
    "                \"format\": \"Norm\",\n",
    "                \"question\": d['conversations_v0'][2*qa_index]['value'].replace(\"<image>\\n\", \"\").replace(\"\\n<image>\", \"\").strip(),\n",
    "                \"steps\": [],\n",
    "                \"answer\": d['conversations_v0'][2*qa_index+1]['value'].replace(\"<image>\\n\", \"\").replace(\"\\n<image>\", \"\").strip() # provided only for format\n",
    "            }\n",
    "        )\n",
    "\n",
    "    tmp = {\n",
    "        \"id\": d[\"id\"],\n",
    "        \"image\": d[\"image\"],\n",
    "        \"conversations_v0\": qa,\n",
    "        \"topic_name\": d[\"topic_name\"] if \"topic_name\" in d else [],\n",
    "        \"caption\": d[\"caption\"],\n",
    "        \"det\": d[\"det\"]\n",
    "\n",
    "    }\n",
    "    data_v2.append(tmp)\n",
    "\n",
    "\n",
    "json.dump(data_v2, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_30k_addition_science_v2.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29032/29032 [00:00<00:00, 1107152.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# 补充seed样本中缺失的部分\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "data = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_all_complementary.json\", \"r\"))\n",
    "\n",
    "c = 0\n",
    "id_list = []\n",
    "for d in tqdm(data):\n",
    "    if len(d[\"conversations_v0\"]) == 0:\n",
    "        c+=1\n",
    "        id_list.append(d[\"id\"])\n",
    "        try:\n",
    "            qa = json.load(open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/gen_qa_single_53k/ini_qa_v1_single_img_53k_corrected\", \"{}.json\".format(d[\"id\"]))))\n",
    "            lll = []\n",
    "            for s in qa:\n",
    "                tmp = {\n",
    "                    \"image_position\": \"single\",\n",
    "                    \"objects\": s[\"objects\"],\n",
    "                    \"skills\": s[\"skills\"],\n",
    "                    \"format\": \"Norm\",\n",
    "                    \"question\": s[\"question\"],\n",
    "                    \"steps\": s[\"steps\"],\n",
    "                    \"answer\": s[\"answer\"]\n",
    "                }\n",
    "                lll.append(tmp)\n",
    "            d[\"conversations_v0\"] = lll\n",
    "            c+=1\n",
    "        except:\n",
    "            continue\n",
    "id_list\n",
    "# json.dump(data, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_all_complementary.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造hash_id建立唯一索引，因为id和image键值都存在重复值\n",
    "import json\n",
    "\n",
    "a = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_gpt4o_evo_add_60-50-10k_2.json\", \"r\"))\n",
    "for index, i in enumerate(a):\n",
    "    # i[\"hash_id\"] = str(index) + \"_\" + i[\"id\"] + \"_\" + i[\"image\"].replace(\"/\", \"_\")\n",
    "    i[\"hash_id\"] = str(index) + \"_\" + i[\"image\"].replace(\"/\", \"_\")\n",
    "\n",
    "json.dump(a, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_gpt4o_evo_add_60-50-10k_2.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "d = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_gpt4o_evo_add_60-50k.json\", \"r\"))\n",
    "json.dump(d[10000:20000], open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_gpt4o_evo_add_60-50-10k_2.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:43<00:00, 96.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# 如果数据格式已经组织完善，直接分开存到meta data里\n",
    "import os\n",
    "import json\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "datasets_path = \"/mnt/workspace/workgroup/haonan/evolution-code/datasets\"\n",
    "if os.path.exists(osp.join(datasets_path, \"meta_data_gpt4o_10k\")):\n",
    "    shutil.rmtree(osp.join(datasets_path, \"meta_data_gpt4o_10k\"))\n",
    "    os.mkdir(osp.join(datasets_path, \"meta_data_gpt4o_10k\"))\n",
    "\n",
    "data = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_gpt4o_evo_add_60-50-10k.json\", \"r\"))\n",
    "\n",
    "for index, d in enumerate(tqdm(data)):\n",
    "    json.dump(d, open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data_gpt4o_10k\", \"{}.json\".format(d[\"hash_id\"])), \"w\"), indent=4)\n",
    "    # json.dump(d, open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data\", \"{}.json\".format(d[\"id\"])), \"w\"), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29032\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(len(os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data_additional_data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 额外的图表数据id会重复，找出重复id的样本重新生成\n",
    "print(len(re))\n",
    "ss = []\n",
    "for k, v in re.items():\n",
    "    if v > 1:\n",
    "        ss.append(k)\n",
    "ss\n",
    "# json.dump(ss, open(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/gen_objects_single_repeated_id_science_2k/id.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27227\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(len(os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data_additional_data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 977/977 [00:08<00:00, 110.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 of 977 is evolved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 合并conversation_v1\n",
    "import os \n",
    "import os.path as osp\n",
    "import shutil\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "meta_data = \"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data\"\n",
    "conversation_v1_path = \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_ini_prompt/round1\"\n",
    "# conversation_v1_files = os.listdir(osp.join(conversation_v1_path, \"gen_qa_corrected\"))\n",
    "meta_data_files = os.listdir(meta_data)\n",
    "\n",
    "if os.path.exists(osp.join(conversation_v1_path, \"filtered_qa\")):\n",
    "    shutil.rmtree(osp.join(conversation_v1_path, \"filtered_qa\"))\n",
    "    os.mkdir(osp.join(conversation_v1_path, \"filtered_qa\"))\n",
    "\n",
    "count = 0\n",
    "for mdf in tqdm(meta_data_files):\n",
    "\n",
    "    meta_sample = json.load(open(osp.join(meta_data, mdf), \"r\"))\n",
    "    qa_path = osp.join(conversation_v1_path, \"gen_qa_corrected\", mdf)\n",
    "    score_path = osp.join(conversation_v1_path, \"score_gpt4_mini_corrected\", mdf)\n",
    "    \n",
    "    try:\n",
    "        data = json.load(open(qa_path, \"r\"))\n",
    "        score = json.load(open(score_path, \"r\"))\n",
    "\n",
    "        temp = []\n",
    "        for index, i in enumerate(data):\n",
    "            if score[index]['improved'] == \"yes\":\n",
    "                i[\"score\"] =  score[index]['score']\n",
    "                temp.append(i)\n",
    "\n",
    "        d = {\n",
    "            \"id\": meta_sample[\"id\"],\n",
    "            \"image\": meta_sample[\"image\"],\n",
    "            \"conversations_v0\": meta_sample[\"conversations_v0\"],\n",
    "            \"conversations_v1\": temp,\n",
    "            \"topic_name\": meta_sample[\"topic_name\"] if \"topic_name\" in meta_sample else [],\n",
    "            \"caption\": meta_sample[\"caption\"],\n",
    "            \"det\": meta_sample[\"det\"]\n",
    "        }\n",
    "        json.dump(d, open(osp.join(conversation_v1_path, \"filtered_qa\", \"{}.json\".format(meta_sample['id'])), \"w\"), indent=4)\n",
    "        count += 1\n",
    "    except:\n",
    "        d = {\n",
    "            \"id\": meta_sample[\"id\"],\n",
    "            \"image\": meta_sample[\"image\"],\n",
    "            \"conversations_v0\": meta_sample[\"conversations_v0\"],\n",
    "            \"conversations_v1\": [],\n",
    "            \"topic_name\": meta_sample[\"topic_name\"] if \"topic_name\" in meta_sample else [],\n",
    "            \"caption\": meta_sample[\"caption\"],\n",
    "            \"det\": meta_sample[\"det\"]\n",
    "        }\n",
    "        json.dump(d, open(osp.join(conversation_v1_path, \"filtered_qa\", \"{}.json\".format(meta_sample['id'])), \"w\"), indent=4)\n",
    "print(\"{} of {} is evolved\".format(count, len(meta_data_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 977/977 [00:07<00:00, 125.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974 of 977 is evolved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 合并conversation_v2\n",
    "import os \n",
    "import os.path as osp\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "meta_data = \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_ini_prompt/round1/filtered_qa\"\n",
    "conversation_v2_path = \"/mnt/workspace/workgroup/haonan/evolution-code/single_imgs/multi_round_v1_single_imgs_ini_prompt/round2\"\n",
    "# conversation_v1_files = os.listdir(osp.join(conversation_v1_path, \"gen_qa_corrected\"))\n",
    "meta_data_files = os.listdir(meta_data)\n",
    "\n",
    "if os.path.exists(osp.join(conversation_v2_path, \"filtered_qa\")):\n",
    "    shutil.rmtree(osp.join(conversation_v2_path, \"filtered_qa\"))\n",
    "    os.mkdir(osp.join(conversation_v2_path, \"filtered_qa\"))\n",
    "\n",
    "count = 0\n",
    "for mdf in tqdm(meta_data_files):\n",
    "\n",
    "    meta_sample = json.load(open(osp.join(meta_data, mdf), \"r\"))\n",
    "    qa_path = osp.join(conversation_v2_path, \"gen_qa_corrected\", mdf)\n",
    "    score_path = osp.join(conversation_v2_path, \"score_gpt4_mini_corrected\", mdf)\n",
    "    \n",
    "    try:\n",
    "        data = json.load(open(qa_path, \"r\"))\n",
    "        score = json.load(open(score_path, \"r\"))\n",
    "        temp = []\n",
    "        for index, i in enumerate(data):\n",
    "            if score[index]['improved'] == \"yes\":\n",
    "                i[\"score\"] =  score[index]['score']\n",
    "                temp.append(i)\n",
    "\n",
    "        d = {\n",
    "            \"id\": meta_sample[\"id\"],\n",
    "            \"image\": meta_sample[\"image\"],\n",
    "            \"conversations_v0\": meta_sample[\"conversations_v0\"],\n",
    "            \"conversations_v1\": meta_sample[\"conversations_v1\"],\n",
    "            \"conversations_v2\": temp,\n",
    "            \"topic_name\": meta_sample[\"topic_name\"] if \"topic_name\" in meta_sample else [],\n",
    "            \"caption\": meta_sample[\"caption\"],\n",
    "            \"det\": meta_sample[\"det\"]\n",
    "        }\n",
    "        json.dump(d, open(osp.join(conversation_v2_path, \"filtered_qa\", \"{}.json\".format(meta_sample['id'])), \"w\"), indent=4)\n",
    "        count += 1\n",
    "    except:\n",
    "        d = {\n",
    "            \"id\": meta_sample[\"id\"],\n",
    "            \"image\": meta_sample[\"image\"],\n",
    "            \"conversations_v0\": meta_sample[\"conversations_v0\"],\n",
    "            \"conversations_v1\": meta_sample[\"conversations_v1\"],\n",
    "            \"conversations_v2\": [],\n",
    "            \"topic_name\": meta_sample[\"topic_name\"] if \"topic_name\" in meta_sample else [],\n",
    "            \"caption\": meta_sample[\"caption\"],\n",
    "            \"det\": meta_sample[\"det\"]\n",
    "        }\n",
    "        json.dump(d, open(osp.join(conversation_v2_path, \"filtered_qa\", \"{}.json\".format(meta_sample['id'])), \"w\"), indent=4)\n",
    "print(\"{} of {} is evolved\".format(count, len(meta_data_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/977 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 977/977 [00:08<00:00, 118.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976 of 977 is evolved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 合并conversation_v3\n",
    "import os \n",
    "import os.path as osp\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "meta_data = \"/mnt/workspace/workgroup/haonan/evolution-codeution/single_imgs/multi_round_v1_single_imgs_ini_prompt/round2/filtered_qa\"\n",
    "conversation_v3_path = \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_ini_prompt/round3\"\n",
    "# conversation_v1_files = os.listdir(osp.join(conversation_v1_path, \"gen_qa_corrected\"))\n",
    "meta_data_files = os.listdir(meta_data)\n",
    "\n",
    "if os.path.exists(osp.join(conversation_v3_path, \"filtered_qa\")):\n",
    "    shutil.rmtree(osp.join(conversation_v3_path, \"filtered_qa\"))\n",
    "    os.mkdir(osp.join(conversation_v3_path, \"filtered_qa\"))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for mdf in tqdm(meta_data_files):\n",
    "\n",
    "    meta_sample = json.load(open(osp.join(meta_data, mdf), \"r\"))\n",
    "    qa_path = osp.join(conversation_v3_path, \"gen_qa_corrected\", mdf)\n",
    "    score_path = osp.join(conversation_v3_path, \"score_gpt4_mini_corrected\", mdf)\n",
    "    \n",
    "    try:\n",
    "        data = json.load(open(qa_path, \"r\"))\n",
    "        score = json.load(open(score_path, \"r\"))\n",
    "        temp = []\n",
    "        for index, i in enumerate(data):\n",
    "            if score[index]['improved'] == \"yes\":\n",
    "                i[\"score\"] =  score[index]['score']\n",
    "                temp.append(i)\n",
    "\n",
    "        d = {\n",
    "            \"id\": meta_sample[\"id\"],\n",
    "            \"image\": meta_sample[\"image\"],\n",
    "            \"conversations_v0\": meta_sample[\"conversations_v0\"],\n",
    "            \"conversations_v1\": meta_sample[\"conversations_v1\"],\n",
    "            \"conversations_v2\": meta_sample[\"conversations_v2\"],\n",
    "            \"conversations_v3\": temp,\n",
    "            \"topic_name\": meta_sample[\"topic_name\"] if \"topic_name\" in meta_sample else [],\n",
    "            \"caption\": meta_sample[\"caption\"],\n",
    "            \"det\": meta_sample[\"det\"]\n",
    "        }\n",
    "        json.dump(d, open(osp.join(conversation_v3_path, \"filtered_qa\", \"{}.json\".format(meta_sample['id'])), \"w\"), indent=4)\n",
    "        count += 1\n",
    "    except:\n",
    "        d = {\n",
    "            \"id\": meta_sample[\"id\"],\n",
    "            \"image\": meta_sample[\"image\"],\n",
    "            \"conversations_v0\": meta_sample[\"conversations_v0\"],\n",
    "            \"conversations_v1\": meta_sample[\"conversations_v1\"],\n",
    "            \"conversations_v2\": meta_sample[\"conversations_v2\"],\n",
    "            \"conversations_v3\": [],\n",
    "            \"topic_name\": meta_sample[\"topic_name\"] if \"topic_name\" in meta_sample else [],\n",
    "            \"caption\": meta_sample[\"caption\"],\n",
    "            \"det\": meta_sample[\"det\"]\n",
    "        }\n",
    "        json.dump(d, open(osp.join(conversation_v3_path, \"filtered_qa\", \"{}.json\".format(meta_sample['id'])), \"w\"), indent=4)\n",
    "print(\"{} of {} is evolved\".format(count, len(meta_data_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 557/977 [00:00<00:00, 1363.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 977/977 [00:00<00:00, 1257.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# 合并成训练文件\n",
    "import os.path as osp\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_ini_prompt/round3/filtered_qa\"\n",
    "data_file_list = os.listdir(data_path)\n",
    "\n",
    "merged_data = []\n",
    "for data_file in tqdm(data_file_list):\n",
    "    data_file_path = osp.join(data_path, data_file)\n",
    "    data = json.load(open(data_file_path, \"r\"))\n",
    "    merged_data.append(data)\n",
    "\n",
    "json.dump(merged_data, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/coco_instruct_ablation_topic_25_1k_v2_3round_evo_ini_prompt.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 演化路径合并\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "root_path = \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_persona/round{}\"\n",
    "\n",
    "files = os.listdir(osp.join(root_path.format(\"1\"), \"evo_path\"))\n",
    "print(len(files))\n",
    "data = {}\n",
    "\n",
    "for f in files:\n",
    "    conv1 = json.load(open(osp.join(root_path.format(\"1\"), \"evo_path\", f), \"r\"))\n",
    "    conv2 = json.load(open(osp.join(root_path.format(\"2\"), \"evo_path\", f), \"r\"))\n",
    "    conv3 = json.load(open(osp.join(root_path.format(\"3\"), \"evo_path\", f), \"r\"))\n",
    "\n",
    "    data[f.replace(\".json\", \"\")] = [conv1, conv2, conv3]  \n",
    "json.dump(data, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/evo_path.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6486/6486 [00:21<00:00, 304.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# 打分情况合并\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "\n",
    "root_path = \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_single_imgs_6k_mini/round{}\"\n",
    "\n",
    "files = os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data_6k\")\n",
    "\n",
    "data = {}\n",
    "\n",
    "for f in tqdm(files):\n",
    "\n",
    "    conv0 = json.load(open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data_6k\", f), \"r\"))\n",
    "    try:\n",
    "        conv1 = json.load(open(osp.join(root_path.format(\"1\"), \"score_gpt4_mini_corrected\", f), \"r\"))\n",
    "    except:\n",
    "        conv1 = []\n",
    "    \n",
    "    try:\n",
    "        conv2 = json.load(open(osp.join(root_path.format(\"2\"), \"score_gpt4_mini_corrected\", f), \"r\"))\n",
    "    except:\n",
    "        conv2 = []\n",
    "    \n",
    "    try:\n",
    "        conv3 = json.load(open(osp.join(root_path.format(\"3\"), \"score_gpt4_mini_corrected\", f), \"r\"))\n",
    "    except:\n",
    "        conv3 = []\n",
    "\n",
    "    data[f.replace(\".json\", \"\")] = {\n",
    "        \"conversation_v0\": conv0,\n",
    "        \"conversation_v1\": conv1,\n",
    "        \"conversation_v2\": conv2,\n",
    "        \"conversation_v3\": conv3,\n",
    "    }\n",
    "\n",
    "json.dump(data, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/improv_score_reason_6k_mini.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persona数据topk\n",
    "import json\n",
    "import os.path as osp\n",
    "import os\n",
    "import jsonlines\n",
    "\n",
    "persona_data = []\n",
    "with open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/persona.jsonl\", \"r\") as file:\n",
    "    for d in jsonlines.Reader(file):\n",
    "        persona_data.append(d['persona'])\n",
    "    \n",
    "len(persona_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1612"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "query_list = []\n",
    "data = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/coco_instruct_ablation_topic_25_1k_add_v2.json\", \"r\"))\n",
    "for d in data:\n",
    "    if len(d['caption']) != 0:\n",
    "        query_list.append(d['caption'][1][\"value\"])\n",
    "    else:\n",
    "        sen = ''\n",
    "        for index in range(len(d['conversations_v0'])):\n",
    "            if index > 5: break\n",
    "            sen += d['conversations_v0'][index]['question'] + \" \" + d['conversations_v0'][index]['answer'] + \" \"\n",
    "        query_list.append(sen)\n",
    "\n",
    "len(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import os.path as osp\n",
    "import json\n",
    "\n",
    "files = os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data\")\n",
    "dd = {}\n",
    "data = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/coco_instruct_ablation_topic_25_1k_add_v2.json\", \"r\"))\n",
    "\n",
    "for index, d in enumerate(data):\n",
    "    dd[d[\"id\"]] = {\n",
    "        \"caption\": d[\"caption\"],\n",
    "        \"persona\": [persona_data[i] for i in indice[index]]\n",
    "    }\n",
    "\n",
    "json.dump(dd, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/persona_top50.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量rename\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "path = '/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/gen_qa_single_53k/ini_qa_v1_single_img_53k'\n",
    "files = os.listdir(path)\n",
    "for i, file in tqdm(enumerate(files)):\n",
    "    NewName = os.path.join(path, file.replace(\"gen_qa_\", \"\"))\n",
    "    OldName = os.path.join(path, file)\n",
    "    # print(NewName, OldName)\n",
    "    os.rename(OldName, NewName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 977/977 [00:04<00:00, 210.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# 合并1k的非采样数据\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "\n",
    "meta_data = \"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data\"\n",
    "gen_data = [\n",
    "    \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/cot_qa_v1_single_imgs_1k/round1\",\n",
    "    \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/format_qa_v1_single_imgs_1k/round1\",\n",
    "    \"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/in_breath_qa_v1_single_imgs_1k/round1\"\n",
    "]\n",
    "merged_data = []\n",
    "# conversation_v1_files = os.listdir(osp.join(conversation_v1_path, \"gen_qa_corrected\"))\n",
    "meta_data_files = os.listdir(meta_data)\n",
    "\n",
    "count = 0\n",
    "for mdf in tqdm(meta_data_files):\n",
    "    meta_sample = json.load(open(osp.join(meta_data, mdf), \"r\"))\n",
    "    temp = []\n",
    "    for gen_qa_path in gen_data:\n",
    "        qa_path = osp.join(gen_qa_path, \"gen_qa_corrected\", mdf)\n",
    "        score_path = osp.join(gen_qa_path, \"score_gpt4_mini_corrected\", mdf)\n",
    "        # print(mdf)\n",
    "        try:\n",
    "            data = json.load(open(qa_path, \"r\"))\n",
    "            score = json.load(open(score_path, \"r\"))\n",
    "            for index, i in enumerate(data):\n",
    "                if score[index]['improved'] == \"yes\":\n",
    "                    i[\"score\"] =  score[index]['score']\n",
    "                    temp.append(i)\n",
    "        except:\n",
    "            continue\n",
    "    d = {\n",
    "        \"id\": meta_sample[\"id\"],\n",
    "        \"image\": meta_sample[\"image\"],\n",
    "        \"conversations_v0\": meta_sample[\"conversations_v0\"],\n",
    "        \"conversations_v1\": temp,\n",
    "        \"topic_name\": meta_sample[\"topic_name\"] if \"topic_name\" in meta_sample else [],\n",
    "        \"caption\": meta_sample[\"caption\"],\n",
    "        \"det\": meta_sample[\"det\"]\n",
    "    }\n",
    "    merged_data.append(d)\n",
    "\n",
    "json.dump(merged_data, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/coco_instruct_ablation_topic_25_1k_parallel_3directions_evo_mini_score.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27164 16576 11149\n",
      "369362 361735 349303\n"
     ]
    }
   ],
   "source": [
    "# 检查objects/skills/steps为空的比例\n",
    "import json\n",
    "data = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_all_136k_mini.json\", \"r\"))\n",
    "\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "\n",
    "total1 = 0\n",
    "total2 = 0\n",
    "total3 = 0\n",
    "\n",
    "for i in data:\n",
    "    for j in i[\"conversations_v1\"]:\n",
    "        if len(j[\"objects\"]) == 0:\n",
    "            count1 += 1\n",
    "    for j in i[\"conversations_v2\"]:\n",
    "        if len(j[\"objects\"]) == 0:\n",
    "            count2 += 1\n",
    "    for j in i[\"conversations_v3\"]:\n",
    "        if len(j[\"objects\"]) == 0:\n",
    "            count3 += 1\n",
    "\n",
    "    total1 += len(i[\"conversations_v1\"])\n",
    "    total2 += len(i[\"conversations_v2\"])\n",
    "    total3 += len(i[\"conversations_v3\"])\n",
    "\n",
    "print(count1, count2, count3)\n",
    "print(total1, total2, total3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29032\n",
      "178554\n",
      "178554\n"
     ]
    }
   ],
   "source": [
    "# 统计qa对中objects/skills/steps缺失的比例\n",
    "import json\n",
    "seed_all_com = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_30k_addition_science_v2.json\", \"r\"))\n",
    "print(len(seed_all_com))\n",
    "count0 = 0\n",
    "total0 = 0\n",
    "for i in seed_all_com:\n",
    "    for j in i[\"conversations_v0\"]:\n",
    "        if len(j[\"objects\"]) == 0:\n",
    "            count0 += 1\n",
    "\n",
    "    total0 += len(i[\"conversations_v0\"])\n",
    "\n",
    "print(count0)\n",
    "print(total0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29032 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29032/29032 [00:56<00:00, 512.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2775"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把生成的objects/skills/steps按照question是否一致进行填充\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "map_ = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/gen_objects_single_repeated_id_science_2k/id.json\", \"r\"))\n",
    "\n",
    "c = 0\n",
    "for i in tqdm(seed_all_com):\n",
    "    # if len(i[\"conversations_v0\"][0][\"objects\"]) != 0:\n",
    "    #     continue\n",
    "    file_id = i[\"id\"]\n",
    "    if file_id in map_:\n",
    "        file_id = i[\"image\"].replace(\"/\", \"_\")\n",
    "        try:\n",
    "            gen_data = json.load(open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/gen_objects_single_repeated_id_science_2k/ini_object_v1_single_img_2k_corrected\", file_id + \".json\"), \"r\"))\n",
    "        except:\n",
    "            c+=1\n",
    "            continue\n",
    "        q_map = {}\n",
    "        for h in gen_data:\n",
    "            try:\n",
    "                q_map[h[\"question\"].lower().strip()] = h\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for x in i[\"conversations_v0\"]:\n",
    "            if len(x[\"objects\"]) != 0:\n",
    "                continue\n",
    "            if x[\"question\"].lower().strip() in q_map:\n",
    "                q = x[\"question\"].lower().strip()\n",
    "                x[\"objects\"] = q_map[q][\"objects\"]\n",
    "                x[\"skills\"] = q_map[q][\"skills\"]\n",
    "                x[\"steps\"] = q_map[q][\"steps\"]\n",
    "    else:\n",
    "        try:\n",
    "            gen_data = json.load(open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/gen_objects_single_153k/ini_object_v1_single_img_153k_corrected\", file_id + \".json\"), \"r\"))\n",
    "        except:\n",
    "            c+=1\n",
    "            continue\n",
    "        q_map = {}\n",
    "        for h in gen_data:\n",
    "            try:\n",
    "                q_map[h[\"question\"].lower().strip()] = h\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        for x in i[\"conversations_v0\"]:\n",
    "            if len(x[\"objects\"]) != 0:\n",
    "                continue\n",
    "            if x[\"question\"].lower().strip() in q_map:\n",
    "                q = x[\"question\"].lower().strip()\n",
    "                x[\"objects\"] = q_map[q][\"objects\"]\n",
    "                x[\"skills\"] = q_map[q][\"skills\"]\n",
    "                x[\"steps\"] = q_map[q][\"steps\"]\n",
    "c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(seed_all_com, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_30k_addition_science_v2_add_obj_v3.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check和filter dataengine的脏图表数据\n",
    "import json\n",
    "import os \n",
    "from tqdm import tqdm\n",
    "paths = os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/dataengine_match_161k/gen_yes_no_id_more_than_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98197/98197 [04:02<00:00, 405.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2097314581911871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "yes = 0\n",
    "no = 0\n",
    "for p in tqdm(paths):\n",
    "    with open(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/dataengine_match_161k/gen_yes_no_id_more_than_one/\" + p, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()    \n",
    "    if content.lower().strip() == \"yes\":\n",
    "        yes += 1\n",
    "    elif content.lower().strip() == \"no\":\n",
    "        no+=1\n",
    "\n",
    "print(yes/len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97049\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "data = {}\n",
    "c = 0\n",
    "ss = []\n",
    "with open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/dataengine_161k.jsonl\", \"r\") as file:\n",
    "    for item in jsonlines.Reader(file):\n",
    "        ss.append(item[\"id\"])\n",
    "        if item[\"id\"] in data:\n",
    "            data[item[\"id\"]] += 1\n",
    "        else:\n",
    "            data[item[\"id\"]] = 1\n",
    "\n",
    "print(len(list(set(ss))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101402"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_list = 0\n",
    "for k, v in data.items():\n",
    "    if v > 1:\n",
    "        map_list+=v\n",
    "map_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data_engine_2', 'data_engine_3', 'data_engine_4', 'data_engine_5', 'data_engine_6']\n"
     ]
    }
   ],
   "source": [
    "print(map_list[:5])\n",
    "import json\n",
    "json.dump(map_list, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/dataengine_id_list_more_than_one.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98197/98197 [02:04<00:00, 786.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20839"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import jsonlines\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_path = \"/mnt/workspace/workgroup/haonan/evolution-code/datasets/dataengine_161k.jsonl\"\n",
    "\n",
    "map_list = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/dataengine_id_list_more_than_one.json\", \"r\"))\n",
    "\n",
    "filted_data = []\n",
    "data = []\n",
    "with open(data_path, \"r\") as file:\n",
    "    for item in jsonlines.Reader(file):\n",
    "        data.append(item)\n",
    "\n",
    "repeated_data = os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/dataengine_match_161k/gen_yes_no_id_more_than_one\")\n",
    "\n",
    "for rd in tqdm(repeated_data):\n",
    "    with open(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/dataengine_match_161k/gen_yes_no_id_more_than_one/\" + rd, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()    \n",
    "    if content.lower().strip() == \"yes\":\n",
    "        filted_data.append(data[int(rd.replace(\".json\", \"\"))])\n",
    "\n",
    "with open(data_path, \"r\") as file:\n",
    "    for item in jsonlines.Reader(file):\n",
    "        if item['id'] not in map_list:\n",
    "            try:\n",
    "                with open(\"/mnt/workspace/workgroup/haonan/evolution-code/initial_stage/dataengine_match_161k/gen_yes_no/\" + item['id'] + \".json\", 'r', encoding='utf-8') as file:\n",
    "                    content = file.read() \n",
    "                    if content.lower().strip() == \"yes\":\n",
    "                        filted_data.append(item)\n",
    "            except:\n",
    "                continue\n",
    "len(filted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26794/26794 [00:00<00:00, 318382.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# 挑可视化的样本\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "d = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_30k_add_mini.json\", \"r\"))\n",
    "sss = []\n",
    "for i in tqdm(d):\n",
    "    if len(i[\"conversations_v1\"]) != 0 and len(i[\"conversations_v2\"]) != 0 and len(i[\"conversations_v3\"]) != 0:\n",
    "        if i[\"conversations_v0\"][0][\"format\"] == \"Norm\" and i[\"conversations_v1\"][0][\"format\"] == \"Norm\" and i[\"conversations_v2\"][0][\"format\"] != \"Norm\" and i[\"conversations_v2\"][0][\"format\"] != \"complex reasoning\" and i[\"conversations_v3\"][0][\"format\"] == \"complex reasoning\": \n",
    "            _ = i.pop(\"caption\")\n",
    "            _ = i.pop(\"det\")\n",
    "            _ = i.pop(\"topic_name\")\n",
    "\n",
    "            sss.append(i)\n",
    "\n",
    "json.dump(sss, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/sample_selection.json\", \"w\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13399 16277 26794 13399 66704\n"
     ]
    }
   ],
   "source": [
    "# 合并最终全量数据\n",
    "import json\n",
    "\n",
    "a = json.load(open(\"/mnt/workspace/workgroup/lr/data_haonan/seed_data_15k_add_mini.json\", \"r\"))\n",
    "b = json.load(open(\"/mnt/workspace/workgroup/lr/data_haonan/seed_data_15k_mini.json\", \"r\"))\n",
    "c = json.load(open(\"/mnt/workspace/workgroup/lr/data_haonan/seed_data_30k_add_mini.json\", \"r\"))\n",
    "d = json.load(open(\"/mnt/workspace/workgroup/lr/data_haonan/seed_data_75-15_add_mini.json\", \"r\"))\n",
    "e = json.load(open(\"/mnt/workspace/workgroup/lr/data_haonan/seed_data_all_add_75k_mini.json\", \"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136573\n"
     ]
    }
   ],
   "source": [
    "print(len(a)+len(b)+len(c)+len(d)+len(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9992\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(len(os.listdir(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/meta_data\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136573\n"
     ]
    }
   ],
   "source": [
    "f = a+b+c+d+e\n",
    "print(len(f))\n",
    "json.dump(f, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_all_136k_mini.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29032 136573\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dd = json.load(open(\"/mnt/workspace/workgroup/lr/data_haonan/seed_data_30k_addition_science_v2_add_obj_v3_mini.json\", \"r\"))\n",
    "ff = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_all_136k_mini.json\", \"r\"))\n",
    "print(len(dd), len(ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165605\n"
     ]
    }
   ],
   "source": [
    "aa = dd + ff\n",
    "print(len(aa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(aa, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_all_165k_mini.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "index = range(len(aa))\n",
    "fff = np.random.choice(index, 50000, replace=False)\n",
    "hhh = [aa[i] for i in fff]\n",
    "print(len(hhh))\n",
    "json.dump(hhh, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_all_165k_mini_50k_for_chart_vis.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path as osp\n",
    "import os\n",
    "\n",
    "data = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/improv_score_reason_old.json\", \"r\"))\n",
    "\n",
    "for k, v in data.items():\n",
    "    # try:\n",
    "    v[\"conversation_v0\"][\"score\"] = json.load(open(osp.join(\"/mnt/workspace/workgroup/haonan/evolution-code/evolution/single_imgs/multi_round_v1_single_imgs_ini_prompt_v0_score/round0/score_gpt4_mini_corrected\", k+\".json\"), \"r\"))\n",
    "    # except:\n",
    "    #     continue\n",
    "\n",
    "json.dump(data, open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/improv_score_reason_old_v0_score.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.load(open(\"/mnt/workspace/workgroup/haonan/evolution-code/datasets/seed_data_all_136k_mini.json\", \"r\"))\n",
    "\n",
    "json.dump(data[:1000], open(\"/mnt/workspace/workgroup/haonan/evolution-code/case_1k.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
