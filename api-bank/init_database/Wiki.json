{
    "artificial intelligence": {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "summary": "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to intelligence of humans and other animals. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs.\nAI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go).As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect.",
        "content": "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to intelligence of humans and other animals. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs.\nAI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go).As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.Artificial intelligence was founded as an academic discipline in 1956, and in the years since it has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success, and renewed funding. AI research has tried and discarded many different approaches, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge, and imitating animal behavior. In the first decades of the 21st century, highly mathematical and statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability, and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.\nThe field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\". This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals. The term artificial intelligence has also been criticized for overhyping AI's true technological capabilities.\n\n\n== History ==\n\nArtificial beings with intelligence appeared as storytelling devices in antiquity, and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis. This, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete \"artificial neurons\".By the 1950s, two visions for how to achieve machine intelligence emerged. One vision, known as Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the \"heuristic search\" approach, which likened intelligence to a problem of exploring a space of possibilities for answers.\nThe second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach, most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons. James Manyika and others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of Descartes, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural networks were pushed to the background but have gained new prominence in recent decades.The field of AI research was born at a workshop at Dartmouth College in 1956. The attendees became the founders and leaders of AI research. They and their students produced programs that the press described as \"astonishing\": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense and laboratories had been established around the world.Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field. Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Marvin Minsky agreed, writing, \"within a generation ... the problem of creating 'artificial intelligence' will substantially be solved\".They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.Interest in neural networks and \"connectionism\" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s. Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.\nAI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012. According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. He attributed this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.In a 2017 survey, one in five companies reported they had \"incorporated AI in some offerings or processes\". The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\n\n\n== Goals ==\nThe general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.\n\n\n=== Reasoning, problem-solving ===\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a \"combinatorial explosion\": they became exponentially slower as the problems grew larger. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.\n\n\n=== Knowledge representation ===\n\nKnowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts.\nA representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in description logic, such as the Web Ontology Language.AI research has developed tools to represent specific domains, such as objects, properties, categories and relations between objects; situations, events, states and time; causes and effects; knowledge about knowledge (what we know about what other people know);. default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); as well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas.\n\n\n=== Learning ===\n\nMachine learning (ML), a fundamental concept of AI research since the field's inception, is the study of computer algorithms that improve automatically through experience.Unsupervised learning finds patterns in a stream of input.\nSupervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in – the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as \"function approximators\" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, \"spam\" or \"not spam\".In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.Transfer learning is when the knowledge gained from one problem is applied to a new problem.Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\n\n\n=== Natural language processing ===\n\nNatural language processing (NLP)\nallows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.\nSymbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic and the breadth of commonsense knowledge. Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), \"Keyword spotting\" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others. They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.\n\n\n=== Perception ===\n\nMachine perception\nis the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,facial recognition, and object recognition.\nComputer vision is the ability to analyze visual input.\n\n\n=== Social intelligence ===\n\nAffective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process or simulate human feeling, emotion and mood.\nFor example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\nHowever, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.\n\n\n=== General intelligence ===\n\nA machine with general intelligence can solve a wide variety of problems with breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, \"master algorithm\" that could lead to AGI.\nOthers believe that anthropomorphic features like an artificial brain\nor simulated child development\nwill someday reach a critical point where general intelligence emerges.\n\n\n== Tools ==\n\n\n=== Search and optimization ===\n\nAI can solve many problems by intelligently searching through many possible solutions. Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.Simple exhaustive searches\nare rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies, heuristics can also serve to eliminate some choices unlikely to lead to a goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies.\nHeuristics limit the search for solutions into a smaller sample size.\n\nA very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other related optimization algorithms include random optimization, beam search and metaheuristics like simulated annealing. Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming. Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\n\n\n=== Logic ===\n\nLogic\nis used for knowledge representation and problem-solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning\nand inductive logic programming is a method for learning.Several different forms of logic are used in AI research. Propositional logic involves truth functions such as \"or\" and \"not\". First-order logic\nadds quantifiers and predicates and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a \"degree of truth\" (between 0 and 1) to vague statements such as \"Alice is old\" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.\nSeveral extensions of logic have been designed to handle specific domains of knowledge, such as description logics;situation calculus, event calculus and fluent calculus (for representing events and time);causal calculus;belief calculus (belief revision); and modal logics.\nLogics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.\n\n\n=== Probabilistic methods for uncertain reasoning ===\n\nMany problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.Bayesian networks\nare a very general tool that can be used for various problems, including reasoning (using the Bayesian inference algorithm),learning (using the expectation-maximization algorithm),planning (using decision networks) and perception (using dynamic Bayesian networks).\nProbabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).A key concept from the science of economics is \"utility\", a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,\nand information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\n\n\n=== Classifiers and statistical learning methods ===\n\nThe simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if diamond then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine the closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.A classifier can be trained in various ways; there are many statistical and machine learning approaches.\nThe decision tree is the simplest and most widely used symbolic machine learning algorithm.K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.Neural networks are also used for classification.Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as \"naive Bayes\" on most practical data sets.\n\n\n=== Artificial neural networks ===\n\nNeural networks\nwere inspired by the architecture of neurons in the human brain. A simple \"neuron\" N accepts input from other neurons, each of which, when activated (or \"fired\"), casts a weighted \"vote\" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.\nModern neural networks model complex relationships between inputs and outputs and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed as a type of mathematical optimization – they perform gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.\nOther learning techniques for neural networks are Hebbian learning (\"fire together, wire together\"), GMDH or competitive learning.The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.\n\n\n==== Deep learning ====\n\nDeep learning\nuses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others.\nDeep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. This can substantially reduce the number of weighted connections between neurons, and creates a hierarchy similar to the organization of the animal visual cortex.In a recurrent neural network (RNN) the signal will propagate through a layer more than once;\nthus, an RNN is an example of deep learning.\nRNNs can be trained by gradient descent,\nhowever long-term gradients which are back-propagated can \"vanish\" (that is, they can tend to zero) or \"explode\" (that is, they can tend to infinity), known as the vanishing gradient problem.\nThe long short term memory (LSTM) technique can prevent this in most cases.\n\n\n=== Specialized languages and hardware ===\n\nSpecialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing.\n\n\n== Applications ==\n\nAI is relevant to any intellectual task.\nModern artificial intelligence techniques are pervasive and are too numerous to list here.\nFrequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),\ntargeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon),\ndriving internet traffic, targeted advertising (AdSense, Facebook),\nvirtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars),\nautomatic language translation (Microsoft Translator, Google Translate),\nfacial recognition (Apple's Face ID or Microsoft's DeepFace),\nimage labeling (used by Facebook, Apple's iPhoto and TikTok)\n, spam filtering and chatbots (such as Chat GPT).\nThere are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are energy storage, deepfakes, medical diagnosis, military logistics, foreign policy, or supply chain management.\nGame playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.\nIn March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus and Cepheus. DeepMind in the 2010s developed a \"generalized artificial intelligence\" that could learn many diverse Atari games on its own.By 2020, natural language processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining a commonsense understanding of the contents of the benchmarks.\nDeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.\nOther applications predict the result of judicial decisions, create art (such as poetry or painting) and prove mathematical theorems.\nAI content detector tools are software applications that use artificial intelligence (AI) algorithms to analyze and detect specific types of content in digital media, such as text, images, and videos. These tools are commonly used to identify inappropriate content, such as speech errors, violent or sexual images, and spam, among others.\nSome benefits of using AI content detector tools include improved efficiency and accuracy in detecting inappropriate content, increased safety and security for users, and reduced legal and reputational risks for websites and platforms.\n\n\n=== Smart traffic lights ===\n\nSmart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.\n\n\n== Intellectual property ==\n\nIn 2019, WIPO reported that AI was the most prolific emerging technology in terms of the number of patent applications and granted patents, the Internet of things was estimated to be the largest in terms of market size. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G). Since AI emerged in the 1950s, 340,000 AI-related patent applications were filed by innovators and 1.6 million scientific papers have been published by researchers, with the majority of all AI-related patent filings published since 2013. Companies represent 26 out of the top 30 AI patent applicants, with universities or public research organizations accounting for the remaining four. The ratio of scientific papers to inventions has significantly decreased from 8:1 in 2010 to 3:1 in 2016, which is attributed to be indicative of a shift from theoretical research to the use of AI technologies in commercial products and services. Machine learning is the dominant AI technique disclosed in patents and is included in more than one-third of all identified inventions (134,777 machine learning patents filed for a total of 167,038 AI patents filed in 2016), with computer vision being the most popular functional application. AI-related patents not only disclose AI techniques and applications, they often also refer to an application field or industry. Twenty application fields were identified in 2016 and included, in order of magnitude: telecommunications (15 percent), transportation (15 percent), life and medical sciences (12 percent), and personal devices, computing and human–computer interaction (11 percent). Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). IBM has the largest portfolio of AI patents with 8,290 patent applications, followed by Microsoft with 5,930 patent applications.\n\n\n== Philosophy ==\n\n\n=== Defining artificial intelligence ===\n\nAlan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"\nHe advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".\nHe devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks\"Russell and Norvig agree with Turing that AI must be defined in terms of \"acting\" and not \"thinking\". However, they are critical that the test compares machines to people. \"Aeronautical engineering texts,\" they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\" AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world.\" Another AI founder, Marvin Minsky similarly defines it as \"the ability to solve hard problems\". These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine—and no other philosophical discussion is required, or may not even be possible.\nA definition that has also been adopted by Google - major practitionary in the field of AI.\nThis definition stipulated the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n\n\n=== Evaluating approaches to AI ===\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers.\n\n\n==== Symbolic AI and its limits ====\n\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.\nPhilosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.\nAlthough his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n\n\n==== Neat vs. scruffy ====\n\n\"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems (especially in areas like common sense reasoning). This issue was actively discussed in the 70s and 80s,\nbut in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed \"the victory of the neats\".\n\n\n==== Soft vs. hard computing ====\n\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n\n\n==== Narrow vs. general AI ====\n\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.\nGeneral intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.\n\n\n=== Machine consciousness, sentience and mind ===\n\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers \"don't care about the [philosophy of AI] – as long as the program works, they don't care whether you call it a simulation of intelligence or real intelligence.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n\n\n==== Consciousness ====\n\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\n\n\n==== Computationalism and functionalism ====\n\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"\nSearle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\n\n\n==== Robot rights ====\n\nIf a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so, then it could also suffer, and thus it would be entitled to certain rights.\nAny hypothetical robot rights would lie on a spectrum with animal rights and human rights.\nThis issue has been considered in fiction for centuries,\nand is now being considered by, for example, California's Institute for the Future; however, critics argue that the discussion is premature.\n\n\n== Future ==\n\n\n=== Superintelligence ===\n\nA superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.\nIts intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the \"singularity\".\nBecause it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.\n\n\n=== Risks ===\n\n\n==== Technological unemployment ====\n\nIn the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.\nA survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.\nSubjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\".Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".\nJobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\n\n\n==== Bad actors and weaponized AI ====\n\nAI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.Machine-learning AI is also able to design tens of thousands of toxic molecules in a matter of hours.\n\n\n==== Algorithmic bias ====\n\nAI programs can become biased after learning from real-world data. It is not typically introduced by the system designers but is learned by the program, and thus the programmers are often unaware that the bias exists.\nBias can be inadvertently introduced by the way training data is selected.\nIt can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be overestimated than that of white defendants, despite the fact that the program was not told the races of the defendants.Health equity issues may also be exacerbated when many-to-many mapping are done without taking steps to ensure equity for populations at risk for bias. At this time equity-focused tools and regulations are not in place to ensure equity application representation and usage. Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.\nAt its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022) the Association for Computing Machinery, in Seoul, South Korea, presented and published findings recommending that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.\n\n\n==== Existential risk ====\n\nSuperintelligent AI may be able to improve itself to the point that humans could not control it. This could, as physicist Stephen Hawking puts it, \"spell the end of the human race\". Philosopher Nick Bostrom argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI's goals do not fully reflect humanity's, it might need to harm humanity to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or \"friendly\" its stated goals might be.\nPolitical scientist Charles T. Rubin argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would share our system of morality.The opinion of experts and industry insiders is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.Stephen Hawking, Microsoft founder Bill Gates, history professor Yuval Noah Harari, and SpaceX founder Elon Musk have all expressed serious misgivings about the future of AI.\nProminent tech titans including Peter Thiel (Amazon Web Services) and Musk have committed more than $1 billion to nonprofit companies that champion responsible AI development, such as OpenAI and the Future of Life Institute.Mark Zuckerberg (CEO, Facebook) has said that artificial intelligence is helpful in its current form and will continue to assist humans.\nOther experts argue is that the risks are far enough in the future to not be worth researching,\nor that humans will be valuable from the perspective of a superintelligent machine.Rodney Brooks, in particular, has said that \"malevolent\" AI is still centuries away.\n\n\n==== Copyright ====\nAI's decisions making abilities raises the questions of legal responsibility and copyright status of created works. This issues are being refined in various jurisdictions.\n\n\n=== Ethical machines ===\n\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\nMachine ethics is also called machine morality, computational ethics or computational morality,\nand was founded at an AAAI symposium in 2005.Other approaches include Wendell Wallach's \"artificial moral agents\"\nand Stuart J. Russell's three principles for developing provably beneficial machines.\n\n\n=== Regulation ===\n\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.\nThe regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.\nBetween 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.\nMost EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.\nThe Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.\n\n\n== In fiction ==\n\nThought-capable artificial beings have appeared as storytelling devices since antiquity,\nand have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;\nwhile almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune.\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\n\n\n== See also ==\nAI safety – Research area on making AI safe and beneficial\nAI alignment – Conformance to the intended objective\nArtificial intelligence in healthcare - Machine-learning algorithms and software in the analysis, presentation, and comprehension of complex medical and health care data\nArtificial intelligence arms race – Arms race for the most advanced AI-related technologies\nBehavior selection algorithm – Algorithm that selects actions for intelligent agents\nBusiness process automation – Technology-enabled automation of complex business processes\nCase-based reasoning – Process of solving new problems based on the solutions of similar past problems\nEmergent algorithm – Algorithm exhibiting emergent behavior\nFemale gendering of AI technologies – Design of digital assistants as female\nGlossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence\nOperations research – Discipline concerning the application of advanced analytical methods\nRobotic process automation – Form of business process automation technology\nSynthetic intelligence – Alternate term for or form of artificial intelligence\nUniversal basic income – Welfare system of unconditional income\nWeak artificial intelligence – Form of artificial intelligence\nData sources – The list of data sources for study and research\nAutonomous robot – Robot that performs behaviors or tasks with a high degree of autonomy\n\n\n== Explanatory notes ==\n\n\n== References ==\n\n\n=== AI textbooks ===\nThese were the four the most widely used AI textbooks in 2008:\n\n\n=== History of AI ===\n\n\n=== Other sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\nThomason, Richmond. \"Logic and Artificial Intelligence\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\nArtificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005)."
    },
    "renaissance art": {
        "url": "https://en.wikipedia.org/wiki/Renaissance_art",
        "summary": "Renaissance art (1350 – 1620 AD) is the painting, sculpture, and decorative arts of the period of European history known as the Renaissance, which emerged as a distinct style in Italy in about AD 1400, in parallel with developments which occurred in philosophy, literature, music, science, and technology. Renaissance art took as its foundation the art of Classical antiquity, perceived as the noblest of ancient traditions, but transformed that tradition by absorbing recent developments in the art of Northern Europe and by applying contemporary scientific knowledge. Along with Renaissance humanist philosophy, it spread throughout Europe, affecting both artists and their patrons with the development of new techniques and new artistic sensibilities.",
        "content": "Renaissance art (1350 – 1620 AD) is the painting, sculpture, and decorative arts of the period of European history known as the Renaissance, which emerged as a distinct style in Italy in about AD 1400, in parallel with developments which occurred in philosophy, literature, music, science, and technology. Renaissance art took as its foundation the art of Classical antiquity, perceived as the noblest of ancient traditions, but transformed that tradition by absorbing recent developments in the art of Northern Europe and by applying contemporary scientific knowledge. Along with Renaissance humanist philosophy, it spread throughout Europe, affecting both artists and their patrons with the development of new techniques and new artistic sensibilities. For art historians, Renaissance art marks the transition of Europe from the medieval period to the Early Modern age.\n\nThe body of art, painting, sculpture, architecture, music and literature identified as \"Renaissance art\" was primarily produced during the 14th, 15th, and 16th centuries in Europe under the combined influences of an increased awareness of nature, a revival of classical learning, and a more individualistic view of man. Scholars no longer believe that the Renaissance marked an abrupt break with medieval values, as is suggested by the French word renaissance, literally meaning \"rebirth\". In many parts of Europe, Early Renaissance art was created in parallel with Late Medieval art.\n\n\n== Origins ==\nMany influences on the development of Renaissance men and women in the early 15th century have been credited with the emergence of Renaissance art; they are the same as those that affected philosophy, literature, architecture, theology, science, government and other aspects of society. The following list presents a summary of changes to social and cultural conditions which have been identified as factors which contributed to the development of Renaissance art. Each is dealt with more fully in the main articles cited above. The scholars of Renaissance period focused on present life and ways improve human life. They did not pay much attention to medieval philosophy or religion. During this period, scholars and humanists like Erasmus, Dante and Petrarch criticized superstitious beliefs and also questioned them.  The concept of education also widened its spectrum and focused more on creating 'an ideal man' who would have a fair understanding of arts, music, poetry and literature and would have the ability to appreciate these aspects of life. During this period, there emerged a scientific outlook which helped people question the needless rituals of the church. \n\nClassical texts, lost to European scholars for centuries, became available. These included documents of philosophy, prose, poetry, drama, science, a thesis on the arts, and early Christian theology.\nEurope gained access to advanced mathematics, which had its provenance in the works of Islamic scholars.\nThe advent of movable type printing in the 15th century meant that ideas could be disseminated easily, and an increasing number of books were written for a broader public.\nThe establishment of the Medici Bank and the subsequent trade it generated brought unprecedented wealth to a single Italian city, Florence.\nCosimo de' Medici set a new standard for patronage of the arts, not associated with the church or monarchy.\nHumanist philosophy meant that man's relationship with humanity, the universe and God was no longer the exclusive province of the church.\nA revived interest in the Classics brought about the first archaeological study of Roman remains by the architect Brunelleschi and sculptor Donatello. The revival of a style of architecture based on classical precedents inspired a corresponding classicism in painting and sculpture, which manifested itself as early as the 1420s in the paintings of Masaccio and Uccello.\nThe improvement of oil paint and developments in oil-painting technique by Belgian artists such as Robert Campin, Jan van Eyck, Rogier van der Weyden and Hugo van der Goes led to its adoption in Italy from about 1475 and had ultimately lasting effects on painting practices worldwide.\nThe serendipitous presence within the region of Florence in the early 15th century of certain individuals of artistic genius, most notably Masaccio, Brunelleschi, Ghiberti, Piero della Francesca, Donatello and Michelozzo formed an ethos out of which sprang the great masters of the High Renaissance, as well as supporting and encouraging many lesser artists to achieve work of extraordinary quality.\nA similar heritage of artistic achievement occurred in Venice through the talented Bellini family, their influential in-law Mantegna, Giorgione, Titian and Tintoretto.\nThe publication of two treatises by Leone Battista Alberti, De pictura (\"On Painting\") in 1435 and De re aedificatoria (\"Ten Books on Architecture\") in 1452.\n\n\n== History ==\n\n\n=== Proto-Renaissance in Italy, 1280–1400 ===\n\nIn Italy in the late 13th and early 14th centuries, the sculpture of Nicola Pisano and his son Giovanni Pisano, working at Pisa, Siena and Pistoia shows markedly classicising tendencies, probably influenced by the familiarity of these artists with ancient Roman sarcophagi. Their masterpieces are the pulpits of the Baptistery and Cathedral of Pisa. \nContemporary with Giovanni Pisano, the Florentine painter Giotto developed a manner of figurative painting that was unprecedentedly naturalistic, three-dimensional, lifelike and classicist, when compared with that of his contemporaries and teacher Cimabue. Giotto, whose greatest work is the cycle of the Life of Christ at the Arena Chapel in Padua, was seen by the 16th-century biographer Giorgio Vasari as \"rescuing and restoring art\" from the \"crude, traditional, Byzantine style\" prevalent in Italy in the 13th century.\n\n\n=== Early Renaissance in Italy, 1400–1495 ===\n\nAlthough both the Pisanos and Giotto had students and followers, the first truly Renaissance artists were not to emerge in Florence until 1401 with the competition to sculpt a set of bronze doors of the Baptistery of Florence Cathedral, which drew entries from seven young sculptors including Brunelleschi, Donatello and the winner, Lorenzo Ghiberti. Brunelleschi, most famous as the architect of the dome of Florence Cathedral and the Church of San Lorenzo, created a number of sculptural works, including a life-sized crucifix in Santa Maria Novella, renowned for its naturalism. His studies of perspective are thought to have influenced the painter Masaccio. Donatello became renowned as the greatest sculptor of the Early Renaissance, his masterpieces being his humanist and unusually erotic statue of David, one of the icons of the Florentine republic, and his great monument to Gattamelata, the first large equestrian bronze to be created since Roman times.\nThe contemporary of Donatello, Masaccio, was the painterly descendant of Giotto and began the Early Renaissance in Italian painting in 1425, furthering the trend towards solidity of form and naturalism of face and gesture that Giotto had begun a century earlier. From 1425–1428, Masaccio completed several panel paintings but is best known for the fresco cycle that he began in the Brancacci Chapel with the older artist Masolino and which had profound influence on later painters, including Michelangelo. Masaccio's developments were carried forward in the paintings of Fra Angelico, particularly in his frescos at the Convent of San Marco in Florence.\nThe treatment of the elements of perspective and light in painting was of particular concern to 15th-century Florentine painters. Uccello was so obsessed with trying to achieve an appearance of perspective that, according to Giorgio Vasari, it disturbed his sleep. His solutions can be seen in his masterpiece set of three paintings, the Battle of San Romano, which is believed to have been completed by 1460. Piero della Francesca made systematic and scientific studies of both light and linear perspective, the results of which can be seen in his fresco cycle of The History of the True Cross in San Francesco, Arezzo.\nIn Naples, the painter Antonello da Messina began using oil paints for portraits and religious paintings at a date that preceded other Italian painters, possibly about 1450. He carried this technique north and influenced the painters of Venice. One of the most significant painters of Northern Italy was Andrea Mantegna, who decorated the interior of a room, the Camera degli Sposi for his patron Ludovico Gonzaga, setting portraits of the family and court into an illusionistic architectural space.\nThe end period of the Early Renaissance in Italian art is marked, like its beginning, by a particular commission that drew artists together, this time in cooperation rather than competition. Pope Sixtus IV had rebuilt the Papal Chapel, named the Sistine Chapel in his honour, and commissioned a group of artists, Sandro Botticelli, Pietro Perugino, Domenico Ghirlandaio and Cosimo Rosselli to decorate its wall with fresco cycles depicting the Life of Christ and the Life of Moses. In the sixteen large paintings, the artists, although each working in his individual style, agreed on principles of format, and utilised the techniques of lighting, linear and atmospheric perspective, anatomy, foreshortening and characterisation that had been carried to a high point in the large Florentine studios of Ghiberti, Verrocchio, Ghirlandaio and Perugino.\n\n\n=== Early Netherlandish art, 1425–1525 ===\n\nThe painters of the Low Countries in this period included Jan van Eyck, his brother Hubert van Eyck, Robert Campin, Hans Memling, Rogier van der Weyden and Hugo van der Goes. Their painting developed partly independently of Early Italian Renaissance painting, and without the influence of a deliberate and conscious striving to revive antiquity.\nThe style of painting grew directly out of medieval painting in tempera, on panels and illuminated manuscripts, and other forms such as stained glass; the medium of fresco was less common in northern Europe. The medium used was oil paint, which had long been utilised for painting leather ceremonial shields and accoutrements because it was flexible and relatively durable. The earliest Netherlandish oil paintings are meticulous and detailed like tempera paintings. The material lent itself to the depiction of tonal variations and texture, so facilitating the observation of nature in great detail.\nThe Netherlandish painters did not approach the creation of a picture through a framework of linear perspective and correct proportion. They maintained a medieval view of hierarchical proportion and religious symbolism, while delighting in a realistic treatment of material elements, both natural and man-made. Jan van Eyck, with his brother Hubert, painted The Altarpiece of the Mystical Lamb. It is probable that Antonello da Messina became familiar with Van Eyck's work, while in Naples or Sicily. In 1475, Hugo van der Goes' Portinari Altarpiece arrived in Florence, where it was to have a profound influence on many painters, most immediately Domenico Ghirlandaio, who painted an altarpiece imitating its elements.\nA very significant Netherlandish painter towards the end of the period was Hieronymus Bosch, who employed the type of fanciful forms that were often utilized to decorate borders and letters in illuminated manuscripts, combining plant and animal forms with architectonic ones. When taken from the context of the illumination and peopled with humans, these forms give Bosch's paintings a surreal quality which have no parallel in the work of any other Renaissance painter. His masterpiece is the triptych The Garden of Earthly Delights.\n\n\n=== Early Renaissance in France, 1375–1528 ===\n\nThe artists of France (including duchies such as Burgundy) were often associated with courts, providing illuminated manuscripts and portraits for the nobility as well as devotional paintings and altarpieces. Among the most famous were the Limbourg brothers, Flemish illuminators and creators of the Très Riches Heures du Duc de Berry manuscript illumination. Jean Fouquet, painter of the royal court, visited Italy in 1437 and reflects the influence of Florentine painters such as Paolo Uccello. Although best known for his portraits such as that of Charles VII of France, Fouquet also created illuminations, and is thought to be the inventor of the portrait miniature. \nThere were a number of artists at this date who painted famed altarpieces, that are stylistically quite distinct from both the Italian and the Flemish. These include two enigmatic figures, Enguerrand Quarton, to whom is ascribed the Pieta of Villeneuve-lès-Avignon, and Jean Hey, otherwise known as \"the Master of Moulins\" after his most famous work, the Moulins Altarpiece. In these works, realism and close observation of the human figure, emotions and lighting are combined with a medieval formality, which includes gilt backgrounds.\n\n\n=== High Renaissance in Italy, 1495–1520 ===\n\nThe \"universal genius\" Leonardo da Vinci was to further perfect the aspects of pictorial art (lighting, linear and atmospheric perspective, anatomy, foreshortening and characterisation) that had preoccupied artists of the Early Renaissance, in a lifetime of studying and meticulously recording his observations of the natural world. His adoption of oil paint as his primary media meant that he could depict light and its effects on the landscape and objects more naturally and with greater dramatic effect than had ever been done before, as demonstrated in the Mona Lisa (1503–1506). His dissection of cadavers carried forward the understanding of skeletal and muscular anatomy, as seen in the unfinished Saint Jerome in the Wilderness (c. 1480). His depiction of human emotion in The Last Supper, completed 1495–1498, set the benchmark for religious painting.\n\nThe art of Leonardo's younger contemporary Michelangelo took a very different direction. Michelangelo in neither his painting nor his sculpture demonstrates any interest in the observation of any natural object except the human body. He perfected his technique in depicting it, while in his early twenties, by the creation of the enormous marble statue of David and the group Pietà, in the St Peter's Basilica, Rome. He then set about an exploration of the expressive possibilities of the human anatomy. His commission by Pope Julius II to paint the Sistine Chapel ceiling resulted in the supreme masterpiece of figurative composition, which was to have profound effect on every subsequent generation of European artists. His later work, The Last Judgement, painted on the altar wall of the Sistine Chapel between 1534 and 1541, shows a Mannerist (also called Late Renaissance) style with generally elongated bodies which took over from the High Renaissance style between 1520 and 1530.\nStanding alongside Leonardo and Michelangelo as the third great painter of the High Renaissance was the younger Raphael, who in a short lifespan painted a great number of life-like and engaging portraits, including those of Pope Julius II and his successor Pope Leo X, and numerous portrayals of the Madonna and Christ Child, including the Sistine Madonna. His death in 1520 at age 37 is considered by many art historians to be the end of the High Renaissance period, although some individual artists continued working in the High Renaissance style for many years thereafter.\nIn Northern Italy, the High Renaissance is represented primarily by members of the Venetian school, especially by the latter works of Giovanni Bellini, especially religious paintings, which include several large altarpieces of a type known as \"Sacred Conversation\", which show a group of saints around the enthroned Madonna. His contemporary Giorgione, who died at about the age of 32 in 1510, left a small number of enigmatic works, including The Tempest, the subject of which has remained a matter of speculation. The earliest works of Titian date from the era of the High Renaissance, including a massive altarpiece The Assumption of the Virgin which combines human action and drama with spectacular colour and atmosphere. Titian continued painting in a generally High Renaissance style until near the end of his career in the 1570s, although he increasingly used colour and light over line to define his figures.\n\n\n=== German Renaissance art ===\n\nGerman Renaissance art falls into the broader category of the Renaissance in Northern Europe, also known as the Northern Renaissance. Renaissance influences began to appear in German art in the 15th century, but this trend was not widespread. Gardner's Art Through the Ages identifies Michael Pacher, a painter and sculptor, as the first German artist whose work begins to show Italian Renaissance influences. According to that source, Pacher's painting, St. Wolfgang Forces the Devil to Hold His Prayerbook (c. 1481), is Late Gothic in style, but also shows the influence of the Italian artist Mantegna.In the 1500s, Renaissance art in Germany became more common as, according to Gardner, \"The art of northern Europe during the sixteenth century is characterized by a sudden awareness of the advances made by the Italian Renaissance and by a desire to assimilate this new style as rapidly as possible.\" One of the best known practitioners of German Renaissance art was Albrecht Dürer (1471–1528), whose fascination with classical ideas led him to Italy to study art. Both Gardner and Russell recognized the importance of Dürer's contribution to German art in bringing Italian Renaissance styles and ideas to Germany. Russell calls this \"Opening the Gothic windows of German art,\" while Gardner calls it Dürer's \"life mission.\" Importantly, as Gardner points out, Dürer \"was the first northern artist who fully understood the basic aims of the southern Renaissance,\" although his style did not always reflect that. The same source says that Hans Holbein the Younger (1497–1543) successfully assimilated Italian ideas while also keeping \"northern traditions of close realism.\" This is contrasted with Dürer's tendency to work in \"his own native German style\" instead of combining German and Italian styles. Other important artists of the German Renaissance were Matthias Grünewald, Albrecht Altdorfer and Lucas Cranach the Elder.Artisans such as engravers became more concerned with aesthetics rather than just perfecting their crafts. Germany had master engravers, such as Martin Schongauer, who did metal engravings in the late 1400s. Gardner relates this mastery of the graphic arts to advances in printing which occurred in Germany, and says that metal engraving began to replace the woodcut during the Renaissance. However, some artists, such as Albrecht Dürer, continued to do woodcuts. Both Gardner and Russell describe the fine quality of Dürer's woodcuts, with Russell stating in The World of Dürer that Dürer \"elevated them into high works of art.\"\n\n\n=== Britain ===\n\nBritain was very late to develop a distinct Renaissance style and most artists of the Tudor court were imported foreigners, usually from the Low Countries, including Hans Holbein the Younger, who died in England. One exception was the portrait miniature, which artists including Nicholas Hilliard developed into a distinct genre well before it became popular in the rest of Europe. Renaissance art in Scotland was similarly dependent on imported artists, and largely restricted to the court.\n\n\n== Themes and symbolism ==\n \n\nRenaissance artists painted a wide variety of themes. Religious altarpieces, fresco cycles, and small works for private devotion were very popular. For inspiration, painters in both Italy and northern Europe frequently turned to Jacobus de Voragine's Golden Legend (1260), a highly influential source book for the lives of saints that had already had a strong influence on Medieval artists. The rebirth of classical antiquity and Renaissance humanism also resulted in many mythological and history paintings. Ovidian stories, for example, were very popular. Decorative ornament, often used in painted architectural elements, was especially influenced by classical Roman motifs.\n\n\n== Techniques ==\nThe use of proportion – The first major treatment of the painting as a window into space appeared in the work of Giotto di Bondone, at the beginning of the 14th century.  True linear perspective was formalized later, by Filippo Brunelleschi and Leon Battista Alberti.  In addition to giving a more realistic presentation of art, it moved Renaissance painters into composing more paintings.\nForeshortening – The term foreshortening refers to the artistic effect of shortening lines in a drawing so as to create an illusion of depth.\nSfumato – The term sfumato was coined by Italian Renaissance artist Leonardo da Vinci and refers to a fine art painting technique of blurring or softening of sharp outlines by subtle and gradual blending of one tone into another through the use of thin glazes to give the illusion of depth or three-dimensionality. This stems from the Italian word sfumare meaning to evaporate or to fade out. The Latin origin is fumare, to smoke.\nChiaroscuro – The term chiaroscuro refers to the fine art painting modeling effect of using a strong contrast between light and dark to give the illusion of depth or three-dimensionality. This comes from the Italian words meaning light (chiaro) and dark (scuro), a technique which came into wide use in the Baroque period.\n\n\n== List of Renaissance artists ==\n\n\n=== Italy ===\n\nGiotto di Bondone (1267–1337)\nFilippo Brunelleschi (1377–1446)\nMasolino (c. 1383 – c. 1447)\nDonatello (c. 1386 – 1466)\nPisanello (c. 1395 – c. 1455)\nFra Angelico (c. 1395 – 1455)\nPaolo Uccello (1397–1475)\nMasaccio (1401–1428)\nLeone Battista Alberti (1404–1472)\nFilippo Lippi (c. 1406 – 1469)\nDomenico Veneziano (c. 1410 – 1461)\nPiero della Francesca (c. 1415 – 1492)\nAndrea del Castagno (c. 1421 – 1457)\nBenozzo Gozzoli (c. 1421 – 1497)\nAlessio Baldovinetti (1425–1499)\nAntonio del Pollaiuolo (1429–1498)\nAntonello da Messina (c. 1430 – 1479)\nGiovanni Bellini (c.1430–1516)\nAndrea Mantegna (c. 1431 – 1506)\nAndrea del Verrocchio (c. 1435 – 1488)\nGiovanni Santi (1435–1494)\nCarlo Crivelli (c. 1435 – c. 1495)\nDonato Bramante (1444–1514)\nSandro Botticelli (c. 1445 – 1510)\nLuca Signorelli (c. 1445 – 1523)\nBiagio d'Antonio (1446–1516)\nPietro Perugino (1446–1523)\nDomenico Ghirlandaio (1449–1494)\nLeonardo da Vinci (1452–1519)\nPinturicchio (1454–1513)\nFilippino Lippi (1457–1504)\nAndrea Solari (1460–1524)\nPiero di Cosimo (1462–1522)\nVittore Carpaccio (1465–1526)\nBernardino de' Conti (1465–1525)\nGiorgione (c. 1473–1510)\nMichelangelo (1475–1564)\nLorenzo Lotto (1480–1557)\nRaphael (1483–1520)\nMarco Cardisco (c. 1486 – c. 1542)\nTitian (c. 1488/1490 – 1576)\nCorregio (c. 1489 – 1534)\nPietro Negroni (c. 1505 – c. 1565)\nSofonisba Anguissola (c. 1532 – 1625)\n\n\n=== Low Countries ===\n\nHubert van Eyck (1366?–1426)\nRobert Campin (c. 1380 – 1444)\nLimbourg brothers (fl. 1385–1416)\nJan van Eyck (1385?–1440?)\nRogier van der Weyden (1399/1400–1464)\nJacques Daret (c. 1404 – c. 1470)\nPetrus Christus (1410/1420–1472)\nDirk Bouts (1415–1475)\nHugo van der Goes (c. 1430/1440 – 1482)\nHans Memling (c. 1430 – 1494)\nHieronymus Bosch (c. 1450 – 1516)\nGerard David (c. 1455 – 1523)\nGeertgen tot Sint Jans (c. 1465 – c. 1495)\nQuentin Matsys (1466–1530)\nJean Bellegambe (c. 1470 – 1535)\nJoachim Patinir (c. 1480 – 1524)\nAdriaen Isenbrant (c. 1490 – 1551)\n\n\n=== Germany ===\nHans Holbein the Elder (c. 1460 – 1524)\nMatthias Grünewald (c. 1470 – 1528)\nAlbrecht Dürer (1471–1528)\nLucas Cranach the Elder (1472–1553)\nHans Burgkmair (1473–1531)\nJerg Ratgeb (c. 1480 – 1526)\nAlbrecht Altdorfer (c. 1480 – 1538)\nLeonhard Beck (c. 1480 – 1542)\nHans Baldung (c. 1480 – 1545)\nWilhelm Stetter (1487–1552)\nBarthel Bruyn the Elder (1493–1555)\nAmbrosius Holbein (1494–1519)\nHans Holbein the Younger (c. 1497 – 1543)\nConrad Faber von Kreuznach (c. 1500 – c. 1553)\nLucas Cranach the Younger (1515–1586)\n\n\n=== France ===\nEnguerrand Quarton (c. 1410 – c. 1466)\nBarthélemy d'Eyck (c. 1420 – after 1470)\nJean Fouquet (1420–1481)\nSimon Marmion (c. 1425 – 1489)\nNicolas Froment (c. 1435 – c. 1486)\nJean Hey (fl. c. 1475 – c. 1505)\nJean Clouet (1480–1541)\nFrançois Clouet (c. 1510 – 1572)\n\n\n=== Portugal ===\nGrão Vasco (1475–1542)\nGregório Lopes (1490–1550)\nFrancisco de Holanda (1517–1585)\nCristóvão Lopes (1516–1594)\nCristóvão de Figueiredo (?-c.1543)\nJorge Afonso (1470–1540)\nAntónio de Holanda (1480–1571)\nCristóvão de Morais\nNuno Gonçalves (c. 1425 – c. 1491)\nFrancisco Henriques (?–1518)\nFrei Carlos (?–1540)\n\n\n=== Spain ===\nJaume Huguet (1412–1492)\nBartolomé Bermejo (c. 1440 – c. 1501)\nPaolo da San Leocadio (1447 – c. 1520)\nPedro Berruguete (c. 1450 – 1504)\nAyne Bru\nJuan de Flandes (c. 1460 – c. 1519)\nLuis de Morales (1512–1586)\nAlonso Sánchez Coello (1531–1588)\nEl Greco (1541–1614)\n\n\n=== Venetian Dalmatia (modern Croatia) ===\nGiorgio da Sebenico (c. 1410 – 1475)\nNiccolò di Giovanni Fiorentino (1418–1506)\nAndrea Alessi (1425–1505)\nFrancesco Laurana (c. 1430 – 1502)\nGiovanni Dalmata (c. 1440 – c. 1514)\nNicholas of Ragusa (1460? – 1517)\nAndrea Schiavone (c. 1510/1515 – 1563)\n\n\n== Works ==\nGhent Altarpiece, by Hubert and Jan van Eyck\nThe Arnolfini Portrait, by Jan van Eyck\nThe Werl Triptych, by Robert Campin\nThe Portinari Triptych, by Hugo van der Goes\nThe Descent from the Cross, by Rogier van der Weyden\nFlagellation of Christ, by Piero della Francesca\nSpring, by Sandro Botticelli\nLamentation of Christ, by Mantegna\nThe Last Supper, by Leonardo da Vinci\nThe School of Athens, by Raphael\nSistine Chapel ceiling, by Michelangelo\nEquestrian Portrait of Charles V, by Titian\nIsenheim Altarpiece, by Matthias Grünewald\nMelencolia I, by Albrecht Dürer\nThe Ambassadors, by Hans Holbein the Younger\nMelun Diptych, by Jean Fouquet\nSaint Vincent Panels, by Nuno Gonçalves\n\n\n== Major collections ==\nNational Gallery, London, UK\nMuseo del Prado, Madrid, Spain\nUffizi, Florence, Italy\nLouvre, Paris, France\nNational Gallery of Art, Washington, USA\nGemäldegalerie, Berlin, Germany\nRijksmuseum, Amsterdam\nMetropolitan Museum of Art, New York City, USA\nRoyal Museums of Fine Arts of Belgium, Belgium, Brussels\nGroeningemuseum, Bruges, Belgium\nOld St. John's Hospital, Bruges, Belgium\nBargello, Florence, Italy\nChâteau d'Écouen (National museum of the Renaissance), Écouen, France\nVatican museums, Vatican city\nPinacoteca di Brera, Milan, Italy\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n\nThe Early Renaissance (video on YouTube)\n\"Limited Freedom\", Marica Hall, Berfrois, 2 March 2011."
    },
    "pythagorean theorem": {
        "url": "https://en.wikipedia.org/wiki/Pythagorean_theorem",
        "summary": "In mathematics, the Pythagorean theorem or Pythagoras' theorem is a fundamental relation in Euclidean geometry between the three sides of a right triangle. It states that the area of the square whose side is the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares on the other two sides. This theorem can be written as an equation relating the lengths of the sides a, b and the hypotenuse c, often called the Pythagorean equation:\n\n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n        .",
        "content": "In mathematics, the Pythagorean theorem or Pythagoras' theorem is a fundamental relation in Euclidean geometry between the three sides of a right triangle. It states that the area of the square whose side is the hypotenuse (the side opposite the right angle) is equal to the sum of the areas of the squares on the other two sides. This theorem can be written as an equation relating the lengths of the sides a, b and the hypotenuse c, often called the Pythagorean equation:\n\n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle a^{2}+b^{2}=c^{2}.}\n  The theorem is named for the Greek philosopher Pythagoras, born around 570 BC.  The theorem has been proven numerous times by many different methods – possibly the most for any mathematical theorem. The proofs are diverse, including both geometric proofs and algebraic proofs, with some dating back thousands of years.\nWhen Euclidean space is represented by a Cartesian coordinate system in analytic geometry, Euclidean distance satisfies the Pythagorean relation: the squared distance between two points equals the sum of squares of the difference in each coordinate between the points.\nThe theorem can be generalized in various ways: to higher-dimensional spaces, to spaces that are not Euclidean, to objects that are not right triangles, and to objects that are not triangles at all but n-dimensional solids. The Pythagorean theorem has attracted interest outside mathematics as a symbol of mathematical abstruseness, mystique, or intellectual power; popular references in literature, plays, musicals, songs, stamps, and cartoons abound.\n\n\n== Other forms of the theorem ==\nIf c denotes the length of the hypotenuse and a and b denote the two lengths of the legs of a right triangle, then the Pythagorean theorem can be expressed as the Pythagorean equation:\n\n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle a^{2}+b^{2}=c^{2}.}\n  If only the lengths of the legs of the right triangle are known but not the hypotenuse, then the length of the hypotenuse can be calculated with the equation\n\n  \n    \n      \n        c\n        =\n        \n          \n            \n              a\n              \n                2\n              \n            \n            +\n            \n              b\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle c={\\sqrt {a^{2}+b^{2}}}.}\n  If the length of the hypotenuse and of one leg is known, then the length of the other leg can be calculated as\n\n  \n    \n      \n        a\n        =\n        \n          \n            \n              c\n              \n                2\n              \n            \n            −\n            \n              b\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle a={\\sqrt {c^{2}-b^{2}}}}\n  or\n\n  \n    \n      \n        b\n        =\n        \n          \n            \n              c\n              \n                2\n              \n            \n            −\n            \n              a\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle b={\\sqrt {c^{2}-a^{2}}}.}\n  A generalization of this theorem is the law of cosines, which allows the computation of the length of any side of any triangle, given the lengths of the other two sides and the angle between them. If the angle between the other sides is a right angle, the law of cosines reduces to the Pythagorean equation.\n\n\n== Proofs using constructed squares ==\n\n\n=== Rearrangement proofs ===\nIn one rearrangement proof, two squares are used whose sides have a measure of  \n  \n    \n      \n        a\n        +\n        b\n      \n    \n    {\\displaystyle a+b}\n    and which contain four right triangles whose sides are a, b and c, with the hypotenuse being c.  In the square on the right side, the triangles are placed such that the corners of the square correspond to the corners of the right angle in the triangles, forming a square in the center whose sides are length c.  Each outer square has an area of \n  \n    \n      \n        (\n        a\n        +\n        b\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (a+b)^{2}}\n    as well as \n  \n    \n      \n        2\n        a\n        b\n        +\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 2ab+c^{2}}\n  , with \n  \n    \n      \n        2\n        a\n        b\n      \n    \n    {\\displaystyle 2ab}\n   representing the total area of the four triangles.  Within the big square on the left side, the four triangles are moved to form two similar rectangles with sides of length a and b.  These rectangles in their new position have now delineated two new squares, one having side length a is formed in the bottom-left corner, and another square of side length b formed in the top-right corner.  In this new position, this left side now has a square of area  \n  \n    \n      \n        (\n        a\n        +\n        b\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (a+b)^{2}}\n    as well as  \n  \n    \n      \n        2\n        a\n        b\n        +\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 2ab+a^{2}+b^{2}}\n  .  Since both squares have the area of \n  \n    \n      \n        (\n        a\n        +\n        b\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (a+b)^{2}}\n   it follows that the other measure of the square area also equal each other such that \n  \n    \n      \n        2\n        a\n        b\n        +\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 2ab+c^{2}}\n   = \n  \n    \n      \n        2\n        a\n        b\n        +\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 2ab+a^{2}+b^{2}}\n  .  With the area of the four triangles removed from both side of the equation what remains is \n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle a^{2}+b^{2}=c^{2}.}\n    In another proof rectangles in the second box can also be placed such that both have one corner that correspond to consecutive corners of the square.  In this way they also form two boxes, this time in consecutive corners, with areas  \n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{2}}\n   and \n  \n    \n      \n        \n          b\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle b^{2}}\n  which will again lead to a second square of with the area \n  \n    \n      \n        2\n        a\n        b\n        +\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle 2ab+a^{2}+b^{2}}\n  .\nEnglish mathematician Sir Thomas Heath gives this proof in his commentary on Proposition I.47 in Euclid's Elements, and mentions the proposals of German mathematicians Carl Anton Bretschneider and Hermann Hankel that Pythagoras may have known this proof.  Heath himself favors a different proposal for a Pythagorean proof, but acknowledges from the outset of his discussion \"that the Greek literature which we possess belonging to the first five centuries after Pythagoras contains no statement specifying this or any other particular great geometric discovery to him.\"  Recent scholarship has cast increasing doubt on any sort of role for Pythagoras as a creator of mathematics, although debate about this continues.\n\n\n=== Algebraic proofs ===\nThe theorem can be proved algebraically using four copies of the same triangle arranged symmetrically around a square with side c, as shown in the lower part of the diagram. This results in a larger square, with side a + b and area (a + b)2. The four triangles and the square side c must have the same area as the larger square,\n\n  \n    \n      \n        (\n        b\n        +\n        a\n        \n          )\n          \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n        +\n        4\n        \n          \n            \n              a\n              b\n            \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n        +\n        2\n        a\n        b\n        ,\n      \n    \n    {\\displaystyle (b+a)^{2}=c^{2}+4{\\frac {ab}{2}}=c^{2}+2ab,}\n  giving\n\n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        =\n        (\n        b\n        +\n        a\n        \n          )\n          \n            2\n          \n        \n        −\n        2\n        a\n        b\n        =\n        \n          b\n          \n            2\n          \n        \n        +\n        2\n        a\n        b\n        +\n        \n          a\n          \n            2\n          \n        \n        −\n        2\n        a\n        b\n        =\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle c^{2}=(b+a)^{2}-2ab=b^{2}+2ab+a^{2}-2ab=a^{2}+b^{2}.}\n  A similar proof uses four copies of a right triangle with sides a, b and c, arranged inside a square with side c as in the top half of the diagram. The triangles are similar with area \n  \n    \n      \n        \n          \n            \n              1\n              2\n            \n          \n        \n        a\n        b\n      \n    \n    {\\displaystyle {\\tfrac {1}{2}}ab}\n  , while the small square has side b − a and area (b − a)2. The area of the large square is therefore\n\n  \n    \n      \n        (\n        b\n        −\n        a\n        \n          )\n          \n            2\n          \n        \n        +\n        4\n        \n          \n            \n              a\n              b\n            \n            2\n          \n        \n        =\n        (\n        b\n        −\n        a\n        \n          )\n          \n            2\n          \n        \n        +\n        2\n        a\n        b\n        =\n        \n          b\n          \n            2\n          \n        \n        −\n        2\n        a\n        b\n        +\n        \n          a\n          \n            2\n          \n        \n        +\n        2\n        a\n        b\n        =\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle (b-a)^{2}+4{\\frac {ab}{2}}=(b-a)^{2}+2ab=b^{2}-2ab+a^{2}+2ab=a^{2}+b^{2}.}\n  But this is a square with side c and area c2, so\n\n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        =\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle c^{2}=a^{2}+b^{2}.}\n  \n\n\n== Other proofs of the theorem ==\nThis theorem may have more known proofs than any other (the law of quadratic reciprocity being another contender for that distinction); the book The Pythagorean Proposition contains 370 proofs.\n\n\n=== Proof using similar triangles ===\n\nThis proof is based on the proportionality of the sides of three similar triangles, that is, upon the fact that the ratio of any two corresponding sides of similar triangles is the same regardless of the size of the triangles.\nLet ABC represent a right triangle, with the right angle located at C, as shown on the figure. Draw the altitude from point C, and call H its intersection with the side AB. Point H divides the length of the hypotenuse c into parts d and e. The new triangle, ACH, is similar to triangle ABC, because they both have a right angle (by definition of the altitude), and they share the angle at A, meaning that the third angle will be the same in both triangles as well, marked as θ in the figure. By a similar reasoning, the triangle CBH is also similar to ABC. The proof of similarity of the triangles requires the triangle postulate: The sum of the angles in a triangle is two right angles, and is equivalent to the parallel postulate. Similarity of the triangles leads to the equality of ratios of corresponding sides:\n\n  \n    \n      \n        \n          \n            \n              B\n              C\n            \n            \n              A\n              B\n            \n          \n        \n        =\n        \n          \n            \n              B\n              H\n            \n            \n              B\n              C\n            \n          \n        \n        \n           and \n        \n        \n          \n            \n              A\n              C\n            \n            \n              A\n              B\n            \n          \n        \n        =\n        \n          \n            \n              A\n              H\n            \n            \n              A\n              C\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {BC}{AB}}={\\frac {BH}{BC}}{\\text{ and }}{\\frac {AC}{AB}}={\\frac {AH}{AC}}.}\n  The first result equates the cosines of the angles θ, whereas the second result equates their sines.\nThese ratios can be written as\n\n  \n    \n      \n        B\n        \n          C\n          \n            2\n          \n        \n        =\n        A\n        B\n        ×\n        B\n        H\n        \n           and \n        \n        A\n        \n          C\n          \n            2\n          \n        \n        =\n        A\n        B\n        ×\n        A\n        H\n        .\n      \n    \n    {\\displaystyle BC^{2}=AB\\times BH{\\text{ and }}AC^{2}=AB\\times AH.}\n  Summing these two equalities results in\n\n  \n    \n      \n        B\n        \n          C\n          \n            2\n          \n        \n        +\n        A\n        \n          C\n          \n            2\n          \n        \n        =\n        A\n        B\n        ×\n        B\n        H\n        +\n        A\n        B\n        ×\n        A\n        H\n        =\n        A\n        B\n        (\n        A\n        H\n        +\n        B\n        H\n        )\n        =\n        A\n        \n          B\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle BC^{2}+AC^{2}=AB\\times BH+AB\\times AH=AB(AH+BH)=AB^{2},}\n  which, after simplification, demonstrates the Pythagorean theorem:\n\n  \n    \n      \n        B\n        \n          C\n          \n            2\n          \n        \n        +\n        A\n        \n          C\n          \n            2\n          \n        \n        =\n        A\n        \n          B\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle BC^{2}+AC^{2}=AB^{2}.}\n  The role of this proof in history is the subject of much speculation. The underlying question is why Euclid did not use this proof, but invented another. One conjecture is that the proof by similar triangles involved a theory of proportions, a topic not discussed until later in the Elements, and that the theory of proportions needed further development at that time.\n\n\n=== Euclid's proof ===\n\nIn outline, here is how the proof in Euclid's Elements proceeds. The large square is divided into a left and right rectangle. A triangle is constructed that has half the area of the left rectangle. Then another triangle is constructed that has half the area of the square on the left-most side. These two triangles are shown to be congruent, proving this square has the same area as the left rectangle. This argument is followed by a similar version for the right rectangle and the remaining square. Putting the two rectangles together to reform the square on the hypotenuse, its area is the same as the sum of the area of the other two squares. The details follow.\nLet A, B, C be the vertices of a right triangle, with a right angle at A. Drop a perpendicular from A to the side opposite the hypotenuse in the square on the hypotenuse. That line divides the square on the hypotenuse into two rectangles, each having the same area as one of the two squares on the legs.\nFor the formal proof, we require four elementary lemmata:\n\nIf two triangles have two sides of the one equal to two sides of the other, each to each, and the angles included by those sides equal, then the triangles are congruent (side-angle-side).\nThe area of a triangle is half the area of any parallelogram on the same base and having the same altitude.\nThe area of a rectangle is equal to the product of two adjacent sides.\nThe area of a square is equal to the product of two of its sides (follows from 3).Next, each top square is related to a triangle congruent with another triangle related in turn to one of two rectangles making up the lower square.\n\nThe proof is as follows:\n\nLet ACB be a right-angled triangle with right angle CAB.\nOn each of the sides BC, AB, and CA, squares are drawn, CBDE, BAGF, and ACIH, in that order. The construction of squares requires the immediately preceding theorems in Euclid, and depends upon the parallel postulate.\nFrom A, draw a line parallel to BD and CE. It will perpendicularly intersect BC and DE at K and L, respectively.\nJoin CF and AD, to form the triangles BCF and BDA.\nAngles CAB and BAG are both right angles; therefore C, A, and G are collinear.\nAngles CBD and FBA are both right angles; therefore angle ABD equals angle FBC, since both are the sum of a right angle and angle ABC.\nSince AB is equal to FB, BD is equal to BC and angle ABD equals angle FBC, triangle ABD must be congruent to triangle FBC.\nSince A-K-L is a straight line, parallel to BD, then rectangle BDLK has twice the area of triangle ABD because they share the base BD and have the same altitude BK, i.e., a line normal to their common base, connecting the parallel lines BD and AL. (lemma 2)\nSince C is collinear with A and G, and this line is parallel to FB, then square BAGF must be twice in area to triangle FBC.\nTherefore, rectangle BDLK must have the same area as square BAGF = AB2.\nBy applying steps 3 to 10 to the other side of the figure, it can be similarly shown that rectangle CKLE must have the same area as square ACIH = AC2.\nAdding these two results, AB2 + AC2 = BD × BK + KL × KC\nSince BD = KL, BD × BK + KL × KC = BD(BK + KC) = BD × BC\nTherefore, AB2 + AC2 = BC2, since CBDE is a square.This proof, which appears in Euclid's Elements as that of Proposition 47 in Book 1, demonstrates that the area of the square on the hypotenuse is the sum of the areas of the other two squares.\nThis is quite distinct from the proof by similarity of triangles, which is conjectured to be the proof that Pythagoras used.\n\n\n=== Proofs by dissection and rearrangement ===\nAnother by rearrangement is given by the middle animation. A large square is formed with area c2, from four identical right triangles with sides a, b and c, fitted around a small central square. Then two rectangles are formed with sides a and b by moving the triangles. Combining the smaller square with these rectangles produces two squares of areas a2 and b2, which must have the same area as the initial large square.\nThe third, rightmost image also gives a proof. The upper two squares are divided as shown by the blue and green shading, into pieces that when rearranged can be made to fit in the lower square on the hypotenuse – or conversely the large square can be divided as shown into pieces that fill the other two. This way of cutting one figure into pieces and rearranging them to get another figure is called dissection. This shows the area of the large square equals that of the two smaller ones.\n\n\n=== Einstein's proof by dissection without rearrangement ===\n\nAlbert Einstein gave a proof by dissection in which the pieces do not need to be moved. Instead of using a square on the hypotenuse and two squares on the legs, one can use any other shape that includes the hypotenuse, and two similar shapes that each include one of two legs instead of the hypotenuse (see Similar figures on the three sides).  In Einstein's proof, the shape that includes the hypotenuse is the right triangle itself.  The dissection consists of dropping a perpendicular from the vertex of the right angle of the triangle to the hypotenuse, thus splitting the whole triangle into two parts.  Those two parts have the same shape as the original right triangle, and have the legs of the original triangle as their hypotenuses, and the sum of their areas is that of the original triangle.  Because the ratio of the area of a right triangle to the square of its hypotenuse is the same for similar triangles, the relationship between the areas of the three triangles holds for the squares of the sides of the large triangle as well.\n\n\n=== Proof by area-preserving shearing ===\n\nAs shown in the accompanying animation, area-preserving shear mappings and translations can transform the squares on the sides adjacent to the right-angle onto the square on the hypotenuse, together covering it exactly. Each shear leaves the base and height unchanged, thus leaving the area unchanged too. The translations also leave the area unchanged, as they do not alter the shapes at all. Each square is first sheared into a parallelogram, and then into a rectangle which can be translated onto one section of the square on the hypotenuse.\n\n\n=== Algebraic proofs ===\nA related proof was published by future U.S. President James A. Garfield (then a U.S. Representative) (see diagram). Instead of a square it uses a trapezoid, which can be constructed from the square in the second of the above proofs by bisecting along a diagonal of the inner square, to give the trapezoid as shown in the diagram. The area of the trapezoid can be calculated to be half the area of the square, that is\n\n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n        (\n        b\n        +\n        a\n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {1}{2}}(b+a)^{2}.}\n  The inner square is similarly halved, and there are only two triangles so the proof proceeds as above except for a factor of \n  \n    \n      \n        \n          \n            1\n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{2}}}\n  , which is removed by multiplying by two to give the result.\n\n\n=== Proof using differentials ===\nOne can arrive at the Pythagorean theorem by studying how changes in a side produce a change in the hypotenuse and employing calculus.The triangle ABC is a right triangle, as shown in the upper part of the diagram, with BC the hypotenuse. At the same time the triangle lengths are measured as shown, with the hypotenuse of length y, the side AC of length x and the side AB of length a, as seen in the lower diagram part.\n\nIf x is increased by a small amount dx by extending the side AC slightly to D, then y also increases by dy. These form two sides of a triangle, CDE, which (with E chosen so CE is perpendicular to the hypotenuse) is a right triangle approximately similar to ABC. Therefore, the ratios of their sides must be the same, that is:\n\n  \n    \n      \n        \n          \n            \n              d\n              y\n            \n            \n              d\n              x\n            \n          \n        \n        =\n        \n          \n            x\n            y\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {dy}{dx}}={\\frac {x}{y}}.}\n  This can be rewritten as \n  \n    \n      \n        y\n        \n        d\n        y\n        =\n        x\n        \n        d\n        x\n      \n    \n    {\\displaystyle y\\,dy=x\\,dx}\n   , which is a differential equation that can be solved by direct integration:\n\n  \n    \n      \n        ∫\n        y\n        \n        d\n        y\n        =\n        ∫\n        x\n        \n        d\n        x\n        \n        ,\n      \n    \n    {\\displaystyle \\int y\\,dy=\\int x\\,dx\\,,}\n  giving\n\n  \n    \n      \n        \n          y\n          \n            2\n          \n        \n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        C\n        .\n      \n    \n    {\\displaystyle y^{2}=x^{2}+C.}\n  The constant can be deduced from x = 0, y = a to give the equation\n\n  \n    \n      \n        \n          y\n          \n            2\n          \n        \n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        \n          a\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle y^{2}=x^{2}+a^{2}.}\n  This is more of an intuitive proof than a formal one: it can be made more rigorous if proper limits are used in place of dx and dy.\n\n\n== Converse ==\nThe converse of the theorem is also true:\nGiven a triangle with sides of length a, b, and c, if a2 + b2 = c2, then the angle between sides a and b is a right angle.\nFor any three positive real numbers a, b, and c such that a2 + b2 = c2, there exists a triangle with sides a, b and c as a consequence of the converse of the triangle inequality.\nThis converse appears in Euclid's Elements (Book I, Proposition 48):  \"If in a triangle the square on one of the sides equals the sum of the squares on the remaining two sides of the triangle, then the angle contained by the remaining two sides of the triangle is right.\"It can be proven using the law of cosines or as follows:\nLet ABC be a triangle with side lengths a, b, and c, with a2 + b2 = c2. Construct a second triangle with sides of length a and b containing a right angle. By the Pythagorean theorem, it follows that the hypotenuse of this triangle has length c = √a2 + b2, the same as the hypotenuse of the first triangle. Since both triangles' sides are the same lengths a, b and c, the triangles are congruent and must have the same angles. Therefore, the angle between the side of lengths a and b in the original triangle is a right angle.\nThe above proof of the converse makes use of the Pythagorean theorem itself. The converse can also be proven without assuming the Pythagorean theorem.A corollary of the Pythagorean theorem's converse is a simple means of determining whether a triangle is right, obtuse, or acute, as follows. Let c be chosen to be the longest of the three sides and a + b > c (otherwise there is no triangle according to the triangle inequality). The following statements apply:\nIf a2 + b2 = c2, then the triangle is right.\nIf a2 + b2 > c2, then the triangle is acute.\nIf a2 + b2 < c2, then the triangle is obtuse.Edsger W. Dijkstra has stated this proposition about acute, right, and obtuse triangles in this language:\n\nsgn(α + β − γ) = sgn(a2 + b2 − c2),where α is the angle opposite to side a, β is the angle opposite to side b, γ is the angle opposite to side c, and sgn is the sign function.\n\n\n== Consequences and uses of the theorem ==\n\n\n=== Pythagorean triples ===\n\nA Pythagorean triple has three positive integers a, b, and c, such that a2 + b2 = c2. In other words, a Pythagorean triple represents the lengths of the sides of a right triangle where all three sides have integer lengths. Such a triple is commonly written (a, b, c). Some well-known examples are (3, 4, 5) and (5, 12, 13).\nA primitive Pythagorean triple is one in which a, b and c are coprime (the greatest common divisor of a, b and c is 1).\nThe following is a list of primitive Pythagorean triples with values less than 100:\n\n(3, 4, 5), (5, 12, 13), (7, 24, 25), (8, 15, 17), (9, 40, 41), (11, 60, 61), (12, 35, 37), (13, 84, 85), (16, 63, 65), (20, 21, 29), (28, 45, 53), (33, 56, 65), (36, 77, 85), (39, 80, 89), (48, 55, 73), (65, 72, 97)\n\n\n=== Inverse Pythagorean theorem ===\nGiven a right triangle with sides \n  \n    \n      \n        a\n        ,\n        b\n        ,\n        c\n      \n    \n    {\\displaystyle a,b,c}\n   and altitude \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n   (a line from the right angle and perpendicular to the hypotenuse \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n  ). The Pythagorean theorem has,\n\n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{2}+b^{2}=c^{2}}\n  while the inverse Pythagorean theorem relates the two legs \n  \n    \n      \n        a\n        ,\n        b\n      \n    \n    {\\displaystyle a,b}\n   to the altitude \n  \n    \n      \n        d\n      \n    \n    {\\displaystyle d}\n  ,\n\n  \n    \n      \n        \n          \n            1\n            \n              a\n              \n                2\n              \n            \n          \n        \n        +\n        \n          \n            1\n            \n              b\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            1\n            \n              d\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{a^{2}}}+{\\frac {1}{b^{2}}}={\\frac {1}{d^{2}}}}\n  The equation can be transformed to,\n\n  \n    \n      \n        \n          \n            1\n            \n              (\n              x\n              z\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        \n          \n            1\n            \n              (\n              y\n              z\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            1\n            \n              (\n              x\n              y\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{(xz)^{2}}}+{\\frac {1}{(yz)^{2}}}={\\frac {1}{(xy)^{2}}}}\n  where \n  \n    \n      \n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            2\n          \n        \n        =\n        \n          z\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle x^{2}+y^{2}=z^{2}}\n   for any non-zero real \n  \n    \n      \n        x\n        ,\n        y\n        ,\n        z\n      \n    \n    {\\displaystyle x,y,z}\n  . If the \n  \n    \n      \n        a\n        ,\n        b\n        ,\n        d\n      \n    \n    {\\displaystyle a,b,d}\n   are to be integers, the smallest solution \n  \n    \n      \n        a\n        >\n        b\n        >\n        d\n      \n    \n    {\\displaystyle a>b>d}\n   is then \n\n  \n    \n      \n        \n          \n            1\n            \n              20\n              \n                2\n              \n            \n          \n        \n        +\n        \n          \n            1\n            \n              15\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            1\n            \n              12\n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{20^{2}}}+{\\frac {1}{15^{2}}}={\\frac {1}{12^{2}}}}\n  using the smallest Pythagorean triple \n  \n    \n      \n        3\n        ,\n        4\n        ,\n        5\n      \n    \n    {\\displaystyle 3,4,5}\n  . The reciprocal Pythagorean theorem is a special case of the optic equation\n\n  \n    \n      \n        \n          \n            1\n            p\n          \n        \n        +\n        \n          \n            1\n            q\n          \n        \n        =\n        \n          \n            1\n            r\n          \n        \n      \n    \n    {\\displaystyle {\\frac {1}{p}}+{\\frac {1}{q}}={\\frac {1}{r}}}\n  where the denominators are squares and also for a heptagonal triangle whose sides \n  \n    \n      \n        p\n        ,\n        q\n        ,\n        r\n      \n    \n    {\\displaystyle p,q,r}\n   are square numbers.\n\n\n=== Incommensurable lengths ===\n\nOne of the consequences of the Pythagorean theorem is that line segments whose lengths are incommensurable (so the ratio of which is not a rational number) can be constructed using a straightedge and compass. Pythagoras' theorem enables construction of incommensurable lengths because the hypotenuse of a triangle is related to the sides by the square root operation.\nThe figure on the right shows how to construct line segments whose lengths are in the ratio of the square root of any positive integer. Each triangle has a side (labeled \"1\") that is the chosen unit for measurement. In each right triangle, Pythagoras' theorem establishes the length of the hypotenuse in terms of this unit. If a hypotenuse is related to the unit by the square root of a positive integer that is not a perfect square, it is a realization of a length incommensurable with the unit, such as √2, √3, √5 . For more detail, see Quadratic irrational.\nIncommensurable lengths conflicted with the Pythagorean school's concept of numbers as only whole numbers. The Pythagorean school dealt with proportions by comparison of integer multiples of a common subunit. According to one legend, Hippasus of Metapontum (ca. 470 B.C.) was drowned at sea for making known the existence of the irrational or incommensurable.\n\n\n=== Complex numbers ===\n\nFor any complex number\n\n  \n    \n      \n        z\n        =\n        x\n        +\n        i\n        y\n        ,\n      \n    \n    {\\displaystyle z=x+iy,}\n  the absolute value or modulus is given by\n\n  \n    \n      \n        r\n        =\n        \n          |\n        \n        z\n        \n          |\n        \n        =\n        \n          \n            \n              x\n              \n                2\n              \n            \n            +\n            \n              y\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle r=|z|={\\sqrt {x^{2}+y^{2}}}.}\n  So the three quantities, r, x and y are related by the Pythagorean equation,\n\n  \n    \n      \n        \n          r\n          \n            2\n          \n        \n        =\n        \n          x\n          \n            2\n          \n        \n        +\n        \n          y\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle r^{2}=x^{2}+y^{2}.}\n  Note that r is defined to be a positive number or zero but x and y can be negative as well as positive. Geometrically r is the distance of the z from zero or the origin O in the complex plane.\nThis can be generalised to find the distance between two points, z1 and z2 say. The required distance is given by\n\n  \n    \n      \n        \n          |\n        \n        \n          z\n          \n            1\n          \n        \n        −\n        \n          z\n          \n            2\n          \n        \n        \n          |\n        \n        =\n        \n          \n            (\n            \n              x\n              \n                1\n              \n            \n            −\n            \n              x\n              \n                2\n              \n            \n            \n              )\n              \n                2\n              \n            \n            +\n            (\n            \n              y\n              \n                1\n              \n            \n            −\n            \n              y\n              \n                2\n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle |z_{1}-z_{2}|={\\sqrt {(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}},}\n  so again they are related by a version of the Pythagorean equation,\n\n  \n    \n      \n        \n          |\n        \n        \n          z\n          \n            1\n          \n        \n        −\n        \n          z\n          \n            2\n          \n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        −\n        \n          x\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        (\n        \n          y\n          \n            1\n          \n        \n        −\n        \n          y\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle |z_{1}-z_{2}|^{2}=(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}.}\n  \n\n\n=== Euclidean distance ===\n\nThe distance formula in Cartesian coordinates is derived from the Pythagorean theorem. If (x1, y1) and (x2, y2) are points in the plane, then the distance between them, also called the Euclidean distance, is given by\n\n  \n    \n      \n        \n          \n            (\n            \n              x\n              \n                1\n              \n            \n            −\n            \n              x\n              \n                2\n              \n            \n            \n              )\n              \n                2\n              \n            \n            +\n            (\n            \n              y\n              \n                1\n              \n            \n            −\n            \n              y\n              \n                2\n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\sqrt {(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}}.}\n  More generally, in Euclidean n-space, the Euclidean distance between two points, \n  \n    \n      \n        A\n        \n        =\n        \n        (\n        \n          a\n          \n            1\n          \n        \n        ,\n        \n          a\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          a\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle A\\,=\\,(a_{1},a_{2},\\dots ,a_{n})}\n   and \n  \n    \n      \n        B\n        \n        =\n        \n        (\n        \n          b\n          \n            1\n          \n        \n        ,\n        \n          b\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          b\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle B\\,=\\,(b_{1},b_{2},\\dots ,b_{n})}\n  , is defined, by generalization of the Pythagorean theorem, as:\n\n  \n    \n      \n        \n          \n            (\n            \n              a\n              \n                1\n              \n            \n            −\n            \n              b\n              \n                1\n              \n            \n            \n              )\n              \n                2\n              \n            \n            +\n            (\n            \n              a\n              \n                2\n              \n            \n            −\n            \n              b\n              \n                2\n              \n            \n            \n              )\n              \n                2\n              \n            \n            +\n            ⋯\n            +\n            (\n            \n              a\n              \n                n\n              \n            \n            −\n            \n              b\n              \n                n\n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            \n              ∑\n              \n                i\n                =\n                1\n              \n              \n                n\n              \n            \n            (\n            \n              a\n              \n                i\n              \n            \n            −\n            \n              b\n              \n                i\n              \n            \n            \n              )\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle {\\sqrt {(a_{1}-b_{1})^{2}+(a_{2}-b_{2})^{2}+\\cdots +(a_{n}-b_{n})^{2}}}={\\sqrt {\\sum _{i=1}^{n}(a_{i}-b_{i})^{2}}}.}\n  If instead of Euclidean distance, the square of this value (the squared Euclidean distance, or SED) is used, the resulting equation avoids square roots and is simply a sum of the SED of the coordinates:\n\n  \n    \n      \n        (\n        \n          a\n          \n            1\n          \n        \n        −\n        \n          b\n          \n            1\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        (\n        \n          a\n          \n            2\n          \n        \n        −\n        \n          b\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        ⋯\n        +\n        (\n        \n          a\n          \n            n\n          \n        \n        −\n        \n          b\n          \n            n\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          a\n          \n            i\n          \n        \n        −\n        \n          b\n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle (a_{1}-b_{1})^{2}+(a_{2}-b_{2})^{2}+\\cdots +(a_{n}-b_{n})^{2}=\\sum _{i=1}^{n}(a_{i}-b_{i})^{2}.}\n  The squared form is a smooth, convex function of both points, and is widely used in optimization theory and statistics, forming the basis of least squares.\n\n\n=== Euclidean distance in other coordinate systems ===\nIf Cartesian coordinates are not used, for example, if polar coordinates are used in two dimensions or, in more general terms, if curvilinear coordinates are used, the formulas expressing the Euclidean distance are more complicated than the Pythagorean theorem, but can be derived from it. A typical example where the straight-line distance between two points is converted to curvilinear coordinates can be found in the applications of Legendre polynomials in physics. The formulas can be discovered by using Pythagoras' theorem with the equations relating the curvilinear coordinates to Cartesian coordinates. For example, the polar coordinates (r, θ) can be introduced as:\n\n  \n    \n      \n        x\n        =\n        r\n        cos\n        ⁡\n        θ\n        ,\n         \n        y\n        =\n        r\n        sin\n        ⁡\n        θ\n        .\n      \n    \n    {\\displaystyle x=r\\cos \\theta ,\\ y=r\\sin \\theta .}\n  Then two points with locations (r1, θ1) and (r2, θ2) are separated by a distance s:\n\n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        −\n        \n          x\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        (\n        \n          y\n          \n            1\n          \n        \n        −\n        \n          y\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        =\n        (\n        \n          r\n          \n            1\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        −\n        \n          r\n          \n            2\n          \n        \n        cos\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        (\n        \n          r\n          \n            1\n          \n        \n        sin\n        ⁡\n        \n          θ\n          \n            1\n          \n        \n        −\n        \n          r\n          \n            2\n          \n        \n        sin\n        ⁡\n        \n          θ\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle s^{2}=(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}=(r_{1}\\cos \\theta _{1}-r_{2}\\cos \\theta _{2})^{2}+(r_{1}\\sin \\theta _{1}-r_{2}\\sin \\theta _{2})^{2}.}\n  Performing the squares and combining terms, the Pythagorean formula for distance in Cartesian coordinates produces the separation in polar coordinates as:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  s\n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                \n                  r\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  r\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                −\n                2\n                \n                  r\n                  \n                    1\n                  \n                \n                \n                  r\n                  \n                    2\n                  \n                \n                \n                  (\n                  \n                    cos\n                    ⁡\n                    \n                      θ\n                      \n                        1\n                      \n                    \n                    cos\n                    ⁡\n                    \n                      θ\n                      \n                        2\n                      \n                    \n                    +\n                    sin\n                    ⁡\n                    \n                      θ\n                      \n                        1\n                      \n                    \n                    sin\n                    ⁡\n                    \n                      θ\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  r\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  r\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                −\n                2\n                \n                  r\n                  \n                    1\n                  \n                \n                \n                  r\n                  \n                    2\n                  \n                \n                cos\n                ⁡\n                \n                  (\n                  \n                    \n                      θ\n                      \n                        1\n                      \n                    \n                    −\n                    \n                      θ\n                      \n                        2\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                \n                  r\n                  \n                    1\n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  r\n                  \n                    2\n                  \n                  \n                    2\n                  \n                \n                −\n                2\n                \n                  r\n                  \n                    1\n                  \n                \n                \n                  r\n                  \n                    2\n                  \n                \n                cos\n                ⁡\n                Δ\n                θ\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}s^{2}&=r_{1}^{2}+r_{2}^{2}-2r_{1}r_{2}\\left(\\cos \\theta _{1}\\cos \\theta _{2}+\\sin \\theta _{1}\\sin \\theta _{2}\\right)\\\\&=r_{1}^{2}+r_{2}^{2}-2r_{1}r_{2}\\cos \\left(\\theta _{1}-\\theta _{2}\\right)\\\\&=r_{1}^{2}+r_{2}^{2}-2r_{1}r_{2}\\cos \\Delta \\theta ,\\end{aligned}}}\n  using the trigonometric product-to-sum formulas. This formula is the law of cosines, sometimes called the generalized Pythagorean theorem. From this result, for the case where the radii to the two locations are at right angles, the enclosed angle Δθ = π/2, and the form corresponding to Pythagoras' theorem is regained: \n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n        =\n        \n          r\n          \n            1\n          \n          \n            2\n          \n        \n        +\n        \n          r\n          \n            2\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle s^{2}=r_{1}^{2}+r_{2}^{2}.}\n   The Pythagorean theorem, valid for right triangles, therefore is a special case of the more general law of cosines, valid for arbitrary triangles.\n\n\n=== Pythagorean trigonometric identity ===\n\nIn a right triangle with sides a, b and hypotenuse c, trigonometry determines the sine and cosine of the angle θ between side a and the hypotenuse as:\n\n  \n    \n      \n        sin\n        ⁡\n        θ\n        =\n        \n          \n            b\n            c\n          \n        \n        ,\n        \n        cos\n        ⁡\n        θ\n        =\n        \n          \n            a\n            c\n          \n        \n        .\n      \n    \n    {\\displaystyle \\sin \\theta ={\\frac {b}{c}},\\quad \\cos \\theta ={\\frac {a}{c}}.}\n  From that it follows:\n\n  \n    \n      \n        \n          \n            cos\n          \n          \n            2\n          \n        \n        θ\n        +\n        \n          \n            sin\n          \n          \n            2\n          \n        \n        θ\n        =\n        \n          \n            \n              \n                a\n                \n                  2\n                \n              \n              +\n              \n                b\n                \n                  2\n                \n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        =\n        1\n        ,\n      \n    \n    {\\displaystyle {\\cos }^{2}\\theta +{\\sin }^{2}\\theta ={\\frac {a^{2}+b^{2}}{c^{2}}}=1,}\n  where the last step applies Pythagoras' theorem. This relation between sine and cosine is sometimes called the fundamental Pythagorean trigonometric identity. In similar triangles, the ratios of the sides are the same regardless of the size of the triangles, and depend upon the angles. Consequently, in the figure, the triangle with hypotenuse of unit size has opposite side of size sin θ and adjacent side of size cos θ in units of the hypotenuse.\n\n\n=== Relation to the cross product ===\n\nThe Pythagorean theorem relates the cross product and dot product in a similar way:\n\n  \n    \n      \n        ‖\n        \n          a\n        \n        ×\n        \n          b\n        \n        \n          ‖\n          \n            2\n          \n        \n        +\n        (\n        \n          a\n        \n        ⋅\n        \n          b\n        \n        \n          )\n          \n            2\n          \n        \n        =\n        ‖\n        \n          a\n        \n        \n          ‖\n          \n            2\n          \n        \n        ‖\n        \n          b\n        \n        \n          ‖\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\|\\mathbf {a} \\times \\mathbf {b} \\|^{2}+(\\mathbf {a} \\cdot \\mathbf {b} )^{2}=\\|\\mathbf {a} \\|^{2}\\|\\mathbf {b} \\|^{2}.}\n  This can be seen from the definitions of the cross product and dot product, as\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  a\n                \n                ×\n                \n                  b\n                \n              \n              \n                \n                =\n                a\n                b\n                \n                  n\n                \n                sin\n                ⁡\n                \n                  θ\n                \n              \n            \n            \n              \n                \n                  a\n                \n                ⋅\n                \n                  b\n                \n              \n              \n                \n                =\n                a\n                b\n                cos\n                ⁡\n                \n                  θ\n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\mathbf {a} \\times \\mathbf {b} &=ab\\mathbf {n} \\sin {\\theta }\\\\\\mathbf {a} \\cdot \\mathbf {b} &=ab\\cos {\\theta },\\end{aligned}}}\n  with n a unit vector normal to both a and b. The relationship follows from these definitions and the Pythagorean trigonometric identity.\nThis can also be used to define the cross product. By rearranging the following equation is obtained\n\n  \n    \n      \n        ‖\n        \n          a\n        \n        ×\n        \n          b\n        \n        \n          ‖\n          \n            2\n          \n        \n        =\n        ‖\n        \n          a\n        \n        \n          ‖\n          \n            2\n          \n        \n        ‖\n        \n          b\n        \n        \n          ‖\n          \n            2\n          \n        \n        −\n        (\n        \n          a\n        \n        ⋅\n        \n          b\n        \n        \n          )\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\|\\mathbf {a} \\times \\mathbf {b} \\|^{2}=\\|\\mathbf {a} \\|^{2}\\|\\mathbf {b} \\|^{2}-(\\mathbf {a} \\cdot \\mathbf {b} )^{2}.}\n  This can be considered as a condition on the cross product and so part of its definition, for example in seven dimensions.\n\n\n== Generalizations ==\n\n\n=== Similar figures on the three sides ===\nThe Pythagorean theorem generalizes beyond the areas of squares on the three sides to  any similar figures. This was known by Hippocrates of Chios in the 5th century BC, and was included by Euclid in his Elements:\nIf one erects similar figures (see Euclidean geometry) with corresponding sides on the sides of a right triangle, then the sum of the areas of the ones on the two smaller sides equals the area of the one on the larger side.\nThis extension assumes that the sides of the original triangle are the corresponding sides of the three congruent figures (so the common ratios of sides between the similar figures are a:b:c). While Euclid's proof only applied to convex polygons, the theorem also applies to concave polygons and even to similar figures that have curved boundaries (but still with part of a figure's boundary being the side of the original triangle).The basic idea behind this generalization is that the area of a plane figure is proportional to the square of any linear dimension, and in particular is proportional to the square of the length of any side. Thus, if similar figures with areas A, B and C are erected on sides with corresponding lengths a, b and c then:\n\n  \n    \n      \n        \n          \n            A\n            \n              a\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            B\n            \n              b\n              \n                2\n              \n            \n          \n        \n        =\n        \n          \n            C\n            \n              c\n              \n                2\n              \n            \n          \n        \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {A}{a^{2}}}={\\frac {B}{b^{2}}}={\\frac {C}{c^{2}}}\\,,}\n  \n\n  \n    \n      \n        ⇒\n        A\n        +\n        B\n        =\n        \n          \n            \n              a\n              \n                2\n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        C\n        +\n        \n          \n            \n              b\n              \n                2\n              \n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        C\n        \n        .\n      \n    \n    {\\displaystyle \\Rightarrow A+B={\\frac {a^{2}}{c^{2}}}C+{\\frac {b^{2}}{c^{2}}}C\\,.}\n  But, by the Pythagorean theorem, a2 + b2 = c2, so A + B = C.\nConversely, if we can prove that A + B = C for three similar figures without using the Pythagorean theorem, then we can work backwards to construct a proof of the theorem. For example, the starting center triangle can be replicated and used as a triangle C on its hypotenuse, and two similar right triangles (A and B ) constructed on the other two sides, formed by dividing the central triangle by its altitude. The sum of the areas of the two smaller triangles therefore is that of the third, thus A + B = C and reversing the above logic leads to the Pythagorean theorem a2 + b2 = c2. (See also Einstein's proof by dissection without rearrangement)\n\n\n=== Law of cosines ===\n\nThe Pythagorean theorem is a special case of the more general theorem relating the lengths of sides in any triangle, the law of cosines:\n\n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        −\n        2\n        a\n        b\n        cos\n        ⁡\n        \n          θ\n        \n        =\n        \n          c\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle a^{2}+b^{2}-2ab\\cos {\\theta }=c^{2},}\n  where \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n   is the angle between sides \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   and \n  \n    \n      \n        b\n      \n    \n    {\\displaystyle b}\n  .\nWhen \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n   is \n  \n    \n      \n        \n          \n            π\n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {\\pi }{2}}}\n   radians or 90°, then \n  \n    \n      \n        cos\n        ⁡\n        \n          θ\n        \n        =\n        0\n      \n    \n    {\\displaystyle \\cos {\\theta }=0}\n  , and the formula reduces to the usual Pythagorean theorem.\n\n\n=== Arbitrary triangle ===\n\nAt any selected angle of a general triangle of sides a, b, c, inscribe an isosceles triangle such that the equal angles at its base θ are the same as the selected angle. Suppose the selected angle θ is opposite the side labeled c. Inscribing the isosceles triangle forms triangle CAD with angle θ opposite side b and with side r along c. A second triangle is formed with angle θ opposite side a and a side with length s along c, as shown in the figure. Thābit ibn Qurra stated that the sides of the three triangles were related as:\n\n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        c\n        (\n        r\n        +\n        s\n        )\n         \n        .\n      \n    \n    {\\displaystyle a^{2}+b^{2}=c(r+s)\\ .}\n  As the angle θ approaches π/2, the base of the isosceles triangle narrows, and lengths r and s overlap less and less. When θ = π/2, ADB becomes a right triangle, r + s = c, and the original Pythagorean theorem is regained.\nOne proof observes that triangle ABC has the same angles as triangle CAD, but in opposite order. (The two triangles share the angle at vertex A, both contain the angle θ, and so also have the same third angle by the triangle postulate.) Consequently, ABC is similar to the reflection of CAD, the triangle DAC in the lower panel. Taking the ratio of sides opposite and adjacent to θ,\n\n  \n    \n      \n        \n          \n            c\n            b\n          \n        \n        =\n        \n          \n            b\n            r\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle {\\frac {c}{b}}={\\frac {b}{r}}\\ .}\n  Likewise, for the reflection of the other triangle,\n\n  \n    \n      \n        \n          \n            c\n            a\n          \n        \n        =\n        \n          \n            a\n            s\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle {\\frac {c}{a}}={\\frac {a}{s}}\\ .}\n  Clearing fractions and adding these two relations:\n\n  \n    \n      \n        c\n        s\n        +\n        c\n        r\n        =\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle cs+cr=a^{2}+b^{2}\\ ,}\n  the required result.\nThe theorem remains valid if the angle \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n   is obtuse so the lengths r and s are non-overlapping.\n\n\n=== General triangles using parallelograms ===\n\nPappus's area theorem is a further generalization, that applies to triangles that are not right triangles, using parallelograms on the three sides in place of squares (squares are a special case, of course). The upper figure shows that for a scalene triangle, the area of the parallelogram on the longest side is the sum of the areas of the parallelograms on the other two sides, provided the parallelogram on the long side is constructed as indicated (the dimensions labeled with arrows are the same, and determine the sides of the bottom parallelogram). This replacement of squares with parallelograms bears a clear resemblance to the original Pythagoras' theorem, and was considered a generalization by Pappus of Alexandria in 4 ADThe lower figure shows the elements of the proof. Focus on the left side of the figure. The left green parallelogram has the same area as the left, blue portion of the bottom parallelogram because both have the same base b and height h. However, the left green parallelogram also has the same area as the left green parallelogram of the upper figure, because they have the same base (the upper left side of the triangle) and the same height normal to that side of the triangle. Repeating the argument for the right side of the figure, the bottom parallelogram has the same area as the sum of the two green parallelograms.\n\n\n=== Solid geometry ===\n\nIn terms of solid geometry, Pythagoras' theorem can be applied to three dimensions as follows. Consider a rectangular solid as shown in the figure. The length of diagonal BD is found from Pythagoras' theorem as:\n\n  \n    \n      \n        \n          \n            \n              \n                B\n                D\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n        =\n        \n          \n            \n              \n                B\n                C\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n        +\n        \n          \n            \n              \n                C\n                D\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle {\\overline {BD}}^{\\,2}={\\overline {BC}}^{\\,2}+{\\overline {CD}}^{\\,2}\\ ,}\n  where these three sides form a right triangle. Using horizontal diagonal BD and the vertical edge AB, the length of diagonal AD then is found by a second application of Pythagoras' theorem as:\n\n  \n    \n      \n        \n          \n            \n              \n                A\n                D\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n        =\n        \n          \n            \n              \n                A\n                B\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n        +\n        \n          \n            \n              \n                B\n                D\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle {\\overline {AD}}^{\\,2}={\\overline {AB}}^{\\,2}+{\\overline {BD}}^{\\,2}\\ ,}\n  or, doing it all in one step:\n\n  \n    \n      \n        \n          \n            \n              \n                A\n                D\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n        =\n        \n          \n            \n              \n                A\n                B\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n        +\n        \n          \n            \n              \n                B\n                C\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n        +\n        \n          \n            \n              \n                C\n                D\n              \n              ¯\n            \n          \n          \n            \n            2\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle {\\overline {AD}}^{\\,2}={\\overline {AB}}^{\\,2}+{\\overline {BC}}^{\\,2}+{\\overline {CD}}^{\\,2}\\ .}\n  This result is the three-dimensional expression for the magnitude of a vector v (the diagonal AD) in terms of its orthogonal components {vk} (the three mutually perpendicular sides):\n\n  \n    \n      \n        ‖\n        \n          v\n        \n        \n          ‖\n          \n            2\n          \n        \n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            3\n          \n        \n        ‖\n        \n          \n            v\n          \n          \n            k\n          \n        \n        \n          ‖\n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\|\\mathbf {v} \\|^{2}=\\sum _{k=1}^{3}\\|\\mathbf {v} _{k}\\|^{2}.}\n  This one-step formulation may be viewed as a generalization of Pythagoras' theorem to higher dimensions. However, this result is really just the repeated application of the original Pythagoras' theorem to a succession of right triangles in a sequence of orthogonal planes.\nA substantial generalization of the Pythagorean theorem to three dimensions is de Gua's theorem, named for Jean Paul de Gua de Malves: If a tetrahedron has a right angle corner (like a corner of a cube), then the square of the area of the face opposite the right angle corner is the sum of the squares of the areas of the other three faces. This result can be generalized as in the \"n-dimensional Pythagorean theorem\":\nLet \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},x_{2},\\ldots ,x_{n}}\n   be orthogonal vectors in Rn. Consider the n-dimensional simplex S with vertices \n  \n    \n      \n        0\n        ,\n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle 0,x_{1},\\ldots ,x_{n}}\n  . (Think of the (n − 1)-dimensional simplex with vertices \n  \n    \n      \n        \n          x\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          x\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{1},\\ldots ,x_{n}}\n   not including the origin as the \"hypotenuse\" of S and the remaining (n − 1)-dimensional faces of S as its \"legs\".) Then the square of the volume of the hypotenuse of S is the sum of the squares of the volumes of the n legs.\nThis statement is illustrated in three dimensions by the tetrahedron in the figure. The \"hypotenuse\" is the base of the tetrahedron at the back of the figure, and the \"legs\" are the three sides emanating from the vertex in the foreground. As the depth of the base from the vertex increases, the area of the \"legs\" increases, while that of the base is fixed. The theorem suggests that when this depth is at the value creating a right vertex, the generalization of Pythagoras' theorem applies. In a different wording:\nGiven an n-rectangular n-dimensional simplex, the square of the (n − 1)-content of the facet opposing the right vertex will equal the sum of the squares of the (n − 1)-contents of the remaining facets.\n\n\n=== Inner product spaces ===\n\nThe Pythagorean theorem can be generalized to inner product spaces, which are generalizations of the familiar 2-dimensional and 3-dimensional Euclidean spaces. For example, a function may be considered as a vector with infinitely many components in an inner product space, as in functional analysis.In an inner product space, the concept of perpendicularity is replaced by the concept of orthogonality: two vectors v and w are orthogonal if their inner product \n  \n    \n      \n        ⟨\n        \n          v\n        \n        ,\n        \n          w\n        \n        ⟩\n      \n    \n    {\\displaystyle \\langle \\mathbf {v} ,\\mathbf {w} \\rangle }\n   is zero. The inner product is a generalization of the dot product of vectors. The dot product is called the standard inner product or the Euclidean inner product. However, other inner products are possible.The concept of length is replaced by the concept of the norm ||v|| of a vector v, defined as:\n\n  \n    \n      \n        ‖\n        \n          v\n        \n        ‖\n        ≡\n        \n          \n            ⟨\n            \n              v\n            \n            ,\n            \n              v\n            \n            ⟩\n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\lVert \\mathbf {v} \\rVert \\equiv {\\sqrt {\\langle \\mathbf {v} ,\\mathbf {v} \\rangle }}\\,.}\n  In an inner-product space, the Pythagorean theorem states that for any two orthogonal vectors v and w we have\n\n  \n    \n      \n        \n          \n            ‖\n            \n              \n                v\n              \n              +\n              \n                w\n              \n            \n            ‖\n          \n          \n            2\n          \n        \n        =\n        \n          \n            ‖\n            \n              v\n            \n            ‖\n          \n          \n            2\n          \n        \n        +\n        \n          \n            ‖\n            \n              w\n            \n            ‖\n          \n          \n            2\n          \n        \n        .\n      \n    \n    {\\displaystyle \\left\\|\\mathbf {v} +\\mathbf {w} \\right\\|^{2}=\\left\\|\\mathbf {v} \\right\\|^{2}+\\left\\|\\mathbf {w} \\right\\|^{2}.}\n  Here the vectors v and w are akin to the sides of a right triangle with hypotenuse given by the vector sum v + w. This form of the Pythagorean theorem is a consequence of the properties of the inner product:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    ‖\n                    \n                      \n                        v\n                      \n                      +\n                      \n                        w\n                      \n                    \n                    ‖\n                  \n                  \n                    2\n                  \n                \n              \n              \n                \n                =\n                ⟨\n                \n                  v\n                  +\n                  w\n                \n                ,\n                 \n                \n                  v\n                  +\n                  w\n                \n                ⟩\n              \n            \n            \n              \n              \n                \n                =\n                ⟨\n                \n                  v\n                \n                ,\n                 \n                \n                  v\n                \n                ⟩\n                +\n                ⟨\n                \n                  w\n                \n                ,\n                 \n                \n                  w\n                \n                ⟩\n                +\n                ⟨\n                \n                  v\n                  ,\n                   \n                  w\n                \n                ⟩\n                +\n                ⟨\n                \n                  w\n                  ,\n                   \n                  v\n                \n                ⟩\n              \n            \n            \n              \n              \n                \n                =\n                \n                  \n                    ‖\n                    \n                      v\n                    \n                    ‖\n                  \n                  \n                    2\n                  \n                \n                +\n                \n                  \n                    ‖\n                    \n                      w\n                    \n                    ‖\n                  \n                  \n                    2\n                  \n                \n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\left\\|\\mathbf {v} +\\mathbf {w} \\right\\|^{2}&=\\langle \\mathbf {v+w} ,\\ \\mathbf {v+w} \\rangle \\\\[3mu]&=\\langle \\mathbf {v} ,\\ \\mathbf {v} \\rangle +\\langle \\mathbf {w} ,\\ \\mathbf {w} \\rangle +\\langle \\mathbf {v,\\ w} \\rangle +\\langle \\mathbf {w,\\ v} \\rangle \\\\[3mu]&=\\left\\|\\mathbf {v} \\right\\|^{2}+\\left\\|\\mathbf {w} \\right\\|^{2},\\end{aligned}}}\n  where \n  \n    \n      \n        ⟨\n        \n          v\n          ,\n           \n          w\n        \n        ⟩\n        =\n        ⟨\n        \n          w\n          ,\n           \n          v\n        \n        ⟩\n        =\n        0\n      \n    \n    {\\displaystyle \\langle \\mathbf {v,\\ w} \\rangle =\\langle \\mathbf {w,\\ v} \\rangle =0}\n   because of orthogonality.\nA further generalization of the Pythagorean theorem in an inner product space to non-orthogonal vectors is the parallelogram law :\n\n  \n    \n      \n        2\n        ‖\n        \n          v\n        \n        \n          ‖\n          \n            2\n          \n        \n        +\n        2\n        ‖\n        \n          w\n        \n        \n          ‖\n          \n            2\n          \n        \n        =\n        ‖\n        \n          v\n          +\n          w\n        \n        \n          ‖\n          \n            2\n          \n        \n        +\n        ‖\n        \n          v\n          −\n          w\n        \n        \n          ‖\n          \n            2\n          \n        \n         \n        ,\n      \n    \n    {\\displaystyle 2\\|\\mathbf {v} \\|^{2}+2\\|\\mathbf {w} \\|^{2}=\\|\\mathbf {v+w} \\|^{2}+\\|\\mathbf {v-w} \\|^{2}\\ ,}\n  which says that twice the sum of the squares of the lengths of the sides of a parallelogram is the sum of the squares of the lengths of the diagonals. Any norm that satisfies this equality is ipso facto a norm corresponding to an inner product.\nThe Pythagorean identity can be extended to sums of more than two orthogonal vectors. If v1, v2, ..., vn are pairwise-orthogonal vectors in an inner-product space, then application of the Pythagorean theorem to successive pairs of these vectors (as described for 3-dimensions in the section on solid geometry) results in the equation\n\n  \n    \n      \n        \n          \n            ‖\n            \n              \n                ∑\n                \n                  k\n                  =\n                  1\n                \n                \n                  n\n                \n              \n              \n                \n                  v\n                \n                \n                  k\n                \n              \n            \n            ‖\n          \n          \n            2\n          \n        \n        =\n        \n          ∑\n          \n            k\n            =\n            1\n          \n          \n            n\n          \n        \n        ‖\n        \n          \n            v\n          \n          \n            k\n          \n        \n        \n          ‖\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\left\\|\\sum _{k=1}^{n}\\mathbf {v} _{k}\\right\\|^{2}=\\sum _{k=1}^{n}\\|\\mathbf {v} _{k}\\|^{2}}\n  \n\n\n=== Sets of m-dimensional objects in n-dimensional space ===\nAnother generalization of the Pythagorean theorem applies to Lebesgue-measurable sets of objects in any number of dimensions.  Specifically, the square of the measure of an m-dimensional set of objects in one or more parallel m-dimensional flats in n-dimensional Euclidean space is equal to the sum of the squares of the measures of the orthogonal projections of the object(s) onto all m-dimensional coordinate subspaces.In mathematical terms:\n\n  \n    \n      \n        \n          μ\n          \n            m\n            s\n          \n          \n            2\n          \n        \n        =\n        \n          ∑\n          \n            i\n            =\n            1\n          \n          \n            x\n          \n        \n        \n          \n            \n              μ\n              \n                2\n              \n            \n          \n          \n            m\n            \n              p\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mu _{ms}^{2}=\\sum _{i=1}^{x}\\mathbf {\\mu ^{2}} _{mp_{i}}}\n  where:\n\n  \n    \n      \n        \n          μ\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle \\mu _{m}}\n   is a measure in m-dimensions (a length in one dimension, an area in two dimensions, a volume in three dimensions, etc.).\n\n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   is a set of one or more non-overlapping m-dimensional objects in one or more parallel m-dimensional flats in n-dimensional Euclidean space.\n\n  \n    \n      \n        \n          μ\n          \n            m\n            s\n          \n        \n      \n    \n    {\\displaystyle \\mu _{ms}}\n   is the total measure (sum) of the set of m-dimensional objects.\n\n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   represents an m-dimensional projection of the original set onto an orthogonal coordinate subspace.\n\n  \n    \n      \n        \n          μ\n          \n            m\n            \n              p\n              \n                i\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mu _{mp_{i}}}\n   is the measure of the m-dimensional set projection onto m-dimensional coordinate subspace \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  .  Because object projections can overlap on a coordinate subspace, the measure of each object projection in the set must be calculated individually, then measures of all projections added together to provide the total measure for the set of projections on the given coordinate subspace.\n\n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is the number of orthogonal, m-dimensional coordinate subspaces in n-dimensional space (Rn) onto which the m-dimensional objects are projected (m ≤ n): \n\n\n=== Non-Euclidean geometry ===\nThe Pythagorean theorem is derived from the axioms of Euclidean geometry, and in fact, were the Pythagorean theorem to fail for some right triangle, then the plane in which this triangle is contained cannot be Euclidean. More precisely, the Pythagorean theorem implies, and is implied by, Euclid's Parallel (Fifth) Postulate. Thus, right triangles in a non-Euclidean geometry\ndo not satisfy the Pythagorean theorem. For example, in spherical geometry, all three sides of the right triangle (say a, b, and c) bounding an octant of the unit sphere have length equal to π/2, and all its angles are right angles, which violates the Pythagorean theorem because \n\n  \n    \n      \n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        =\n        2\n        \n          c\n          \n            2\n          \n        \n        >\n        \n          c\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle a^{2}+b^{2}=2c^{2}>c^{2}}\n  .\nHere two cases of non-Euclidean geometry are considered—spherical geometry and hyperbolic plane geometry; in each case, as in the Euclidean case for non-right triangles, the result replacing the Pythagorean theorem follows from the appropriate law of cosines.\nHowever, the Pythagorean theorem remains true in hyperbolic geometry and elliptic geometry if the condition that the triangle be right is replaced with the condition that two of the angles sum to the third, say A+B = C. The sides are then related as follows: the sum of the areas of the circles with diameters a and b equals the area of the circle with diameter c.\n\n\n==== Spherical geometry ====\n\nFor any right triangle on a sphere of radius R (for example, if γ in the figure is a right angle), with sides a, b, c, the relation between the sides takes the form:\n\n  \n    \n      \n        cos\n        ⁡\n        \n          \n            c\n            R\n          \n        \n        =\n        cos\n        ⁡\n        \n          \n            a\n            R\n          \n        \n        \n        cos\n        ⁡\n        \n          \n            b\n            R\n          \n        \n        .\n      \n    \n    {\\displaystyle \\cos {\\frac {c}{R}}=\\cos {\\frac {a}{R}}\\,\\cos {\\frac {b}{R}}.}\n  This equation can be derived as a special case of the spherical law of cosines that applies to all spherical triangles:\n\n  \n    \n      \n        cos\n        ⁡\n        \n          \n            c\n            R\n          \n        \n        =\n        cos\n        ⁡\n        \n          \n            a\n            R\n          \n        \n        \n        cos\n        ⁡\n        \n          \n            b\n            R\n          \n        \n        +\n        sin\n        ⁡\n        \n          \n            a\n            R\n          \n        \n        \n        sin\n        ⁡\n        \n          \n            b\n            R\n          \n        \n        \n        cos\n        ⁡\n        \n          γ\n        \n        .\n      \n    \n    {\\displaystyle \\cos {\\frac {c}{R}}=\\cos {\\frac {a}{R}}\\,\\cos {\\frac {b}{R}}+\\sin {\\frac {a}{R}}\\,\\sin {\\frac {b}{R}}\\,\\cos {\\gamma }.}\n  For infinitesimal triangles on the sphere (or equivalently, for finite spherical triangles on a sphere of infinite radius), the spherical relation between the sides of a right triangle reduces to the Euclidean form of the Pythagorean theorem. To see how, assume we have a spherical triangle of fixed side lengths a, b, and c on a sphere with expanding radius R. As R approaches infinity the quantities a/R, b/R, and c/R tend to zero and the spherical Pythagorean identity reduces to \n  \n    \n      \n        1\n        =\n        1\n        ,\n      \n    \n    {\\displaystyle 1=1,}\n   so we must look at its asymptotic expansion.\nThe Maclaurin series for the cosine function can be written as \n  \n    \n      \n        cos\n        ⁡\n        x\n        =\n        1\n        −\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          x\n          \n            2\n          \n        \n        +\n        O\n        \n          \n            (\n            \n              x\n              \n                4\n              \n            \n            )\n          \n        \n      \n    \n    {\\textstyle \\cos x=1-{\\tfrac {1}{2}}x^{2}+O{\\left(x^{4}\\right)}}\n   with the remainder term in big O notation. Letting \n  \n    \n      \n        x\n        =\n        c\n        \n          /\n        \n        R\n      \n    \n    {\\displaystyle x=c/R}\n   be a side of the triangle, and treating the expression as an asymptotic expansion in terms of R for a fixed c,\n\n  \n    \n      \n        \n          \n            \n              \n                cos\n                ⁡\n                \n                  \n                    c\n                    R\n                  \n                \n                =\n                1\n                −\n                \n                  \n                    \n                      c\n                      \n                        2\n                      \n                    \n                    \n                      2\n                      \n                        R\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                +\n                O\n                \n                  \n                    (\n                    \n                      R\n                      \n                        −\n                        4\n                      \n                    \n                    )\n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\cos {\\frac {c}{R}}=1-{\\frac {c^{2}}{2R^{2}}}+O{\\left(R^{-4}\\right)}\\end{aligned}}}\n  and likewise for a and b. Substituting the asymptotic expansion for each of the cosines into the spherical relation for a right triangle yields\n\n  \n    \n      \n        \n          \n            \n              \n                1\n                −\n                \n                  \n                    \n                      c\n                      \n                        2\n                      \n                    \n                    \n                      2\n                      \n                        R\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                +\n                O\n                \n                  \n                    (\n                    \n                      R\n                      \n                        −\n                        4\n                      \n                    \n                    )\n                  \n                \n              \n              \n                \n                =\n                \n                  (\n                  \n                    1\n                    −\n                    \n                      \n                        \n                          a\n                          \n                            2\n                          \n                        \n                        \n                          2\n                          \n                            R\n                            \n                              2\n                            \n                          \n                        \n                      \n                    \n                    +\n                    O\n                    \n                      \n                        (\n                        \n                          R\n                          \n                            −\n                            4\n                          \n                        \n                        )\n                      \n                    \n                  \n                  )\n                \n                \n                  (\n                  \n                    1\n                    −\n                    \n                      \n                        \n                          b\n                          \n                            2\n                          \n                        \n                        \n                          2\n                          \n                            R\n                            \n                              2\n                            \n                          \n                        \n                      \n                    \n                    +\n                    O\n                    \n                      \n                        (\n                        \n                          R\n                          \n                            −\n                            4\n                          \n                        \n                        )\n                      \n                    \n                  \n                  )\n                \n              \n            \n            \n              \n              \n                \n                =\n                1\n                −\n                \n                  \n                    \n                      a\n                      \n                        2\n                      \n                    \n                    \n                      2\n                      \n                        R\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                −\n                \n                  \n                    \n                      b\n                      \n                        2\n                      \n                    \n                    \n                      2\n                      \n                        R\n                        \n                          2\n                        \n                      \n                    \n                  \n                \n                +\n                O\n                \n                  \n                    (\n                    \n                      R\n                      \n                        −\n                        4\n                      \n                    \n                    )\n                  \n                \n                .\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}1-{\\frac {c^{2}}{2R^{2}}}+O{\\left(R^{-4}\\right)}&=\\left(1-{\\frac {a^{2}}{2R^{2}}}+O{\\left(R^{-4}\\right)}\\right)\\left(1-{\\frac {b^{2}}{2R^{2}}}+O{\\left(R^{-4}\\right)}\\right)\\\\&=1-{\\frac {a^{2}}{2R^{2}}}-{\\frac {b^{2}}{2R^{2}}}+O{\\left(R^{-4}\\right)}.\\end{aligned}}}\n  Subtracting 1 and then negating each side,\n\n  \n    \n      \n        \n          \n            \n              c\n              \n                2\n              \n            \n            \n              2\n              \n                R\n                \n                  2\n                \n              \n            \n          \n        \n        =\n        \n          \n            \n              a\n              \n                2\n              \n            \n            \n              2\n              \n                R\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              b\n              \n                2\n              \n            \n            \n              2\n              \n                R\n                \n                  2\n                \n              \n            \n          \n        \n        +\n        O\n        \n          \n            (\n            \n              R\n              \n                −\n                4\n              \n            \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\frac {c^{2}}{2R^{2}}}={\\frac {a^{2}}{2R^{2}}}+{\\frac {b^{2}}{2R^{2}}}+O{\\left(R^{-4}\\right)}.}\n  Multiplying through by 2R2, the asymptotic expansion for c in terms of fixed a, b and variable R is\n\n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        =\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        +\n        O\n        \n          \n            (\n            \n              R\n              \n                −\n                2\n              \n            \n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle c^{2}=a^{2}+b^{2}+O{\\left(R^{-2}\\right)}.}\n  The Euclidean Pythagorean relationship \n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        =\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n      \n    \n    {\\textstyle c^{2}=a^{2}+b^{2}}\n   is recovered in the limit, as the remainder vanishes when the radius R approaches infinity.\nFor practical computation in spherical trigonometry with small right triangles, cosines can be replaced with sines using the double-angle identity \n  \n    \n      \n        cos\n        ⁡\n        \n          2\n          θ\n        \n        =\n        1\n        −\n        2\n        \n          sin\n          \n            2\n          \n        \n        ⁡\n        \n          θ\n        \n      \n    \n    {\\displaystyle \\cos {2\\theta }=1-2\\sin ^{2}{\\theta }}\n   to avoid loss of significance. Then the spherical Pythagorean theorem can alternately be written as\n\n  \n    \n      \n        \n          sin\n          \n            2\n          \n        \n        ⁡\n        \n          \n            c\n            \n              2\n              R\n            \n          \n        \n        =\n        \n          sin\n          \n            2\n          \n        \n        ⁡\n        \n          \n            a\n            \n              2\n              R\n            \n          \n        \n        +\n        \n          sin\n          \n            2\n          \n        \n        ⁡\n        \n          \n            b\n            \n              2\n              R\n            \n          \n        \n        −\n        2\n        \n          sin\n          \n            2\n          \n        \n        ⁡\n        \n          \n            a\n            \n              2\n              R\n            \n          \n        \n        \n        \n          sin\n          \n            2\n          \n        \n        ⁡\n        \n          \n            b\n            \n              2\n              R\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\sin ^{2}{\\frac {c}{2R}}=\\sin ^{2}{\\frac {a}{2R}}+\\sin ^{2}{\\frac {b}{2R}}-2\\sin ^{2}{\\frac {a}{2R}}\\,\\sin ^{2}{\\frac {b}{2R}}.}\n  \n\n\n==== Hyperbolic geometry ====\n\nIn a hyperbolic space with uniform Gaussian curvature −1/R2, for a right triangle with legs a, b, and hypotenuse c, the relation between the sides takes the form:\n\n  \n    \n      \n        cosh\n        ⁡\n        \n          \n            c\n            R\n          \n        \n        =\n        cosh\n        ⁡\n        \n          \n            a\n            R\n          \n        \n        \n        cosh\n        ⁡\n        \n          \n            b\n            R\n          \n        \n      \n    \n    {\\displaystyle \\cosh {\\frac {c}{R}}=\\cosh {\\frac {a}{R}}\\,\\cosh {\\frac {b}{R}}}\n  where cosh is the hyperbolic cosine. This formula is a special form of the hyperbolic law of cosines that applies to all hyperbolic triangles:\n\n  \n    \n      \n        cosh\n        ⁡\n        \n          \n            c\n            R\n          \n        \n        =\n        cosh\n        ⁡\n        \n          \n            a\n            R\n          \n        \n         \n        cosh\n        ⁡\n        \n          \n            b\n            R\n          \n        \n        −\n        sinh\n        ⁡\n        \n          \n            a\n            R\n          \n        \n         \n        sinh\n        ⁡\n        \n          \n            b\n            R\n          \n        \n         \n        cos\n        ⁡\n        γ\n         \n        ,\n      \n    \n    {\\displaystyle \\cosh {\\frac {c}{R}}=\\cosh {\\frac {a}{R}}\\ \\cosh {\\frac {b}{R}}-\\sinh {\\frac {a}{R}}\\ \\sinh {\\frac {b}{R}}\\ \\cos \\gamma \\ ,}\n  with γ the angle at the vertex opposite the side c.\nBy using the Maclaurin series for the hyperbolic cosine, cosh x ≈ 1 + x2/2, it can be shown that as a hyperbolic triangle becomes very small (that is, as a, b, and c all approach zero), the hyperbolic relation for a right triangle approaches the form of Pythagoras' theorem.\nFor small right triangles (a, b << R), the hyperbolic cosines can be eliminated to avoid loss of significance, giving\n\n  \n    \n      \n        \n          sinh\n          \n            2\n          \n        \n        ⁡\n        \n          \n            c\n            \n              2\n              R\n            \n          \n        \n        =\n        \n          sinh\n          \n            2\n          \n        \n        ⁡\n        \n          \n            a\n            \n              2\n              R\n            \n          \n        \n        +\n        \n          sinh\n          \n            2\n          \n        \n        ⁡\n        \n          \n            b\n            \n              2\n              R\n            \n          \n        \n        +\n        2\n        \n          sinh\n          \n            2\n          \n        \n        ⁡\n        \n          \n            a\n            \n              2\n              R\n            \n          \n        \n        \n          sinh\n          \n            2\n          \n        \n        ⁡\n        \n          \n            b\n            \n              2\n              R\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle \\sinh ^{2}{\\frac {c}{2R}}=\\sinh ^{2}{\\frac {a}{2R}}+\\sinh ^{2}{\\frac {b}{2R}}+2\\sinh ^{2}{\\frac {a}{2R}}\\sinh ^{2}{\\frac {b}{2R}}\\,.}\n  \n\n\n==== Very small triangles ====\nFor any uniform curvature K (positive, zero, or negative), in very small right triangles (|K|a2, |K|b2 << 1) with hypotenuse c, it can be shown that\n\n  \n    \n      \n        \n          c\n          \n            2\n          \n        \n        =\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        −\n        \n          \n            K\n            3\n          \n        \n        \n          a\n          \n            2\n          \n        \n        \n          b\n          \n            2\n          \n        \n        −\n        \n          \n            \n              K\n              \n                2\n              \n            \n            45\n          \n        \n        \n          a\n          \n            2\n          \n        \n        \n          b\n          \n            2\n          \n        \n        (\n        \n          a\n          \n            2\n          \n        \n        +\n        \n          b\n          \n            2\n          \n        \n        )\n        −\n        \n          \n            \n              2\n              \n                K\n                \n                  3\n                \n              \n            \n            945\n          \n        \n        \n          a\n          \n            2\n          \n        \n        \n          b\n          \n            2\n          \n        \n        (\n        \n          a\n          \n            2\n          \n        \n        −\n        \n          b\n          \n            2\n          \n        \n        \n          )\n          \n            2\n          \n        \n        +\n        O\n        (\n        \n          K\n          \n            4\n          \n        \n        \n          c\n          \n            10\n          \n        \n        )\n        \n        .\n      \n    \n    {\\displaystyle c^{2}=a^{2}+b^{2}-{\\frac {K}{3}}a^{2}b^{2}-{\\frac {K^{2}}{45}}a^{2}b^{2}(a^{2}+b^{2})-{\\frac {2K^{3}}{945}}a^{2}b^{2}(a^{2}-b^{2})^{2}+O(K^{4}c^{10})\\,.}\n  \n\n\n=== Differential geometry ===\n\nThe Pythagorean theorem applies to infinitesimal triangles seen in differential geometry. In three dimensional space, the distance between two infinitesimally separated points satisfies\n\n  \n    \n      \n        d\n        \n          s\n          \n            2\n          \n        \n        =\n        d\n        \n          x\n          \n            2\n          \n        \n        +\n        d\n        \n          y\n          \n            2\n          \n        \n        +\n        d\n        \n          z\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle ds^{2}=dx^{2}+dy^{2}+dz^{2},}\n  with ds the element of distance and (dx, dy, dz) the components of the vector separating the two points. Such a space is called a Euclidean space. However, in Riemannian geometry, a generalization of this expression useful for general coordinates (not just Cartesian) and general spaces (not just Euclidean) takes the form:\n\n  \n    \n      \n        d\n        \n          s\n          \n            2\n          \n        \n        =\n        \n          ∑\n          \n            i\n            ,\n            j\n          \n          \n            n\n          \n        \n        \n          g\n          \n            i\n            j\n          \n        \n        \n        d\n        \n          x\n          \n            i\n          \n        \n        \n        d\n        \n          x\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle ds^{2}=\\sum _{i,j}^{n}g_{ij}\\,dx_{i}\\,dx_{j}}\n  which is called the metric tensor. (Sometimes, by abuse of language, the same term is applied to the set of coefficients gij.) It may be a function of position, and often describes curved space. A simple example is Euclidean (flat) space expressed in curvilinear coordinates. For example, in polar coordinates:\n\n  \n    \n      \n        d\n        \n          s\n          \n            2\n          \n        \n        =\n        d\n        \n          r\n          \n            2\n          \n        \n        +\n        \n          r\n          \n            2\n          \n        \n        d\n        \n          θ\n          \n            2\n          \n        \n         \n        .\n      \n    \n    {\\displaystyle ds^{2}=dr^{2}+r^{2}d\\theta ^{2}\\ .}\n  \n\n\n== History ==\n\nThere is debate whether the Pythagorean theorem was discovered once, or many times in many places, and the date of first discovery is uncertain, as is the date of the first proof.  Historians of Mesopotamian mathematics have concluded that the Pythagorean rule was in widespread use during the Old Babylonian period (20th to 16th centuries BC), over a thousand years before Pythagoras was born. The history of the theorem can be divided into four parts: knowledge of Pythagorean triples, knowledge of the relationship among the sides of a right triangle, knowledge of the relationships among adjacent angles, and proofs of the theorem within some deductive system.\nWritten c. 1800 BC, the Egyptian Middle Kingdom Berlin Papyrus 6619 includes a problem whose solution is the Pythagorean triple 6:8:10, but the problem does not mention a triangle. The Mesopotamian tablet Plimpton 322, also written c. 1800 BC near Larsa, contains many entries closely related to Pythagorean triples.In India, the Baudhayana Shulba Sutra, the dates of which are given variously as between the 8th and 5th century BC, contains a list of Pythagorean triples and a statement of the Pythagorean theorem, both in the special case of the isosceles right triangle and in the general case, as does the Apastamba Shulba Sutra (c. 600 BC).Byzantine Neoplatonic philosopher and mathematician Proclus, writing in the fifth century AD, states two arithmetic rules, \"one of them attributed to Plato, the other to Pythagoras\", for generating special Pythagorean triples.  The rule attributed to Pythagoras (c. 570 – c. 495 BC) starts from an odd number and produces a triple with leg and hypotenuse differing by one unit; the rule attributed to Plato (428/427 or 424/423 – 348/347 BC) starts from an even number and produces a triple with leg and hypotenuse differing by two units. According to Thomas L. Heath (1861–1940), no specific attribution of the theorem to Pythagoras exists in the surviving Greek literature from the five centuries after Pythagoras lived. However, when authors such as Plutarch and Cicero attributed the theorem to Pythagoras, they did so in a way which suggests that the attribution was widely known and undoubted. Classicist Kurt von Fritz wrote, \"Whether this formula is rightly attributed to Pythagoras personally, but one can safely assume that it belongs to the very oldest period of Pythagorean mathematics.\" Around 300 BC, in Euclid's Elements, the oldest extant axiomatic proof of the theorem is presented.\n\nWith contents known much earlier, but in surviving texts dating from roughly the 1st century BC, the Chinese text Zhoubi Suanjing (周髀算经), (The Arithmetical Classic of the Gnomon and the Circular Paths of Heaven) gives a reasoning for the Pythagorean theorem for the (3, 4, 5) triangle — in China it is called the \"Gougu theorem\" (勾股定理). During the Han Dynasty (202 BC to 220 AD), Pythagorean triples appear in The Nine Chapters on the Mathematical Art, together with a mention of right triangles. Some believe the theorem arose first in China, where it is alternatively known as the \"Shang Gao theorem\" (商高定理), named after the Duke of Zhou's astronomer and mathematician, whose reasoning composed most of what was in the Zhoubi Suanjing.\n\n\n== See also ==\n\n\n== Notes and references ==\n\n\n=== Notes ===\n\n\n=== References ===\n\n\n=== Works cited ===\n\n\n== External links ==\nPythagorean theorem at ProofWiki\nEuclid (1997) [c. 300 BC].  David E. Joyce (ed.). Elements. Retrieved 2006-08-30. In HTML with Java-based interactive figures.\n\"Pythagorean theorem\". Encyclopedia of Mathematics. EMS Press. 2001 [1994].\nHistory topic: Pythagoras's theorem in Babylonian mathematics\nInteractive links:\nInteractive proof in Java of the Pythagorean theorem\nAnother interactive proof in Java of the Pythagorean theorem\nPythagorean theorem with interactive animation\nAnimated, non-algebraic, and user-paced Pythagorean theorem\nPythagorean theorem water demo on YouTube\nPythagorean theorem (more than 70 proofs from cut-the-knot)\nWeisstein, Eric W. \"Pythagorean theorem\". MathWorld."
    },
    "climate change": {
        "url": "https://en.wikipedia.org/wiki/Climate_change",
        "summary": "In common usage, climate change describes global warming—the ongoing increase in global average temperature—and its effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global average temperature is more rapid than previous changes, and is primarily caused by humans burning fossil fuels.",
        "content": "In common usage, climate change describes global warming—the ongoing increase in global average temperature—and its effects on Earth's climate system. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global average temperature is more rapid than previous changes, and is primarily caused by humans burning fossil fuels. Fossil fuel use, deforestation, and some agricultural and industrial practices increase greenhouse gases, notably carbon dioxide and methane. Greenhouse gases absorb some of the heat that the Earth radiates after it warms from sunlight. Larger amounts of these gases trap more heat in Earth's lower atmosphere, causing global warming.\nDue to climate change, deserts are expanding, while heat waves and wildfires are becoming more common. Increased warming in the Arctic has contributed to melting permafrost, glacial retreat and sea ice loss. Higher temperatures are also causing more intense storms, droughts, and other weather extremes. Rapid environmental change in mountains, coral reefs, and the Arctic is forcing many species to relocate or become extinct. Even if efforts to minimise future warming are successful, some effects will continue for centuries. These include ocean heating, ocean acidification and sea level rise.Climate change threatens people with increased flooding, extreme heat, increased food and water scarcity, more disease, and economic loss. Human migration and conflict can also be a result. The World Health Organization (WHO) calls climate change the greatest threat to global health in the 21st century. Societies and ecosystems will experience more severe risks in the future without action to limit warming. Adapting to climate change through efforts like flood control measures or drought-resistant crops reduces climate change risks, although this may not be possible with increasing warming. Poorer countries are responsible for a small share of global emissions, yet they have the least ability to adapt and are most vulnerable to climate change.\nMany climate change impacts are already felt at the current 1.2 °C (2.2 °F) level of warming. Additional warming will increase these impacts and can trigger tipping points, such as the melting of the Greenland ice sheet. Under the 2015 Paris Agreement, nations collectively agreed to keep warming \"well under 2 °C\". However, with pledges made under the Agreement, global warming would still reach about 2.7 °C (4.9 °F) by the end of the century. Limiting warming to 1.5 °C will require halving emissions by 2030 and achieving net-zero emissions by 2050.\n\nReducing emissions requires generating electricity from low-carbon sources rather than burning fossil fuels. This change includes phasing out coal and natural gas fired power plants, vastly increasing use of wind, solar, and other types of renewable energy, and reducing energy use. Electricity generated from non-carbon-emitting sources will need to replace fossil fuels for powering transportation, heating buildings, and operating industrial facilities. Carbon can also be removed from the atmosphere, for instance by increasing forest cover and by farming with methods that capture carbon in soil.\n\n\n== Terminology ==\nBefore the 1980s, when it was unclear whether the warming effect of increased greenhouse gases were stronger than the cooling effect of airborne particulates in air pollution, scientists used the term inadvertent climate modification to refer to human impacts on the climate.In the 1980s, the terms global warming and climate change became more common. Though the two terms are sometimes used interchangeably, scientifically, global warming refers only to increased surface warming, while climate change describes the totality of changes to Earth's climate system. Global warming—used as early as 1975—became the more popular term after NASA climate scientist James Hansen used it in his 1988 testimony in the U.S. Senate. Since the 2000s, climate change has increased in usage. Climate change can also refer more broadly to both human-caused changes or natural changes throughout Earth's history.Various scientists, politicians and media now use the terms climate crisis or climate emergency to talk about climate change, and global heating instead of global warming.\n\n\n== Observed temperature rise ==\n\nMultiple independent instrumental datasets show that the climate system is warming. The 2011–2020 decade warmed to an average 1.09 °C [0.95–1.20 °C] compared to the pre-industrial baseline (1850–1900). Surface temperatures are rising by about 0.2 °C per decade, with 2020 reaching a temperature of 1.2 °C above the pre-industrial era. Since 1950, the number of cold days and nights has decreased, and the number of warm days and nights has increased.There was little net warming between the 18th century and the mid-19th century. Climate information for that period comes from climate proxies, such as trees and ice cores. Thermometer records began to provide global coverage around 1850. Historical patterns of warming and cooling, like the Medieval Climate Anomaly and the Little Ice Age, did not occur at the same time across different regions. Temperatures may have reached as high as those of the late-20th century in a limited set of regions. There have been prehistorical episodes of global warming, such as the Paleocene–Eocene Thermal Maximum. However, the modern observed rise in temperature and CO2 concentrations has been so rapid that even abrupt geophysical events in Earth's history do not approach current rates.Evidence of warming from air temperature measurements are reinforced with a wide range of other observations. For example, changes to the natural water cycle have been predicted and observed, such as an increase in the frequency and intensity of heavy precipitation, melting of snow and land ice, and increased atmospheric humidity. Flora and fauna are also behaving in a manner consistent with warming; for instance, plants are flowering earlier in spring. Another key indicator is the cooling of the upper atmosphere, which demonstrates that greenhouse gases are trapping heat near the Earth's surface and preventing it from radiating into space.Regions of the world warm at differing rates. The pattern is independent of where greenhouse gases are emitted, because the gases persist long enough to diffuse across the planet. Since the pre-industrial period, the average surface temperature over land regions has increased almost twice as fast as the global-average surface temperature. This is because of the larger heat capacity of oceans, and because oceans lose more heat by evaporation. The thermal energy in the global climate system has grown with only brief pauses since at least 1970, and over 90% of this extra energy has been stored in the ocean. The rest has heated the atmosphere, melted ice, and warmed the continents.The Northern Hemisphere and the North Pole have warmed much faster than the South Pole and Southern Hemisphere. The Northern Hemisphere not only has much more land, but also more seasonal snow cover and sea ice. As these surfaces flip from reflecting a lot of light to being dark after the ice has melted, they start absorbing more heat. Local black carbon deposits on snow and ice also contribute to Arctic warming. Arctic temperatures are increasing at over twice the rate of the rest of the world. Melting of glaciers and ice sheets in the Arctic disrupts ocean circulation, including a weakened Gulf Stream, further changing the climate.\n\n\n== Attribution of recent temperature rise ==\n\nThe climate system experiences various cycles on its own which can last for years (such as the El Niño–Southern Oscillation (ENSO)), decades or even centuries. Other changes are caused by an imbalance of energy that is \"external\" to the climate system, but not always external to the Earth. Examples of external forcings include changes in the concentrations of greenhouse gases, solar luminosity, volcanic eruptions, and variations in the Earth's orbit around the Sun.To determine the human contribution to climate change, known internal climate variability and natural external forcings need to be ruled out. A key approach is to determine unique \"fingerprints\" for all potential causes, then compare these fingerprints with observed patterns of climate change. For example, solar forcing can be ruled out as a major cause. Its fingerprint would be warming in the entire atmosphere. Yet, only the lower atmosphere has warmed, consistent with greenhouse gas forcing. Attribution of recent climate change shows that the main driver is elevated greenhouse gases, with aerosols having a dampening effect.\n\n\n=== Greenhouse gases ===\n\nGreenhouse gases are transparent to sunlight, and thus allow it to pass through the atmosphere to heat the Earth's surface. The Earth radiates it as heat, and greenhouse gases absorb a portion of it. This absorption slows the rate at which heat escapes into space, trapping heat near the Earth's surface and warming it over time. Before the Industrial Revolution, naturally-occurring amounts of greenhouse gases caused the air near the surface to be about 33 °C warmer than it would have been in their absence. While water vapour (~50%) and clouds (~25%) are the biggest contributors to the greenhouse effect, they increase as a function of temperature and are therefore feedbacks. On the other hand, concentrations of gases such as CO2 (~20%), tropospheric ozone, CFCs and nitrous oxide are not temperature-dependent, and are therefore external forcings.Human activity since the Industrial Revolution, mainly extracting and burning fossil fuels (coal, oil, and natural gas), has increased the amount of greenhouse gases in the atmosphere, resulting in a radiative imbalance. In 2019, the concentrations of CO2 and methane had increased by about 48% and 160%, respectively, since 1750. These CO2 levels are higher than they have been at any time during the last 2 million years. Concentrations of methane are far higher than they were over the last 800,000 years.\n\nGlobal anthropogenic greenhouse gas emissions in 2019 were equivalent to 59 billion tonnes of CO2. Of these emissions, 75% was CO2, 18% was methane, 4% was nitrous oxide, and 2% was fluorinated gases. CO2 emissions primarily come from burning fossil fuels to provide energy for transport, manufacturing, heating, and electricity. Additional CO2 emissions come from deforestation and industrial processes, which include the CO2 released by the chemical reactions for making cement, steel, aluminum, and fertiliser. Methane emissions come from livestock, manure, rice cultivation, landfills, wastewater, and coal mining, as well as oil and gas extraction. Nitrous oxide emissions largely come from the microbial decomposition of fertiliser.Despite the contribution of deforestation to greenhouse gas emissions, the Earth's land surface, particularly its forests, remain a significant carbon sink for CO2. Land-surface sink processes, such as carbon fixation in the soil and photosynthesis, remove about 29% of annual global CO2 emissions. The ocean also serves as a significant carbon sink via a two-step process. First, CO2 dissolves in the surface water. Afterwards, the ocean's overturning circulation distributes it deep into the ocean's interior, where it accumulates over time as part of the carbon cycle. Over the last two decades, the world's oceans have absorbed 20 to 30% of emitted CO2.\n\n\n=== Aerosols and clouds ===\nAir pollution, in the form of aerosols, affects the climate on a large scale. Aerosols scatter and absorb solar radiation. From 1961 to 1990, a gradual reduction in the amount of sunlight reaching the Earth's surface was observed. This phenomenon is popularly known as global dimming, and is attributed to aerosols produced by dust, pollution and combustion of biofuels and fossil fuels. Globally, aerosols have been declining since 1990 due to pollution controls, meaning that they no longer mask greenhouse gas warming as much.Aerosols also have indirect effects on the Earth's radiation budget. Sulfate aerosols act as cloud condensation nuclei and lead to clouds that have more and smaller cloud droplets. These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets. They also reduce the growth of raindrops, which makes clouds more reflective to incoming sunlight. Indirect effects of aerosols are the largest uncertainty in radiative forcing.While aerosols typically limit global warming by reflecting sunlight, black carbon in soot that falls on snow or ice can contribute to global warming. Not only does this increase the absorption of sunlight, it also increases melting and sea-level rise. Limiting new black carbon deposits in the Arctic could reduce global warming by 0.2 °C by 2050.\n\n\n=== Land surface changes ===\n\nHumans change the Earth's surface mainly to create more agricultural land. Today, agriculture takes up 34% of Earth's land area, while 26% is forests, and 30% is uninhabitable (glaciers, deserts, etc.). The amount of forested land continues to decrease, which is the main land use change that causes global warming. Deforestation releases CO2 contained in trees when they are destroyed, plus it prevents those trees from absorbing more CO2 in the future. The main causes of deforestation are: permanent land-use change from forest to agricultural land producing products such as beef and palm oil (27%), logging to produce forestry/forest products (26%), short term shifting cultivation (24%), and wildfires (23%).The type of vegetation in a region affects the local temperature. It impacts how much of the sunlight gets reflected back into space (albedo), and how much heat is lost by evaporation. For instance, the change from a dark forest to grassland makes the surface lighter, causing it to reflect more sunlight. Deforestation can also affect temperatures by modifying the release of chemical compounds that influence clouds, and by changing wind patterns. In tropic and temperate areas the net effect is to produce significant warming, while at latitudes closer to the poles a gain of albedo (as forest is replaced by snow cover) leads to a cooling effect. Globally, these effects are estimated to have led to a slight cooling, dominated by an increase in surface albedo. According to FAO, forest degradation aggravates the impacts of climate change as it reduces the carbon sequestration abilities of forests. Indeed, among their many benefits, forests also have the potential to reduce the impact of high temperatures.\n\n\n=== Solar and volcanic activity ===\n\nAs the Sun is the Earth's primary energy source, changes in incoming sunlight directly affect the climate system. Solar irradiance has been measured directly by satellites, and indirect measurements are available from the early 1600s onwards. There has been no upward trend in the amount of the Sun's energy reaching the Earth.Explosive volcanic eruptions represent the largest natural forcing over the industrial era. When the eruption is sufficiently strong (with sulfur dioxide reaching the stratosphere), sunlight can be partially blocked for a couple of years. The temperature signal lasts about twice as long. In the industrial era, volcanic activity has had negligible impacts on global temperature trends. Present-day volcanic CO2 emissions are equivalent to less than 1% of current anthropogenic CO2 emissions.Physical climate models are unable to reproduce the rapid warming observed in recent decades when taking into account only variations in solar output and volcanic activity. Further evidence for greenhouse gases causing global warming comes from measurements that show a warming of the lower atmosphere (the troposphere), coupled with a cooling of the upper atmosphere (the stratosphere). If solar variations were responsible for the observed warming, the troposphere and stratosphere would both warm.\n\n\n=== Climate change feedback ===\n\nThe response of the climate system to an initial forcing is modified by feedbacks: increased by \"self-reinforcing\" or \"positive\" feedbacks and reduced by \"balancing\" or \"negative\" feedbacks. The main reinforcing feedbacks are the water-vapour feedback, the ice–albedo feedback, and the net effect of clouds. The primary balancing mechanism is radiative cooling, as Earth's surface gives off more heat to space in response to rising temperature. In addition to temperature feedbacks, there are feedbacks in the carbon cycle, such as the fertilizing effect of CO2 on plant growth. Uncertainty over feedbacks is the major reason why different climate models project different magnitudes of warming for a given amount of emissions.As air warms, it can hold more moisture. Water vapour, as a potent greenhouse gas, holds heat in the atmosphere. If cloud cover increases, more sunlight will be reflected back into space, cooling the planet. If clouds become higher and thinner, they act as an insulator, reflecting heat from below back downwards and warming the planet. The effect of clouds is the largest source of feedback uncertainty.Another major feedback is the reduction of snow cover and sea ice in the Arctic, which reduces the reflectivity of the Earth's surface.\nMore of the Sun's energy is now absorbed in these regions, contributing to amplification of Arctic temperature changes. Arctic amplification is also melting permafrost, which releases methane and CO2 into the atmosphere. Climate change can also cause methane releases from wetlands, marine systems, and freshwater systems. Overall, climate feedbacks are expected to become increasingly positive.Around half of human-caused CO2 emissions have been absorbed by land plants and by the oceans. On land, elevated CO2 and an extended growing season have stimulated plant growth. Climate change increases droughts and heat waves that inhibit plant growth, which makes it uncertain whether this carbon sink will continue to grow in the future. Soils contain large quantities of carbon and may release some when they heat up. As more CO2 and heat are absorbed by the ocean, it acidifies, its circulation changes and phytoplankton takes up less carbon, decreasing the rate at which the ocean absorbs atmospheric carbon. Overall, at higher CO2 concentrations the Earth will absorb a reduced fraction of our emissions.\n\n\n== Modelling ==\n\nA climate model is a representation of the physical, chemical, and biological processes that affect the climate system.  Models also include natural processes like changes in the Earth's orbit, historical changes in the Sun's activity, and volcanic forcing. Models are used to estimate the degree of warming future emissions will cause when accounting for the strength of climate feedbacks, or reproduce and predict the circulation of the oceans, the annual cycle of the seasons, and the flows of carbon between the land surface and the atmosphere.The physical realism of models is tested by examining their ability to simulate contemporary or past climates. Past models have underestimated the rate of Arctic shrinkage and underestimated the rate of precipitation increase. Sea level rise since 1990 was underestimated in older models, but more recent models agree well with observations. The 2017 United States-published National Climate Assessment notes that \"climate models may still be underestimating or missing relevant feedback processes\".A subset of climate models add societal factors to a simple physical climate model. These models simulate how population, economic growth, and energy use affect – and interact with – the physical climate. With this information, these models can produce scenarios of future greenhouse gas emissions. This is then used as input for physical climate models and carbon cycle models to predict how atmospheric concentrations of greenhouse gases might change in the future. Depending on the socioeconomic scenario and the mitigation scenario, models produce atmospheric CO2 concentrations that range widely between 380 and 1400 ppm.The IPCC Sixth Assessment Report projects that global warming is very likely to reach 1.0 °C to 1.8 °C by the late 21st century under the very low GHG emissions scenario. In an intermediate scenario global warming would reach 2.1 °C to 3.5 °C, and 3.3 °C to 5.7 °C under the very high GHG emissions scenario. These projections are based on climate models in combination with observations.The remaining carbon budget is determined by modelling the carbon cycle and the climate sensitivity to greenhouse gases. According to the IPCC, global warming can be kept below 1.5 °C with a two-thirds chance if emissions after 2018 do not exceed 420 or 570 gigatonnes of CO2. This corresponds to 10 to 13 years of current emissions. There are high uncertainties about the budget. For instance, it may be 100 gigatonnes of CO2 smaller due to methane release from permafrost and wetlands. However, it is clear that fossil fuel resources are too abundant for shortages to be relied on to limit carbon emissions in the 21st century.\n\n\n== Impacts ==\n\n\n=== Environmental effects ===\n\nThe environmental effects of climate change are broad and far-reaching, affecting oceans, ice, and weather. Changes may occur gradually or rapidly. Evidence for these effects comes from studying climate change in the past, from modelling, and from modern observations. Since the 1950s, droughts and heat waves have appeared simultaneously with increasing frequency. Extremely wet or dry events within the monsoon period have increased in India and East Asia. The rainfall rate and intensity of hurricanes and typhoons is likely increasing, and the geographic range likely expanding poleward in response to climate warming. Frequency of tropical cyclones has not increased as a result of climate change.\n\nGlobal sea level is rising as a consequence of glacial melt, melt of the ice sheets in Greenland and Antarctica, and thermal expansion. Between 1993 and 2020, the rise increased over time, averaging 3.3 ± 0.3 mm per year. Over the 21st century, the IPCC projects that in a very high emissions scenario the sea level could rise by 61–110 cm. Increased ocean warmth is undermining and threatening to unplug Antarctic glacier outlets, risking a large melt of the ice sheet and the possibility of a 2-meter sea level rise by 2100 under high emissions.Climate change has led to decades of shrinking and thinning of the Arctic sea ice. While ice-free summers are expected to be rare at 1.5 °C degrees of warming, they are set to occur once every three to ten years at a warming level of 2 °C. Higher atmospheric CO2 concentrations have led to changes in ocean chemistry. An increase in dissolved CO2 is causing oceans to acidify. In addition, oxygen levels are decreasing as oxygen is less soluble in warmer water. Dead zones in the ocean, regions with very little oxygen, are expanding too.\n\n\n=== Tipping points and long-term impacts ===\nGreater degrees of global warming increase the risk of passing through ‘tipping points’—thresholds beyond which certain impacts can no longer be avoided even if temperatures are reduced. An example is the collapse of West Antarctic and Greenland ice sheets, where a temperature rise of 1.5 to 2 °C may commit the ice sheets to melt, although the time scale of melt is uncertain and depends on future warming. Some large-scale changes could occur over a short time period, such as a shutdown of certain ocean currents like the Atlantic Meridional Overturning Circulation (AMOC). Tipping points can also include irreversible damage to ecosystems like the Amazon rainforest and coral reefs.The long-term effects of climate change on oceans include further ice melt, ocean warming, sea level rise, and ocean acidification. On the timescale of centuries to millennia, the magnitude of climate change will be determined primarily by anthropogenic CO2 emissions. This is due to CO2's long atmospheric lifetime. Oceanic CO2 uptake is slow enough that ocean acidification will continue for hundreds to thousands of years. These emissions are estimated to have prolonged the current interglacial period by at least 100,000 years. Sea level rise will continue over many centuries, with an estimated rise of 2.3 metres per degree Celsius (4.2 ft/°F) after 2000 years.\n\n\n=== Nature and wildlife ===\n\nRecent warming has driven many terrestrial and freshwater species poleward and towards higher altitudes. Higher atmospheric CO2 levels and an extended growing season have resulted in global greening. However, heatwaves and drought have reduced ecosystem productivity in some regions. The future balance of these opposing effects is unclear. Climate change has contributed to the expansion of drier climate zones, such as the expansion of deserts in the subtropics. The size and speed of global warming is making abrupt changes in ecosystems more likely. Overall, it is expected that climate change will result in the extinction of many species.The oceans have heated more slowly than the land, but plants and animals in the ocean have migrated towards the colder poles faster than species on land. Just as on land, heat waves in the ocean occur more frequently due to climate change, harming a wide range of organisms such as corals, kelp, and seabirds. Ocean acidification makes it harder for marine calcifying organisms such as mussels, barnacles and corals to produce shells and skeletons; and heatwaves have bleached coral reefs. Harmful algal blooms enhanced by climate change and eutrophication lower oxygen levels, disrupt food webs and cause great loss of marine life. Coastal ecosystems are under particular stress. Almost half of global wetlands have disappeared due to climate change and other human impacts.\n\n\n=== Humans ===\n\nThe effects of climate change are impacting humans everywhere in the world. Impacts can now be observed on all continents and ocean regions, with low-latitude, less developed areas facing the greatest risk. Continued warming has potentially “severe, pervasive and irreversible impacts” for people and ecosystems. The risks are unevenly distributed, but are generally greater for disadvantaged people in developing and developed countries.\n\n\n==== Food and health ====\nThe WHO has classified climate change as the greatest threat to global health in the 21st century. Extreme weather leads to injury and loss of life, and crop failures to undernutrition. Various infectious diseases are more easily transmitted in a warmer climate, such as dengue fever and malaria. Young children are the most vulnerable to food shortages. Both children and older people are vulnerable to extreme heat. The World Health Organization (WHO) has estimated that between 2030 and 2050, climate change would cause around 250,000 additional deaths per year. They assessed deaths from heat exposure in elderly people, increases in diarrhea, malaria, dengue, coastal flooding, and childhood undernutrition. Over 500,000 more adult deaths are projected yearly by 2050 due to reductions in food availability and quality. By 2100, 50% to 75% of the global population may face climate conditions that are life-threatening due to combined effects of extreme heat and humidity.Climate change is affecting food security. It has caused reduction in global yields of maize, wheat, and soybeans between 1981 and 2010. Future warming could further reduce global yields of major crops. Crop production will probably be negatively affected in low-latitude countries, while effects at northern latitudes may be positive or negative. Up to an additional 183 million people worldwide, particularly those with lower incomes, are at risk of hunger as a consequence of these impacts. Climate change also impacts fish populations. Globally, less will be available to be fished. Regions dependent on glacier water, regions that are already dry, and small islands have a higher risk of water stress due to climate change.\n\n\n==== Livelihoods ====\nEconomic damages due to climate change may be severe and there is a chance of disastrous consequences. Climate change has likely already increased global economic inequality, and this trend is projected to continue. Most of the severe impacts are expected in sub-Saharan Africa, where most of the local inhabitants are dependent upon natural and agricultural resources and South-East Asia. The World Bank estimates that climate change could drive over 120 million people into poverty by 2030.Current inequalities based on wealth and social status have worsened due to climate change. Major difficulties in mitigating, adapting, and recovering to climate shocks are faced by marginalized people who have less control over resources. Indigenous people, who are subsistent on their land and ecosystems, will face endangerment to their wellness and lifestyles due to climate change. An expert elicitation concluded that the role of climate change in armed conflict has been small compared to factors such as socio-economic inequality and state capabilities.Low-lying islands and coastal communities are threatened by sea level rise, which makes flooding more common. Sometimes, land is permanently lost to the sea. This could lead to statelessness for people in island nations, such as the Maldives and Tuvalu. In some regions, the rise in temperature and humidity may be too severe for humans to adapt to. With worst-case climate change, models project that almost one-third of humanity might live in extremely hot and uninhabitable climates, similar to the current climate found in the Sahara. These factors can drive environmental migration, both within and between countries. More people are expected to be displaced because of sea level rise, extreme weather and conflict from increased competition over natural resources. Climate change may also increase vulnerability, leading to \"trapped populations\" who are not able to move due to a lack of resources.\n\n\n== Reducing and recapturing emissions ==\n\nClimate change can be mitigated by reducing greenhouse gas emissions and enhancing sinks that absorb greenhouse gases from the atmosphere. In order to limit global warming to less than 1.5 °C global greenhouse gas emissions needs to be net-zero by 2050, or by 2070 with a 2 °C target. This requires far-reaching, systemic changes on an unprecedented scale in energy, land, cities, transport, buildings, and industry. The United Nations Environment Programme estimates that countries need to triple their pledges under the Paris Agreement within the next decade to limit global warming to 2 °C. An even greater level of reduction is required to meet the 1.5 °C goal. With pledges made under the Agreement as of October 2021, global warming would still have a 66% chance of reaching about 2.7 °C (range: 2.2–3.2 °C) by the end of the century. Globally, limiting warming to 2 °C may result in higher benefits than costs.Although there is no single pathway to limit global warming to 1.5 or 2 °C, most scenarios and strategies see a major increase in the use of renewable energy in combination with increased energy efficiency measures to generate the needed greenhouse gas reductions. To reduce pressures on ecosystems and enhance their carbon sequestration capabilities, changes would also be necessary in agriculture and forestry, such as preventing deforestation and restoring natural ecosystems by reforestation.Other approaches to mitigating climate change have a higher level of risk. Scenarios that limit global warming to 1.5 °C typically project the large-scale use of carbon dioxide removal methods over the 21st century. There are concerns, though, about over-reliance on these technologies, and environmental impacts. Solar radiation modification (SRM) is also a possible supplement to deep reductions in emissions. However, SRM would raise significant ethical and legal issues, and the risks are poorly understood.\n\n\n=== Clean energy ===\n\nRenewable energy is key to limiting climate change. Fossil fuels accounted for 80% of the world's energy in 2018. The remaining share was split between nuclear power and renewables (including hydropower, bioenergy, wind and solar power and geothermal energy). That mix is projected to change significantly over the next 30 years. Solar panels and onshore wind are now among the cheapest forms of adding new power generation capacity in many locations. Renewables represented 75% of all new electricity generation installed in 2019, nearly all solar and wind. Other forms of clean energy, such as nuclear and hydropower, currently have a larger share of the energy supply. However, their future growth forecasts appear limited in comparison.To achieve carbon neutrality by 2050, renewable energy would become the dominant form of electricity generation, rising to 85% or more by 2050 in some scenarios. Investment in coal would be eliminated and coal use nearly phased out by 2050.Electricity generated from renewable sources would also need to become the main energy source for heating and transport. Transport can switch away from internal combustion engine vehicles and towards electric vehicles, public transit, and active transport (cycling and walking). For shipping and flying, low-carbon fuels would reduce emissions. Heating could be increasingly decarbonised with technologies like heat pumps.There are obstacles to the continued rapid growth of clean energy, including renewables. For wind and solar, there are environmental and land use concerns for new projects. Wind and solar also produce energy intermittently and with seasonal variability. Traditionally, hydro dams with reservoirs and conventional power plants have been used when variable energy production is low. Going forward, battery storage can be expanded, energy demand and supply can be matched, and long-distance transmission can smooth variability of renewable outputs. Bioenergy is often not carbon-neutral and may have negative consequences for food security. The growth of nuclear power is constrained by controversy around nuclear waste, nuclear weapon proliferation, and accidents. Hydropower growth is limited by the fact that the best sites have been developed, and new projects are confronting increased social and environmental concerns.Low-carbon energy improves human health by minimising climate change. It also has the near-term benefit of reducing air pollution deaths, which were estimated at 7 million annually in 2016. Meeting the Paris Agreement goals that limit warming to a 2 °C increase could save about a million of those lives per year by 2050, whereas limiting global warming to 1.5 °C could save millions and simultaneously increase energy security and reduce poverty. Improving air quality also has economic benefits which may be larger than mitigation costs.\n\n\n=== Energy conservation ===\n\nReducing energy demand is another major aspect of reducing emissions. If less energy is needed, there is more flexibility for clean energy development. It also makes it easier to manage the electricity grid, and minimises carbon-intensive infrastructure development. Major increases in energy efficiency investment will be required to achieve climate goals, comparable to the level of investment in renewable energy. Several COVID-19 related changes in energy use patterns, energy efficiency investments, and funding have made forecasts for this decade more difficult and uncertain.Strategies to reduce energy demand vary by sector. In transport, passengers and freight can switch to more efficient travel modes, such as buses and trains, or use electric vehicles. Industrial strategies to reduce energy demand include improving heating systems and motors, designing less energy-intensive products, and increasing product lifetimes. In the building sector the focus is on better design of new buildings, and higher levels of energy efficiency in retrofitting. The use of technologies like heat pumps can also increase building energy efficiency.\n\n\n=== Agriculture and industry ===\n\n Agriculture and forestry face a triple challenge of limiting greenhouse gas emissions, preventing the further conversion of forests to agricultural land, and meeting increases in world food demand. A set of actions could reduce agriculture and forestry-based emissions by two thirds from 2010 levels. These include reducing growth in demand for food and other agricultural products, increasing land productivity, protecting and restoring forests, and reducing greenhouse gas emissions from agricultural production.On the demand side, a key component of reducing emissions is shifting people towards plant-based diets. Eliminating the production of livestock for meat and dairy would eliminate about 3/4ths of all emissions from agriculture and other land use. Livestock also occupy 37% of ice-free land area on Earth and consume feed from the 12% of land area used for crops, driving deforestation and land degradation.Steel and cement production are responsible for about 13% of industrial CO2 emissions. In these industries, carbon-intensive materials such as coke and lime play an integral role in the production, so that reducing CO2 emissions requires research into alternative chemistries.\n\n\n=== Carbon sequestration ===\n\nNatural carbon sinks can be enhanced to sequester significantly larger amounts of CO2 beyond naturally occurring levels. Reforestation and tree planting on non-forest lands are among the most mature sequestration techniques, although the latter raises food security concerns. Farmers can promote sequestration of carbon in soils through practices such as use of winter cover crops, reducing the intensity and frequency of tillage, and using compost and manure as soil amendments. In one of its recent publications, FAO maintains that forest and landscape restoration yields many benefits for the climate, including greenhouse gas emissions sequestration and reduction. Restoration/recreation of coastal wetlands and seagrass meadows increases the uptake of carbon into organic matter (blue carbon). When carbon is sequestered in soils and in organic matter such as trees, there is a risk of the carbon being re-released into the atmosphere later through changes in land use, fire, or other changes in ecosystems.Where energy production or CO2-intensive heavy industries continue to produce waste CO2, the gas can be captured and stored instead of released to the atmosphere. Although its current use is limited in scale and expensive, carbon capture and storage (CCS) may be able to play a significant role in limiting CO2 emissions by mid-century. This technique, in combination with bioenergy (BECCS) can result in net negative emissions: CO2 is drawn from the atmosphere. It remains highly uncertain whether carbon dioxide removal techniques will be able to play a large role in limiting warming to 1.5 °C. Policy decisions that rely on carbon dioxide removal increase the risk of global warming rising beyond international goals.\n\n\n== Adaptation ==\n\nAdaptation is \"the process of adjustment to current or expected changes in climate and its effects\".: 5  Without additional mitigation, adaptation cannot avert the risk of \"severe, widespread and irreversible\" impacts. More severe climate change requires more transformative adaptation, which can be prohibitively expensive. The capacity and potential for humans to adapt is unevenly distributed across different regions and populations, and developing countries generally have less. The first two decades of the 21st century saw an increase in adaptive capacity in most low- and middle-income countries with improved access to basic sanitation and electricity, but progress is slow. Many countries have implemented adaptation policies. However, there is a considerable gap between necessary and available finance.Adaptation to sea level rise consists of avoiding at-risk areas, learning to live with increased flooding and protection. If that fails, managed retreat may be needed. There are economic barriers for tackling dangerous heat impact. Avoiding strenuous work or having air conditioning is not possible for everybody. In agriculture, adaptation options include a switch to more sustainable diets, diversification, erosion control and genetic improvements for increased tolerance to a changing climate. Insurance allows for risk-sharing, but is often difficult to get for people on lower incomes. Education, migration and early warning systems can reduce climate vulnerability. Planting mangroves or encouraging other coastal vegetation can buffer storms.Ecosystems adapt to climate change, a process that can be supported by human intervention. By increasing connectivity between ecosystems, species can migrate to more favourable climate conditions. Species can also be introduced to areas acquiring a favorable climate. Protection and restoration of natural and semi-natural areas helps build resilience, making it easier for ecosystems to adapt. Many of the actions that promote adaptation in ecosystems, also help humans adapt via ecosystem-based adaptation. For instance, restoration of natural fire regimes makes catastrophic fires less likely, and reduces human exposure. Giving rivers more space allows for more water storage in the natural system, reducing flood risk. Restored forest acts as a carbon sink, but planting trees in unsuitable regions can exacerbate climate impacts.There are synergies but also trade-offs between adaptation and mitigation. Adaptation often offer short-term benefits, whereas mitigation has longer-term benefits. Two examples for trade-offs include: Increased use of air conditioning allows people to better cope with heat, but increases energy demand. Compact urban development may lead to reduced emissions from transport and construction. At the same time, this kind of urban development may increase the urban heat island effect, leading to higher temperatures and increased exposure. An example for synergy is increased food productivity which has large benefits for both adaptation and mitigation.\n\n\n== Policies and politics ==\n\nCountries that are most vulnerable to climate change have typically been responsible for a small share of global emissions. This raises questions about justice and fairness. Climate change is strongly linked to sustainable development. Limiting global warming makes it easier to achieve sustainable development goals, such as eradicating poverty and reducing inequalities. The connection is recognised in Sustainable Development Goal 13 which is to \"take urgent action to combat climate change and its impacts\". The goals on food, clean water and ecosystem protection have synergies with climate mitigation.The geopolitics of climate change is complex. It has often been framed as a free-rider problem, in which all countries benefit from mitigation done by other countries, but individual countries would lose from switching to a low-carbon economy themselves. This framing has been challenged. For instance, the benefits of a coal phase-out to public health and local environments exceed the costs in almost all regions. Furthermore, net importers of fossil fuels win economically from switching to clean energy, causing net exporters to face stranded assets: fossil fuels they cannot sell.\n\n\n=== Policy options ===\nA wide range of policies, regulations, and laws are being used to reduce emissions. As of 2019, carbon pricing covers about 20% of global greenhouse gas emissions. Carbon can be priced with carbon taxes and emissions trading systems. Direct global fossil fuel subsidies reached $319 billion in 2017, and $5.2 trillion when indirect costs such as air pollution are priced in. Ending these can cause a 28% reduction in global carbon emissions and a 46% reduction in air pollution deaths. Money saved on fossil subsidies could be used to support the transition to clean energy instead. More direct methods to reduce greenhouse gases include vehicle efficiency standards, renewable fuel standards, and air pollution regulations on heavy industry. Several countries require utilities to increase the share of renewables in power production.Policy designed through the lens of climate justice tries to address human rights issues and social inequality. For instance, wealthy nations responsible for the largest share of emissions would have to pay poorer countries to adapt. As the use of fossil fuels is reduced, jobs in the sector are being lost. To achieve a just transition, these people would need to be retrained for other jobs. Communities with many fossil fuel workers would need additional investments.\n\n\n=== International climate agreements ===\n\nNearly all countries in the world are parties to the 1994 United Nations Framework Convention on Climate Change (UNFCCC). The goal of the UNFCCC is to prevent dangerous human interference with the climate system. As stated in the convention, this requires that greenhouse gas concentrations are stabilised in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can be sustained. The UNFCCC does not itself restrict emissions but rather provides a framework for protocols that do. Global emissions have risen since the UNFCCC was signed. Its yearly conferences are the stage of global negotiations.The 1997 Kyoto Protocol extended the UNFCCC and included legally binding commitments for most developed countries to limit their emissions. During the negotiations, the G77 (representing developing countries) pushed for a mandate requiring developed countries to \"[take] the lead\" in reducing their emissions, since developed countries contributed most to the accumulation of greenhouse gases in the atmosphere. Per-capita emissions were also still relatively low in developing countries and developing countries would need to emit more to meet their development needs.The 2009 Copenhagen Accord has been widely portrayed as disappointing because of its low goals, and was rejected by poorer nations including the G77. Associated parties aimed to limit the global temperature rise to below 2 °C. The Accord set the goal of sending $100 billion per year to developing countries for mitigation and adaptation by 2020, and proposed the founding of the Green Climate Fund. As of 2020, the fund has failed to reach its expected target, and risks a shrinkage in its funding.In 2015 all UN countries negotiated the Paris Agreement, which aims to keep global warming well below 2.0 °C and contains an aspirational goal of keeping warming under 1.5 °C. The agreement replaced the Kyoto Protocol. Unlike Kyoto, no binding emission targets were set in the Paris Agreement. Instead, a set of procedures was made binding. Countries have to regularly set ever more ambitious goals and reevaluate these goals every five years. The Paris Agreement restated that developing countries must be financially supported. As of October 2021, 194 states and the European Union have signed the treaty and 191 states and the EU have ratified or acceded to the agreement.The 1987 Montreal Protocol, an international agreement to stop emitting ozone-depleting gases, may have been more effective at curbing greenhouse gas emissions than the Kyoto Protocol specifically designed to do so. The 2016 Kigali Amendment to the Montreal Protocol aims to reduce the emissions of hydrofluorocarbons, a group of powerful greenhouse gases which served as a replacement for banned ozone-depleting gases. This made the Montreal Protocol a stronger agreement against climate change.\n\n\n=== National responses ===\nIn 2019, the United Kingdom parliament became the first national government to declare a climate emergency. Other countries and jurisdictions followed suit. That same year, the European Parliament declared a \"climate and environmental emergency\". The European Commission presented its European Green Deal with the goal of making the EU carbon-neutral by 2050. Major countries in Asia have made similar pledges: South Korea and Japan have committed to become carbon-neutral by 2050, and China by 2060. In 2021, the European Commission released its “Fit for 55” legislation package, which contains guidelines for the car industry; all new cars on the European market must be zero-emission vehicles from 2035. While India has strong incentives for renewables, it also plans a significant expansion of coal in the country. Vietnam is among very few coal-dependent fast developing countries that pledged to phase out unabated coal power by the 2040s or as soon as possible there after.As of 2021, based on information from 48 national climate plans, which represent 40% of the parties to the Paris Agreement, estimated total greenhouse gas emissions will be 0.5% lower compared to 2010 levels, below the 45% or 25% reduction goals to limit global warming to 1.5 °C or 2 °C, respectively.\n\n\n== Society ==\n\n\n=== Denial and misinformation ===\n\nPublic debate about climate change has been strongly affected by climate change denial and misinformation, which originated in the United States and has since spread to other countries, particularly Canada and Australia. The actors behind climate change denial form a well-funded and relatively coordinated coalition of fossil fuel companies, industry groups, conservative think tanks, and contrarian scientists. Like the tobacco industry, the main strategy of these groups has been to manufacture doubt about scientific data and results. Many who deny, dismiss, or hold unwarranted doubt about the scientific consensus on anthropogenic climate change are labelled as \"climate change skeptics\", which several scientists have noted is a misnomer.There are different variants of climate denial: some deny that warming takes place at all, some acknowledge warming but attribute it to natural influences, and some minimise the negative impacts of climate change. Manufacturing uncertainty about the science later developed into a manufactured controversy: creating the belief that there is significant uncertainty about climate change within the scientific community in order to delay policy changes. Strategies to promote these ideas include criticism of scientific institutions, and questioning the motives of individual scientists. An echo chamber of climate-denying blogs and media has further fomented misunderstanding of climate change.\n\n\n=== Public awareness and opinion ===\n\nClimate change came to international public attention in the late 1980s. Due to media coverage in the early 1990s, people often confused climate change with other environmental issues like ozone depletion. In popular culture, the climate fiction movie The Day After Tomorrow (2004) and the Al Gore documentary An Inconvenient Truth (2006) focused on climate change.Significant regional, gender, age and political differences exist in both public concern for, and understanding of, climate change. More highly educated people, and in some countries, women and younger people, were more likely to see climate change as a serious threat. Partisan gaps also exist in many countries, and countries with high CO2 emissions tend to be less concerned. Views on causes of climate change vary widely between countries. Concern has increased over time, to the point where in 2021 a majority of citizens in many countries express a high level of worry about climate change, or view it as a global emergency. Higher levels of worry are associated with stronger public support for policies that address climate change.\n\n\n==== Climate movement ====\n\nClimate protests demand that political leaders take action to prevent climate change. They can take the form of public demonstrations, fossil fuel divestment, lawsuits and other activities. Prominent demonstrations include the School Strike for Climate. In this initiative, young people across the globe have been protesting since 2018 by skipping school on Fridays, inspired by Swedish teenager Greta Thunberg. Mass civil disobedience actions by groups like Extinction Rebellion have protested by disrupting roads and public transport. Litigation is increasingly used as a tool to strengthen climate action from public institutions and companies. Activists also initiate lawsuits which target governments and demand that they take ambitious action or enforce existing laws on climate change. Lawsuits against fossil-fuel companies generally seek compensation for loss and damage.\n\n\n== History ==\n\n\n=== Early discoveries ===\n\nScientists in the 19th century such as Alexander von Humboldt began to foresee the effects of climate change. In the 1820s, Joseph Fourier proposed the greenhouse effect to explain why Earth's temperature was higher than the sun's energy alone could explain. Earth's atmosphere is transparent to sunlight, so sunlight reaches the surface where it is converted to heat. However, the atmosphere is not transparent to heat radiating from the surface, and captures some of that heat, which in turn warms the planet.In 1856 Eunice Newton Foote demonstrated that the warming effect of the sun is greater for air with water vapour than for dry air, and that the effect is even greater with carbon dioxide (CO2). She concluded that \"An atmosphere of that gas would give to our earth a high temperature...\"Starting in 1859, John Tyndall established that nitrogen and oxygen—together totaling 99% of dry air—are transparent to radiated heat. However, water vapour and gases such as methane and carbon dioxide absorb radiated heat and re-radiate that heat into the atmosphere. Tyndall proposed that changes in the concentrations of these gases may have caused climatic changes in the past, including ice ages.Svante Arrhenius noted that water vapour in air continuously varied, but the CO2 concentration in air was influenced by long-term geological processes. Warming from increased CO2 levels would increase the amount of water vapour, amplifying warming in a positive feedback loop. In 1896, he published the first climate model of its kind, projecting that halving CO2 levels could have produced a drop in temperature initiating an ice age. Arrhenius calculated the temperature increase expected from doubling CO2 to be around 5–6 °C. Other scientists were initially skeptical and believed that the greenhouse effect was saturated so that adding more CO2 would make no difference, and that the climate would be self-regulating. Beginning in 1938, Guy Stewart Callendar published evidence that climate was warming and CO2 levels were rising, but his calculations met the same objections.\n\n\n=== Development of a scientific consensus ===\n\nIn the 1950s, Gilbert Plass created a detailed computer model that included different atmospheric layers and the infrared spectrum. This model predicted that increasing CO2 levels would cause warming. Around the same time, Hans Suess found evidence that CO2 levels had been rising, and Roger Revelle showed that the oceans would not absorb the increase. The two scientists subsequently helped Charles Keeling to begin a record of continued increase, which has been termed the \"Keeling Curve\". Scientists alerted the public, and the dangers were highlighted at James Hansen's 1988 Congressional testimony. The Intergovernmental Panel on Climate Change (IPCC), set up in 1988 to provide formal advice to the world's governments, spurred interdisciplinary research. As part of the IPCC reports, scientists assess the scientific discussion that takes place in peer-reviewed journal articles.There is a near-complete scientific consensus that the climate is warming and that this is caused by human activities. As of 2019, agreement in recent literature reached over 99%. No scientific body of national or international standing disagrees with this view. Consensus has further developed that some form of action should be taken to protect people against the impacts of climate change. National science academies have called on world leaders to cut global emissions. The 2021 IPCC Assessment Report stated that it is \"unequivocal\" that climate change is caused by humans.\n\n\n== See also ==\nAnthropocene – proposed new geological time interval in which humans are having significant geological impact\nList of climate scientists\n\n\n== References ==\n\n\n=== Sources ===\n\n\n==== IPCC reports ====\n\n\n==== Other peer-reviewed sources ====\n\n\n==== Books, reports and legal documents ====\n\n\n==== Non-technical sources ====\n\n\n== External links ==\n\nMet Office: Climate Guide – UK National Weather Service\nGlobal Climate Change Indicators – NOAA\nUp-to-the-second assessment of human-induced global warming since the second half of the 19th century – Oxford University\nGlobal warming, britannica.com"
    },
    "human evolution": {
        "url": "https://en.wikipedia.org/wiki/Human_evolution",
        "summary": "Human evolution is the evolutionary process within the history of primates that led to the emergence of Homo sapiens as a distinct species of the hominid family, which includes all the great apes. This process involved the gradual development of traits such as human bipedalism, dexterity and complex language, as well as interbreeding with other hominins (a tribe of the African hominid subfamily), indicating that human evolution was not linear but weblike. The study of human evolution involves several scientific disciplines, including physical and evolutionary anthropology, paleontology, and genetics.Primates diverged from other mammals about 85 million years ago (mya), in the Late Cretaceous period, with their earliest fossils appearing over 55 mya, during the Paleocene.",
        "content": "Human evolution is the evolutionary process within the history of primates that led to the emergence of Homo sapiens as a distinct species of the hominid family, which includes all the great apes. This process involved the gradual development of traits such as human bipedalism, dexterity and complex language, as well as interbreeding with other hominins (a tribe of the African hominid subfamily), indicating that human evolution was not linear but weblike. The study of human evolution involves several scientific disciplines, including physical and evolutionary anthropology, paleontology, and genetics.Primates diverged from other mammals about 85 million years ago (mya), in the Late Cretaceous period, with their earliest fossils appearing over 55 mya, during the Paleocene. Primates produced successive clades leading to the ape superfamily, which gave rise to the hominid and the gibbon families; these diverged some 15–20 mya. African and Asian hominids (including orangutans) diverged about 14 mya. Hominins (including the Australopithecine and Panina subtribes) parted from the Gorillini tribe (gorillas) between 8–9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the Pan genus (containing chimpanzees and bonobos) 4–7 mya. The Homo genus is evidenced by the appearance of H. habilis over 2 mya, while anatomically modern humans emerged in Africa approximately 300,000 years ago.\n\n\n== Before Homo ==\n\n\n=== Early evolution of primates ===\n\nThe evolutionary history of primates can be traced back 65 million years. One of the oldest known primate-like mammal species, the Plesiadapis, came from North America; another, Archicebus, came from China. Other similar basal primates were widespread in Eurasia and Africa during the tropical conditions of the Paleocene and Eocene.\n\nDavid R. Begun concluded that early primates flourished in Eurasia and that a lineage leading to the African apes and humans, including to Dryopithecus, migrated south from Europe or Western Asia into Africa. The surviving tropical population of primates—which is seen most completely in the Upper Eocene and lowermost Oligocene fossil beds of the Faiyum depression southwest of Cairo—gave rise to all extant primate species, including the lemurs of Madagascar, lorises of Southeast Asia, galagos or \"bush babies\" of Africa, and to the anthropoids, which are the Platyrrhines or New World monkeys, the Catarrhines or Old World monkeys, and the great apes, including humans and other hominids.\nThe earliest known catarrhine is Kamoyapithecus from uppermost Oligocene at Eragaleit in the northern Great Rift Valley in Kenya, dated to 24 million years ago. Its ancestry is thought to be species related to Aegyptopithecus, Propliopithecus, and Parapithecus from the Faiyum, at around 35 mya. In 2010, Saadanius was described as a close relative of the last common ancestor of the crown catarrhines, and tentatively dated to 29–28 mya, helping to fill an 11-million-year gap in the fossil record.\n\nIn the Early Miocene, about 22 million years ago, the many kinds of arboreally adapted primitive catarrhines from East Africa suggest a long history of prior diversification. Fossils at 20 million years ago include fragments attributed to Victoriapithecus, the earliest Old World monkey. Among the genera thought to be in the ape lineage leading up to 13 million years ago are Proconsul, Rangwapithecus, Dendropithecus, Limnopithecus, Nacholapithecus, Equatorius, Nyanzapithecus, Afropithecus, Heliopithecus, and Kenyapithecus, all from East Africa.\nThe presence of other generalized non-cercopithecids of Middle Miocene from sites far distant—Otavipithecus from cave deposits in Namibia, and Pierolapithecus and Dryopithecus from France, Spain and Austria—is evidence of a wide diversity of forms across Africa and the Mediterranean basin during the relatively warm and equable climatic regimes of the Early and Middle Miocene. The youngest of the Miocene hominoids, Oreopithecus, is from coal beds in Italy that have been dated to 9 million years ago.\nMolecular evidence indicates that the lineage of gibbons diverged from the line of great apes some 18–12 mya, and that of orangutans (subfamily Ponginae) diverged from the other great apes at about 12 million years; there are no fossils that clearly document the ancestry of gibbons, which may have originated in a so-far-unknown Southeast Asian hominoid population, but fossil proto-orangutans may be represented by Sivapithecus from India and Griphopithecus from Turkey, dated to around 10 mya.Hominidae subfamily Homininae (African hominids) diverged from Ponginae (orangutans) about 14 mya. Hominins (including humans and the Australopithecine and Panina subtribes) parted from the Gorillini tribe (gorillas) between 8–9 mya; Australopithecine (including the extinct biped ancestors of humans) separated from the Pan genus (containing chimpanzees and bonobos) 4–7 mya. The Homo genus is evidenced by the appearance of H. habilis over 2 mya, while anatomically modern humans emerged in Africa approximately 300,000 years ago.\n\n\n=== Divergence of the human clade from other great apes ===\nSpecies close to the last common ancestor of gorillas, chimpanzees and humans may be represented by Nakalipithecus fossils found in Kenya and Ouranopithecus found in Greece. Molecular evidence suggests that between 8 and 4 million years ago, first the gorillas, and then the chimpanzees (genus Pan) split off from the line leading to the humans. Human DNA is approximately 98.4% identical to that of chimpanzees when comparing single nucleotide polymorphisms (see human evolutionary genetics). The fossil record, however, of gorillas and chimpanzees is limited; both poor preservation – rain forest soils tend to be acidic and dissolve bone – and sampling bias probably contribute to this problem.\nOther hominins probably adapted to the drier environments outside the equatorial belt; and there they encountered antelope, hyenas, dogs, pigs, elephants, horses, and others. The equatorial belt contracted after about 8 million years ago, and there is very little fossil evidence for the split—thought to have occurred around that time—of the hominin lineage from the lineages of gorillas and chimpanzees. The earliest fossils argued by some to belong to the human lineage are Sahelanthropus tchadensis (7 Ma) and Orrorin tugenensis (6 Ma), followed by Ardipithecus (5.5–4.4 Ma), with species Ar. kadabba and Ar. ramidus.\nIt has been argued in a study of the life history of Ar. ramidus that the species provides evidence for a suite of anatomical and behavioral adaptations in very early hominins unlike any species of extant great ape. This study demonstrated affinities between the skull morphology of Ar. ramidus and that of infant and juvenile chimpanzees, suggesting the species evolved a juvenalised or paedomorphic craniofacial morphology via heterochronic dissociation of growth trajectories. It was also argued that the species provides support for the notion that very early hominins, akin to bonobos (Pan paniscus) the less aggressive species of the genus Pan, may have evolved via the process of self-domestication. Consequently, arguing against the so-called \"chimpanzee referential model\" the authors suggest it is no longer tenable to use chimpanzee (Pan troglodytes) social and mating behaviors in models of early hominin social evolution. When commenting on the absence of aggressive canine morphology in Ar. ramidus and the implications this has for the evolution of hominin social psychology, they wrote:\n\nOf course Ar. ramidus differs significantly from bonobos, bonobos having retained a functional canine honing complex. However, the fact that Ar. ramidus shares with bonobos reduced sexual dimorphism, and a more paedomorphic form relative to chimpanzees, suggests that the developmental and social adaptations evident in bonobos may be of assistance in future reconstructions of early hominin social and sexual psychology. In fact the trend towards increased maternal care, female mate selection and self-domestication may have been stronger and more refined in Ar. ramidus than what we see in bonobos.: 128 \nThe authors argue that many of the basic human adaptations evolved in the ancient forest and woodland ecosystems of late Miocene and early Pliocene Africa. Consequently, they argue that humans may not represent evolution from a chimpanzee-like ancestor as has traditionally been supposed. This suggests many modern human adaptations represent phylogenetically deep traits and that the behavior and morphology of chimpanzees may have evolved subsequent to the split with the common ancestor they share with humans.\n\n\n=== Genus Australopithecus ===\n\nThe genus Australopithecus evolved in eastern Africa around 4 million years ago before spreading throughout the continent and eventually becoming extinct 2 million years ago. During this time period various forms of australopiths existed, including Australopithecus anamensis, Au. afarensis, Au. sediba, and Au. africanus. There is still some debate among academics whether certain African hominid species of this time, such as Au. robustus and Au. boisei, constitute members of the same genus; if so, they would be considered to be Au. robust australopiths whilst the others would be considered Au. gracile australopiths. However, if these species do indeed constitute their own genus, then they may be given their own name, Paranthropus.\n\nAustralopithecus (4–1.8 Ma), with species Au. anamensis, Au. afarensis, Au. africanus, Au. bahrelghazali, Au. garhi, and Au. sediba;\nKenyanthropus (3–2.7 Ma), with species K. platyops;\nParanthropus (3–1.2 Ma), with species P. aethiopicus, P. boisei, and P. robustusA new proposed species Australopithecus deyiremeda is claimed to have been discovered living at the same time period of Au. afarensis. There is debate if Au. deyiremeda is a new species or is Au. afarensis. Australopithecus prometheus, otherwise known as Little Foot has recently been dated at 3.67 million years old through a new dating technique, making the genus Australopithecus as old as afarensis. Given the opposable big toe found on Little Foot, it seems that the specimen was a good climber. It is thought given the night predators of the region that he built a nesting platform at night in the trees in a similar fashion to chimpanzees and gorillas.\n\n\n== Evolution of genus Homo ==\n\nThe earliest documented representative of the genus Homo is Homo habilis, which evolved around 2.8 million years ago, and is arguably the earliest species for which there is positive evidence of the use of stone tools. The brains of these early hominins were about the same size as that of a chimpanzee, although it has been suggested that this was the time in which the human SRGAP2 gene doubled, producing a more rapid wiring of the frontal cortex. During the next million years a process of rapid encephalization occurred, and with the arrival of Homo erectus and Homo ergaster in the fossil record, cranial capacity had doubled to 850 cm3. (Such an increase in human brain size is equivalent to each generation having 125,000 more neurons than their parents.) It is believed that H. erectus and H. ergaster were the first to use fire and complex tools, and were the first of the hominin line to leave Africa, spreading throughout Africa, Asia, and Europe between 1.3 to 1.8 million years ago.\n\nAccording to the recent African origin of modern humans theory, modern humans evolved in Africa possibly from H. heidelbergensis, H. rhodesiensis or H. antecessor and migrated out of the continent some 50,000 to 100,000 years ago, gradually replacing local populations of H. erectus, Denisova hominins, H. floresiensis, H. luzonensis and H. neanderthalensis. Archaic Homo sapiens, the forerunner of anatomically modern humans, evolved in the Middle Paleolithic between 400,000 and 250,000 years ago. Recent DNA evidence suggests that several haplotypes of Neanderthal origin are present among all non-African populations, and Neanderthals and other hominins, such as Denisovans, may have contributed up to 6% of their genome to present-day humans, suggestive of a limited interbreeding between these species. The transition to behavioral modernity with the development of symbolic culture, language, and specialized lithic technology happened around 50,000 years ago, according to some anthropologists, although others point to evidence that suggests that a gradual change in behavior took place over a longer time span.Homo sapiens is the only extant species of its genus, Homo. While some (extinct) Homo species might have been ancestors of Homo sapiens, many, perhaps most, were likely \"cousins\", having speciated away from the ancestral hominin line. There is yet no consensus as to which of these groups should be considered a separate species and which should be a subspecies; this may be due to the dearth of fossils or to the slight differences used to classify species in the genus Homo. The Sahara pump theory (describing an occasionally passable \"wet\" Sahara desert) provides one possible explanation of the early variation in the genus Homo.\nBased on archaeological and paleontological evidence, it has been possible to infer, to some extent, the ancient dietary practices of various Homo species and to study the role of diet in physical and behavioral evolution within Homo.Some anthropologists and archaeologists subscribe to the Toba catastrophe theory, which posits that the supereruption of Lake Toba on Sumatran island in Indonesia some 70,000 years ago caused global consequences, killing the majority of humans and creating a population bottleneck that affected the genetic inheritance of all humans today. The genetic and archaeological evidence for this remains in question however.\n\n\n=== H. habilis and H. gautengensis ===\nHomo habilis lived from about 2.8 to 1.4 Ma. The species evolved in South and East Africa in the Late Pliocene or Early Pleistocene, 2.5–2 Ma, when it diverged from the australopithecines with the development of smaller molars and larger brains. One of the first known hominins, it made tools from stone and perhaps animal bones, leading to its name homo habilis (Latin 'handy man') bestowed by discoverer Louis Leakey. Some scientists have proposed moving this species from Homo into Australopithecus due to the morphology of its skeleton being more adapted to living in trees rather than walking on two legs like later hominins.In May 2010, a new species, Homo gautengensis, was discovered in South Africa.\n\n\n=== H. rudolfensis and H. georgicus ===\nThese are proposed species names for fossils from about 1.9–1.6 Ma, whose relation to Homo habilis is not yet clear.\n\nHomo rudolfensis refers to a single, incomplete skull from Kenya. Scientists have suggested that this was a specimen of Homo habilis, but this has not been confirmed.\nHomo georgicus, from Georgia, may be an intermediate form between Homo habilis and Homo erectus, or a subspecies of Homo erectus.\n\n\n=== H. ergaster and H. erectus ===\nThe first fossils of Homo erectus were discovered by Dutch physician Eugene Dubois in 1891 on the Indonesian island of Java. He originally named the material Anthropopithecus erectus (1892–1893, considered at this point as a chimpanzee-like fossil primate) and Pithecanthropus erectus (1893–1894, changing his mind as of based on its morphology, which he considered to be intermediate between that of humans and apes). Years later, in the 20th century, the German physician and paleoanthropologist Franz Weidenreich (1873–1948) compared in detail the characters of Dubois' Java Man, then named Pithecanthropus erectus, with the characters of the Peking Man, then named Sinanthropus pekinensis. Weidenreich concluded in 1940 that because of their anatomical similarity with modern humans it was necessary to gather all these specimens of Java and China in a single species of the genus Homo, the species H. erectus.Homo erectus lived from about 1.8 Ma to about 70,000 years ago – which would indicate that they were probably wiped out by the Toba catastrophe; however, nearby H. floresiensis survived it. The early phase of H. erectus, from 1.8 to 1.25 Ma, is considered by some to be a separate species, H. ergaster, or as H. erectus ergaster, a subspecies of H. erectus. Many paleoanthropologists now use the term Homo ergaster for the non-Asian forms of this group, and reserve H. erectus only for those fossils that are found in Asia and meet certain skeletal and dental requirements which differ slightly from H. ergaster.\nIn Africa in the Early Pleistocene, 1.5–1 Ma, some populations of Homo habilis are thought to have evolved larger brains and to have made more elaborate stone tools; these differences and others are sufficient for anthropologists to classify them as a new species, Homo erectus—in Africa. The evolution of locking knees and the movement of the foramen magnum are thought to be likely drivers of the larger population changes. This species also may have used fire to cook meat. Richard Wrangham  notes that Homo seems to have been ground dwelling, with reduced intestinal length, smaller dentition, and \"brains [swollen] to their current, horrendously fuel-inefficient size\", and hypothesizes that control of fire and cooking, which released increased nutritional value, was the key adaptation that separated Homo from tree-sleeping Australopithecines.\n\n\n=== H. cepranensis and H. antecessor ===\nThese are proposed as species intermediate between H. erectus and H. heidelbergensis.\n\nH. antecessor is known from fossils from Spain and England that are dated 1.2 Ma–500 ka.\nH. cepranensis refers to a single skull cap from Italy, estimated to be about 800,000 years old.\n\n\n=== H. heidelbergensis ===\n\nH. heidelbergensis (\"Heidelberg Man\") lived from about 800,000 to about 300,000 years ago. Also proposed as Homo sapiens heidelbergensis or Homo sapiens paleohungaricus.\n\n\n=== H. rhodesiensis, and the Gawis cranium ===\nH. rhodesiensis, estimated to be 300,000–125,000 years old. Most current researchers place Rhodesian Man within the group of Homo heidelbergensis, though other designations such as archaic Homo sapiens and Homo sapiens rhodesiensis have been proposed.\nIn February 2006 a fossil, the Gawis cranium, was found which might possibly be a species intermediate between H. erectus and H. sapiens or one of many evolutionary dead ends. The skull from Gawis, Ethiopia, is believed to be 500,000–250,000 years old. Only summary details are known, and the finders have not yet released a peer-reviewed study. Gawis man's facial features suggest its being either an intermediate species or an example of a \"Bodo man\" female.\n\n\n=== Neanderthal and Denisovan ===\n\nHomo neanderthalensis, alternatively designated as Homo sapiens neanderthalensis, lived in Europe and Asia from 400,000 to about 28,000 years ago.\nThere are a number of clear anatomical differences between anatomically modern humans (AMH) and Neanderthal specimens, many relating to the superior Neanderthal adaptation to cold environments. Neanderthal surface to volume ratio was even lower than that among modern Inuit populations, indicating superior retention of body heat.\nNeanderthals also had significantly larger brains, as shown from brain endocasts, casting doubt on their intellectual inferiority to modern humans. However, the higher body mass of Neanderthals may have required larger brain mass for body control. Also, recent research by Pearce, Stringer, and Dunbar has shown important differences in brain architecture. The larger size of the Neanderthal orbital chamber and occipital lobe suggests that they had a better visual acuity than modern humans, useful in the dimmer light of glacial Europe.\nNeanderthals may have had less brain capacity available for social functions. Inferring social group size from endocranial volume (minus occipital lobe size) suggests that Neanderthal groups may have been limited to 120 individuals, compared to 144 possible relationships for modern humans. Larger social groups could imply that modern humans had less risk of inbreeding within their clan, trade over larger areas (confirmed in the distribution of stone tools), and faster spread of social and technological innovations. All these may have all contributed to modern Homo sapiens replacing Neanderthal populations by 28,000 BP.Earlier evidence from sequencing mitochondrial DNA suggested that no significant gene flow occurred between H. neanderthalensis and H. sapiens, and that the two were separate species that shared a common ancestor about 660,000 years ago. However, a sequencing of the Neanderthal genome in 2010 indicated that Neanderthals did indeed interbreed with anatomically modern humans c. 45,000-80,000 years ago, around the time modern humans migrated out from Africa, but before they dispersed throughout Europe, Asia and elsewhere. The genetic sequencing of a 40,000-year-old human skeleton from Romania showed that 11% of its genome was Neanderthal, implying the individual had a Neanderthal ancestor 4–6 generations previously, in addition to a contribution from earlier interbreeding in the Middle East. Though this interbred Romanian population seems not to have been ancestral to modern humans, the finding indicates that interbreeding happened repeatedly.All modern non-African humans have about 1% to 4% (or 1.5% to 2.6% by more recent data) of their DNA derived from Neanderthals. This finding is consistent with recent studies indicating that the divergence of some human alleles dates to one Ma, although this interpretation has been questioned. Neanderthals and AMH Homo sapiens could have co-existed in Europe for as long as 10,000 years, during which AMH populations exploded, vastly outnumbering Neanderthals, possibly outcompeting them by sheer numbers.In 2008, archaeologists working at the site of Denisova Cave in the Altai Mountains of Siberia uncovered a small bone fragment from the fifth finger of a juvenile member of another human species, the Denisovans. Artifacts, including a bracelet, excavated in the cave at the same level were carbon dated to around 40,000 BP. As DNA had survived in the fossil fragment due to the cool climate of the Denisova Cave, both mtDNA and nuclear DNA were sequenced.While the divergence point of the mtDNA was unexpectedly deep in time, the full genomic sequence suggested the Denisovans belonged to the same lineage as Neanderthals, with the two diverging shortly after their line split from the lineage that gave rise to modern humans. Modern humans are known to have overlapped with Neanderthals in Europe and the Near East for possibly more than 40,000 years, and the discovery raises the possibility that Neanderthals, Denisovans, and modern humans may have co-existed and interbred. The existence of this distant branch creates a much more complex picture of humankind during the Late Pleistocene than previously thought. Evidence has also been found that as much as 6% of the DNA of some modern Melanesians derive from Denisovans, indicating limited interbreeding in Southeast Asia.Alleles thought to have originated in Neanderthals and Denisovans have been identified at several genetic loci in the genomes of modern humans outside Africa. HLA haplotypes from Denisovans and Neanderthal represent more than half the HLA alleles of modern Eurasians, indicating strong positive selection for these introgressed alleles. Corinne Simoneti at Vanderbilt University, in Nashville and her team have found from medical records of 28,000 people of European descent that the presence of Neanderthal DNA segments may be associated with a higher rate of depression.The flow of genes from Neanderthal populations to modern humans was not all one way. Sergi Castellano of the Max Planck Institute for Evolutionary Anthropology reported in 2016 that while Denisovan and Neanderthal genomes are more related to each other than they are to us, Siberian Neanderthal genomes show more similarity to modern human genes than do European Neanderthal populations. This suggests Neanderthal populations interbred with modern humans around 100,000 years ago, probably somewhere in the Near East.Studies of a Neanderthal child at Gibraltar show from brain development and tooth eruption that Neanderthal children may have matured more rapidly than Homo sapiens.\n\n\n=== H. floresiensis ===\n\nH. floresiensis, which lived from approximately 190,000 to 50,000 years before present (BP), has been nicknamed the hobbit for its small size, possibly a result of insular dwarfism. H. floresiensis is intriguing both for its size and its age, being an example of a recent species of the genus Homo that exhibits derived traits not shared with modern humans. In other words, H. floresiensis shares a common ancestor with modern humans, but split from the modern human lineage and followed a distinct evolutionary path. The main find was a skeleton believed to be a woman of about 30 years of age. Found in 2003, it has been dated to approximately 18,000 years old. The living woman was estimated to be one meter in height, with a brain volume of just 380 cm3 (considered small for a chimpanzee and less than a third of the H. sapiens average of 1400 cm3).However, there is an ongoing debate over whether H. floresiensis is indeed a separate species. Some scientists hold that H. floresiensis was a modern H. sapiens with pathological dwarfism. This hypothesis is supported in part, because some modern humans who live on Flores, the Indonesian island where the skeleton was found, are pygmies. This, coupled with pathological dwarfism, could have resulted in a significantly diminutive human. The other major attack on H. floresiensis as a separate species is that it was found with tools only associated with H. sapiens.The hypothesis of pathological dwarfism, however, fails to explain additional anatomical features that are unlike those of modern humans (diseased or not) but much like those of ancient members of our genus. Aside from cranial features, these features include the form of bones in the wrist, forearm, shoulder, knees, and feet. Additionally, this hypothesis fails to explain the find of multiple examples of individuals with these same characteristics, indicating they were common to a large population, and not limited to one individual.In 2016, fossil teeth and a partial jaw from hominins assumed to be ancestral to H. floresiensis were discovered at Mata Menge, about 74 km (46 mi) from Liang Bua. They date to about 700,000 years ago and are noted by Australian archaeologist Gerrit van den Bergh for being even smaller than the later fossils.\n\n\n=== H. luzonensis ===\n\nA small number of specimens from the island of Luzon, dated 50,000 to 67,000 years ago, have recently been assigned by their discoverers, based on dental characteristics, to a novel human species, H. luzonensis.\n\n\n=== H. sapiens ===\n\nH. sapiens (the adjective sapiens is Latin for \"wise\" or \"intelligent\") emerged in Africa around 300,000 years ago, likely derived from H. heidelbergensis or a related lineage. In September 2019, scientists reported the computerized determination, based on 260 CT scans, of a virtual skull shape of the last common human ancestor to modern humans/H. sapiens, representative of the earliest modern humans, and suggested that modern humans arose between 260,000 and 350,000 years ago through a merging of populations in East and South Africa.Between 400,000 years ago and the second interglacial period in the Middle Pleistocene, around 250,000 years ago, the trend in intra-cranial volume expansion and the elaboration of stone tool technologies developed, providing evidence for a transition from H. erectus to H. sapiens. The direct evidence suggests there was a migration of H. erectus out of Africa, then a further speciation of H. sapiens from H. erectus in Africa. A subsequent migration (both within and out of Africa) eventually replaced the earlier dispersed H. erectus. This migration and origin theory is usually referred to as the \"recent single-origin hypothesis\" or \"out of Africa\" theory. H. sapiens interbred with archaic humans both in Africa and in Eurasia, in Eurasia notably with Neanderthals and Denisovans.The Toba catastrophe theory, which postulates a population bottleneck for H. sapiens about 70,000 years ago, was controversial from its first proposal in the 1990s and by the 2010s had very little support. Distinctive human genetic variability has arisen as the result of the founder effect, by archaic admixture and by recent evolutionary pressures.\n\n\n== Anatomical changes ==\nSince Homo sapiens separated from its last common ancestor shared with chimpanzees, human evolution is characterized by a number of morphological, developmental, physiological, behavioral, and environmental changes. Environmental (cultural) evolution discovered much later during the Pleistocene played a significant role in human evolution observed via human transitions between subsistence systems. The most significant of these adaptations are bipedalism, increased brain size, lengthened ontogeny (gestation and infancy), and decreased sexual dimorphism. The relationship between these changes is the subject of ongoing debate. Other significant morphological changes included the evolution of a power and precision grip, a change first occurring in H. erectus.\n\n\n=== Bipedalism ===\n\nBipedalism is the basic adaptation of the hominid and is considered the main cause behind a suite of skeletal changes shared by all bipedal hominids. The earliest hominin, of presumably primitive bipedalism, is considered to be either Sahelanthropus or Orrorin, both of which arose some 6 to 7 million years ago. The non-bipedal knuckle-walkers, the gorillas and chimpanzees, diverged from the hominin line over a period covering the same time, so either Sahelanthropus or Orrorin may be our last shared ancestor. Ardipithecus, a full biped, arose approximately 5.6 million years ago.The early bipeds eventually evolved into the australopithecines and still later into the genus Homo. There are several theories of the adaptation value of bipedalism. It is possible that bipedalism was favored because it freed the hands for reaching and carrying food, saved energy during locomotion, enabled long-distance running and hunting, provided an enhanced field of vision, and helped avoid hyperthermia by reducing the surface area exposed to direct sun; features all advantageous for thriving in the new savanna and woodland environment created as a result of the East African Rift Valley uplift versus the previous closed forest habitat. A 2007 study provides support for the hypothesis that walking on two legs, or bipedalism, evolved because it used less energy than quadrupedal knuckle-walking. However, recent studies suggest that bipedality without the ability to use fire would not have allowed global dispersal. This change in gait saw a lengthening of the legs proportionately when compared to the length of the arms, which were shortened through the removal of the need for brachiation. Another change is the shape of the big toe. Recent studies suggest that australopithecines still lived part of the time in trees as a result of maintaining a grasping big toe. This was progressively lost in habilines.\nAnatomically, the evolution of bipedalism has been accompanied by a large number of skeletal changes, not just to the legs and pelvis, but also to the vertebral column, feet and ankles, and skull. The femur evolved into a slightly more angular position to move the center of gravity toward the geometric center of the body. The knee and ankle joints became increasingly robust to better support increased weight. To support the increased weight on each vertebra in the upright position, the human vertebral column became S-shaped and the lumbar vertebrae became shorter and wider. In the feet the big toe moved into alignment with the other toes to help in forward locomotion. The arms and forearms shortened relative to the legs making it easier to run. The foramen magnum migrated under the skull and more anterior.The most significant changes occurred in the pelvic region, where the long downward facing iliac blade was shortened and widened as a requirement for keeping the center of gravity stable while walking; bipedal hominids have a shorter but broader, bowl-like pelvis due to this. A drawback is that the birth canal of bipedal apes is smaller than in knuckle-walking apes, though there has been a widening of it in comparison to that of australopithecine and modern humans, thus permitting the passage of newborns due to the increase in cranial size. This is limited to the upper portion, since further increase can hinder normal bipedal movement.The shortening of the pelvis and smaller birth canal evolved as a requirement for bipedalism and had significant effects on the process of human birth, which is much more difficult in modern humans than in other primates. During human birth, because of the variation in size of the pelvic region, the fetal head must be in a transverse position (compared to the mother) during entry into the birth canal and rotate about 90 degrees upon exit. The smaller birth canal became a limiting factor to brain size increases in early humans and prompted a shorter gestation period leading to the relative immaturity of human offspring, who are unable to walk much before 12 months and have greater neoteny, compared to other primates, who are mobile at a much earlier age. The increased brain growth after birth and the increased dependency of children on mothers had a major effect upon the female reproductive cycle, and the more frequent appearance of alloparenting in humans when compared with other hominids. Delayed human sexual maturity also led to the evolution of menopause with one explanation, the grandmother hypothesis, providing that elderly women could better pass on their genes by taking care of their daughter's offspring, as compared to having more children of their own.\n\n\n=== Encephalization ===\n\nThe human species eventually developed a much larger brain than that of other primates—typically 1,330 cm3 (81 cu in) in modern humans, nearly three times the size of a chimpanzee or gorilla brain. After a period of stasis with Australopithecus anamensis and Ardipithecus, species which had smaller brains as a result of their bipedal locomotion, the pattern of encephalization started with Homo habilis, whose 600 cm3 (37 cu in) brain was slightly larger than that of chimpanzees. This evolution continued in Homo erectus with 800–1,100 cm3 (49–67 cu in), and reached a maximum in Neanderthals with 1,200–1,900 cm3 (73–116 cu in), larger even than modern Homo sapiens. This brain increase manifested during postnatal brain growth, far exceeding that of other apes (heterochrony). It also allowed for extended periods of social learning and language acquisition in juvenile humans, beginning as much as 2 million years ago. Encephalization may be due to a dependency on calorie-dense, difficult-to-acquire food.\nFurthermore, the changes in the structure of human brains may be even more significant than the increase in size. Fossilized skulls shows the brain size in early humans fell within the range of modern humans 300,000 years ago, but only got it present-day brain shape between 100,000 and 35,000 years ago.  The temporal lobes, which contain centers for language processing, have increased disproportionately, as has the prefrontal cortex, which has been related to complex decision-making and moderating social behavior. Encephalization has been tied to increased starches and meat  in the diet, however a 2022 meta study called into question the role of meat. Other factors are the development of cooking, and it has been proposed that intelligence increased as a response to an increased necessity for solving social problems as human society became more complex. Changes in skull morphology, such as smaller mandibles and mandible muscle attachments, allowed more room for the brain to grow.The increase in volume of the neocortex also included a rapid increase in size of the cerebellum. Its function has traditionally been associated with balance and fine motor control, but more recently with speech and cognition. The great apes, including hominids, had a more pronounced cerebellum relative to the neocortex than other primates. It has been suggested that because of its function of sensory-motor control and learning complex muscular actions, the cerebellum may have underpinned human technological adaptations, including the preconditions of speech.The immediate survival advantage of encephalization is difficult to discern, as the major brain changes from Homo erectus to Homo heidelbergensis were not accompanied by major changes in technology. It has been suggested that the changes were mainly social and behavioural, including increased empathic abilities, increases in size of social groups, and increased behavioral plasticity. Humans are unique in the ability to acquire information through social transmission and adapt that information. The emerging field of cultural evolution studies human sociocultural change from an evolutionary perspective.\n\n\n=== Sexual dimorphism ===\nThe reduced degree of sexual dimorphism in humans is visible primarily in the reduction of the male canine tooth relative to other ape species (except gibbons) and reduced brow ridges and general robustness of males. Another important physiological change related to sexuality in humans was the evolution of hidden estrus. Humans are the only hominoids in which the female is fertile year round and in which no special signals of fertility are produced by the body (such as genital swelling or overt changes in proceptivity during estrus).Nonetheless, humans retain a degree of sexual dimorphism in the distribution of body hair and subcutaneous fat, and in the overall size, males being around 15% larger than females. These changes taken together have been interpreted as a result of an increased emphasis on pair bonding as a possible solution to the requirement for increased parental investment due to the prolonged infancy of offspring.\n\n\n=== Ulnar opposition ===\n\nThe ulnar opposition—the contact between the thumb and the tip of the little finger of the same hand—is unique to the genus Homo, including Neanderthals, the Sima de los Huesos hominins and anatomically modern humans. In other primates, the thumb is short and unable to touch the little finger. The ulnar opposition facilitates the precision grip and power grip of the human hand, underlying all the skilled manipulations.\n\n\n=== Other changes ===\nA number of other changes have also characterized the evolution of humans, among them an increased reliance on vision rather than smell (highly reduced olfactory bulb); a longer juvenile developmental period and higher infant dependency; a smaller gut and small, misaligned teeth; faster basal metabolism; loss of body hair; evolution of sweat glands; a change in the shape of the dental arcade from u-shaped to parabolic; development of a chin (found in Homo sapiens alone); styloid processes; and a descended larynx.\n\n\n== Use of tools ==\n\nThe use of tools has been interpreted as a sign of intelligence, and it has been theorized that tool use may have stimulated certain aspects of human evolution, especially the continued expansion of the human brain. Paleontology has yet to explain the expansion of this organ over millions of years despite being extremely demanding in terms of energy consumption. The brain of a modern human consumes, on average, about 13 watts (260 kilocalories per day), a fifth of the body's resting power consumption. Increased tool use would allow hunting for energy-rich meat products, and would enable processing more energy-rich plant products. Researchers have suggested that early hominins were thus under evolutionary pressure to increase their capacity to create and use tools.Precisely when early humans started to use tools is difficult to determine, because the more primitive these tools are (for example, sharp-edged stones) the more difficult it is to decide whether they are natural objects or human artifacts. There is some evidence that the australopithecines (4 Ma) may have used broken bones as tools, but this is debated.Many species make and use tools, but it is the human genus that dominates the areas of making and using more complex tools. The oldest known tools are flakes from West Turkana, Kenya, which date to 3.3 million years ago. The next oldest stone tools are from Gona, Ethiopia, and are considered the beginning of the Oldowan technology. These tools date to about 2.6 million years ago. A Homo fossil was found near some Oldowan tools, and its age was noted at 2.3 million years old, suggesting that maybe the Homo species did indeed create and use these tools. It is a possibility but does not yet represent solid evidence. The third metacarpal styloid process enables the hand bone to lock into the wrist bones, allowing for greater amounts of pressure to be applied to the wrist and hand from a grasping thumb and fingers. It allows humans the dexterity and strength to make and use complex tools. This unique anatomical feature separates humans from apes and other nonhuman primates, and is not seen in human fossils older than 1.8 million years.Bernard Wood noted that Paranthropus co-existed with the early Homo species in the area of the \"Oldowan Industrial Complex\" over roughly the same span of time. Although there is no direct evidence which identifies Paranthropus as the tool makers, their anatomy lends to indirect evidence of their capabilities in this area. Most paleoanthropologists agree that the early Homo species were indeed responsible for most of the Oldowan tools found. They argue that when most of the Oldowan tools were found in association with human fossils, Homo was always present, but Paranthropus was not.In 1994, Randall Susman used the anatomy of opposable thumbs as the basis for his argument that both the Homo and Paranthropus species were toolmakers. He compared bones and muscles of human and chimpanzee thumbs, finding that humans have 3 muscles which are lacking in chimpanzees. Humans also have thicker metacarpals with broader heads, allowing more precise grasping than the chimpanzee hand can perform. Susman posited that modern anatomy of the human opposable thumb is an evolutionary response to the requirements associated with making and handling tools and that both species were indeed toolmakers.\n\n\n== Transition to behavioral modernity ==\n\nAnthropologists describe modern human behavior to include cultural and behavioral traits such as specialization of tools, use of jewellery and images (such as cave drawings), organization of living space, rituals (such as grave gifts), specialized hunting techniques, exploration of less hospitable geographical areas, and barter trade networks, as well as more general traits such as language and complex symbolic thinking. Debate continues as to whether a \"revolution\" led to modern humans (\"big bang of human consciousness\"), or whether the evolution was more gradual.Until about 50,000–40,000 years ago, the use of stone tools seems to have progressed stepwise. Each phase (H. habilis, H. ergaster, H. neanderthalensis) marked a new technology, followed by very slow development until the next phase. Currently paleoanthropologists are debating whether these Homo species possessed some or many modern human behaviors. They seem to have been culturally conservative, maintaining the same technologies and foraging patterns over very long periods.\nAround 50,000 BP, human culture started to evolve more rapidly. The transition to behavioral modernity has been characterized by some as a \"Great Leap Forward\", or as the \"Upper Palaeolithic Revolution\", due to the sudden appearance in the archaeological record of distinctive signs of modern behavior and big game hunting. Evidence of behavioral modernity significantly earlier also exists from Africa, with older evidence of abstract imagery, widened subsistence strategies, more sophisticated tools and weapons, and other \"modern\" behaviors, and many scholars have recently argued that the transition to modernity occurred sooner than previously believed. Some other scholars consider the transition to have been more gradual, noting that some features had already appeared among archaic African Homo sapiens 300,000–200,000 years ago. Recent evidence suggests that the Australian Aboriginal population separated from the African population 75,000 years ago, and that they made a 160 km sea journey 60,000 years ago, which may diminish the significance of the Upper Paleolithic Revolution.Modern humans started burying their dead, making clothing from animal hides, hunting with more sophisticated techniques (such as using pit traps or driving animals off cliffs), and cave painting. As human culture advanced, different populations innovated existing technologies: artifacts such as fish hooks, buttons, and bone needles show signs of cultural variation, which had not been seen prior to 50,000 BP. Typically, the older H. neanderthalensis populations did not vary in their technologies, although the Chatelperronian assemblages have been found to be Neanderthal imitations of H. sapiens Aurignacian technologies.\n\n\n== Recent and ongoing human evolution ==\n\nAnatomically modern human populations continue to evolve, as they are affected by both natural selection and genetic drift. Although selection pressure on some traits, such as resistance to smallpox, has decreased in the modern age, humans are still undergoing natural selection for many other traits. Some of these are due to specific environmental pressures, while others are related to lifestyle changes since the development of agriculture (10,000 years ago), urbanization (5,000), and industrialization (250 years ago). It has been argued that human evolution has accelerated since the development of agriculture 10,000 years ago and civilization some 5,000 years ago, resulting, it is claimed, in substantial genetic differences between different current human populations, and more recent research indicates that for some traits, the developments and innovations of human culture have driven a new form of selection that coexists with, and in some cases has largely replaced, natural selection.Particularly conspicuous is variation in superficial characteristics, such as Afro-textured hair, or the recent evolution of light skin and blond hair in some populations, which are attributed to differences in climate. Particularly strong selective pressures have resulted in high-altitude adaptation in humans, with different ones in different isolated populations. Studies of the genetic basis show that some developed very recently, with Tibetans evolving over 3,000 years to have high proportions of an allele of EPAS1 that is adaptive to high altitudes.\nOther evolution is related to endemic diseases: the presence of malaria selects for sickle cell trait (the heterozygous form of sickle cell gene), while in the absence of malaria, the health effects of sickle-cell anemia select against this trait. For another example, the population at risk of the severe debilitating disease kuru has significant over-representation of an immune variant of the prion protein gene G127V versus non-immune alleles. The frequency of this genetic variant is due to the survival of immune persons. Some reported trends remain unexplained and the subject of ongoing research in the novel field of evolutionary medicine: polycystic ovary syndrome (PCOS) reduces fertility and thus is expected to be subject to extremely strong negative selection, but its relative commonality in human populations suggests a counteracting selection pressure. The identity of that pressure remains the subject of some debate.Recent human evolution related to agriculture includes genetic resistance to infectious disease that has appeared in human populations by crossing the species barrier from domesticated animals, as well as changes in metabolism due to changes in diet, such as lactase persistence.\nCulturally-driven evolution can defy the expectations of natural selection:  while human populations experience some pressure that drives a selection for producing children at younger ages, the advent of effective contraception, higher education, and changing social norms have driven the observed selection in the opposite direction. However, culturally-driven selection need not necessarily work counter or in opposition to natural selection:  some proposals to explain the high rate of recent human brain expansion indicate a kind of feedback whereupon the brain's increased social learning efficiency encourages cultural developments that in turn encourage more efficiency, which drive more complex cultural developments that demand still-greater efficiency, and so forth. Culturally-driven evolution has an advantage in that in addition to the genetic effects, it can be observed also in the archaeological record:  the development of stone tools across the Palaeolithic period connects to culturally-driven cognitive development in the form of skill acquisition supported by the culture and the development of increasingly complex technologies and the cognitive ability to elaborate them.In contemporary times, since industrialization, some trends have been observed: for instance, menopause is evolving to occur later. Other reported trends appear to include lengthening of the human reproductive period and reduction in cholesterol levels, blood glucose and blood pressure in some populations.\n\n\n== History of study ==\n\n\n=== Before Darwin ===\nThe word homo, the name of the biological genus to which humans belong, is Latin for \"human\". It was chosen originally by Carl Linnaeus in his classification system. The word \"human\" is from the Latin humanus, the adjectival form of homo. The Latin \"homo\" derives from the Indo-European root *dhghem, or \"earth\". Linnaeus and other scientists of his time also considered the great apes to be the closest relatives of humans based on morphological and anatomical similarities.\n\n\n=== Darwin ===\nThe possibility of linking humans with earlier apes by descent became clear only after 1859 with the publication of Charles Darwin's On the Origin of Species, in which he argued for the idea of the evolution of new species from earlier ones. Darwin's book did not address the question of human evolution, saying only that \"Light will be thrown on the origin of man and his history.\"The first debates about the nature of human evolution arose between Thomas Henry Huxley and Richard Owen. Huxley argued for human evolution from apes by illustrating many of the similarities and differences between humans and other apes, and did so particularly in his 1863 book Evidence as to Man's Place in Nature. Many of Darwin's early supporters (such as Alfred Russel Wallace and Charles Lyell) did not initially agree that the origin of the mental capacities and the moral sensibilities of humans could be explained by natural selection, though this later changed. Darwin applied the theory of evolution and sexual selection to humans in his 1871 book The Descent of Man, and Selection in Relation to Sex.\n\n\n=== First fossils ===\nA major problem in the 19th century was the lack of fossil intermediaries. Neanderthal remains were discovered in a limestone quarry in 1856, three years before the publication of On the Origin of Species, and Neanderthal fossils had been discovered in Gibraltar even earlier, but it was originally claimed that these were the remains of a modern human who had suffered some kind of illness. Despite the 1891 discovery by Eugène Dubois of what is now called Homo erectus at Trinil, Java, it was only in the 1920s when such fossils were discovered in Africa, that intermediate species began to accumulate. In 1925, Raymond Dart described Australopithecus africanus. The type specimen was the Taung Child, an australopithecine infant which was discovered in a cave. The child's remains were a remarkably well-preserved tiny skull and an endocast of the brain.\nAlthough the brain was small (410 cm3), its shape was rounded, unlike that of chimpanzees and gorillas, and more like a modern human brain. Also, the specimen showed short canine teeth, and the position of the foramen magnum (the hole in the skull where the spine enters) was evidence of bipedal locomotion. All of these traits convinced Dart that the Taung Child was a bipedal human ancestor, a transitional form between apes and humans.\n\n\n=== The East African fossils ===\n\nDuring the 1960s and 1970s, hundreds of fossils were found in East Africa in the regions of the Olduvai Gorge and Lake Turkana. These searches were carried out by the Leakey family, with Louis Leakey and his wife Mary Leakey, and later their son Richard and daughter-in-law Meave, fossil hunters and paleoanthropologists. From the fossil beds of Olduvai and Lake Turkana they amassed specimens of the early hominins: the australopithecines and Homo species, and even H. erectus.\nThese finds cemented Africa as the cradle of humankind. In the late 1970s and the 1980s, Ethiopia emerged as the new hot spot of paleoanthropology after \"Lucy\", the most complete fossil member of the species Australopithecus afarensis, was found in 1974 by Donald Johanson near Hadar in the desertic Afar Triangle region of northern Ethiopia. Although the specimen had a small brain, the pelvis and leg bones were almost identical in function to those of modern humans, showing with certainty that these hominins had walked erect. Lucy was classified as a new species, Australopithecus afarensis, which is thought to be more closely related to the genus Homo as a direct ancestor, or as a close relative of an unknown ancestor, than any other known hominid or hominin from this early time range. (The specimen was nicknamed \"Lucy\" after the Beatles' song \"Lucy in the Sky with Diamonds\", which was played loudly and repeatedly in the camp during the excavations.) The Afar Triangle area would later yield discovery of many more hominin fossils, particularly those uncovered or described by teams headed by Tim D. White in the 1990s, including Ardipithecus ramidus and A. kadabba.In 2013, fossil skeletons of Homo naledi, an extinct species of hominin assigned (provisionally) to the genus Homo, were found in the Rising Star Cave system, a site in South Africa's Cradle of Humankind region in Gauteng province near Johannesburg. As of September 2015, fossils of at least fifteen individuals, amounting to 1,550 specimens, have been excavated from the cave. The species is characterized by a body mass and stature similar to small-bodied human populations, a smaller endocranial volume similar to Australopithecus, and a cranial morphology (skull shape) similar to early Homo species. The skeletal anatomy combines primitive features known from australopithecines with features known from early hominins. The individuals show signs of having been deliberately disposed of within the cave near the time of death. The fossils were dated close to 250,000 years ago, and thus are not a direct ancestor but a contemporary with the first appearance of larger-brained anatomically modern humans.\n\n\n=== The genetic revolution ===\nThe genetic revolution in studies of human evolution started when Vincent Sarich and Allan Wilson measured the strength of immunological cross-reactions of blood serum albumin between pairs of creatures, including humans and African apes (chimpanzees and gorillas). The strength of the reaction could be expressed numerically as an immunological distance, which was in turn proportional to the number of amino acid differences between homologous proteins in different species. By constructing a calibration curve of the ID of species' pairs with known divergence times in the fossil record, the data could be used as a molecular clock to estimate the times of divergence of pairs with poorer or unknown fossil records.\nIn their seminal 1967 paper in Science, Sarich and Wilson estimated the divergence time of humans and apes as four to five million years ago, at a time when standard interpretations of the fossil record gave this divergence as at least 10 to as much as 30 million years. Subsequent fossil discoveries, notably \"Lucy\", and reinterpretation of older fossil materials, notably Ramapithecus, showed the younger estimates to be correct and validated the albumin method.\nProgress in DNA sequencing, specifically mitochondrial DNA (mtDNA) and then Y-chromosome DNA (Y-DNA) advanced the understanding of human origins. Application of the molecular clock principle revolutionized the study of molecular evolution.\nOn the basis of a separation from the orangutan between 10 and 20 million years ago, earlier studies of the molecular clock suggested that there were about 76 mutations per generation that were not inherited by human children from their parents; this evidence supported the divergence time between hominins and chimpanzees noted above. However, a 2012 study in Iceland of 78 children and their parents suggests a mutation rate of only 36 mutations per generation; this datum extends the separation between humans and chimpanzees to an earlier period greater than 7 million years ago (Ma). Additional research with 226 offspring of wild chimpanzee populations in eight locations suggests that chimpanzees reproduce at age 26.5 years on average; which suggests the human divergence from chimpanzees occurred between 7 and 13 mya. And these data suggest that Ardipithecus (4.5 Ma), Orrorin (6 Ma) and Sahelanthropus (7 Ma) all may be on the hominid lineage, and even that the separation may have occurred outside the East African Rift region.\nFurthermore, analysis of the two species' genes in 2006 provides evidence that after human ancestors had started to diverge from chimpanzees, interspecies mating between \"proto-human\" and \"proto-chimpanzees\" nonetheless occurred regularly enough to change certain genes in the new gene pool:\n\nA new comparison of the human and chimpanzee genomes suggests that after the two lineages separated, they may have begun interbreeding... A principal finding is that the X chromosomes of humans and chimpanzees appear to have diverged about 1.2 million years more recently than the other chromosomes.The research suggests:\n\nThere were in fact two splits between the human and chimpanzee lineages, with the first being followed by interbreeding between the two populations and then a second split. The suggestion of a hybridization has startled paleoanthropologists, who nonetheless are treating the new genetic data seriously.\n\n\n=== The quest for the earliest hominin ===\nIn the 1990s, several teams of paleoanthropologists were working throughout Africa looking for evidence of the earliest divergence of the hominin lineage from the great apes. In 1994, Meave Leakey discovered Australopithecus anamensis. The find was overshadowed by Tim D. White's 1995 discovery of Ardipithecus ramidus, which pushed back the fossil record to 4.2 million years ago.\nIn 2000, Martin Pickford and Brigitte Senut discovered, in the Tugen Hills of Kenya, a 6-million-year-old bipedal hominin which they named Orrorin tugenensis. And in 2001, a team led by Michel Brunet discovered the skull of Sahelanthropus tchadensis which was dated as 7.2 million years ago, and which Brunet argued was a bipedal, and therefore a hominid—that is, a hominin (cf Hominidae; terms \"hominids\" and hominins).\n\n\n=== Human dispersal ===\n\nAnthropologists in the 1980s were divided regarding some details of reproductive barriers and migratory dispersals of the genus Homo. Subsequently, genetics has been used to investigate and resolve these issues. According to the Sahara pump theory evidence suggests that the genus Homo have migrated out of Africa at least three and possibly four times (e.g. Homo erectus, Homo heidelbergensis and two or three times for Homo sapiens). Recent evidence suggests these dispersals are closely related to fluctuating periods of climate change.Recent evidence suggests that humans may have left Africa half a million years earlier than previously thought. A joint Franco-Indian team has found human artifacts in the Siwalk Hills north of New Delhi dating back at least 2.6 million years. This is earlier than the previous earliest finding of genus Homo at Dmanisi, in Georgia, dating to 1.85 million years. Although controversial, tools found at a Chinese cave strengthen the case that humans used tools as far back as 2.48 million years ago. This suggests that the Asian \"Chopper\" tool tradition, found in Java and northern China may have left Africa before the appearance of the Acheulian hand axe.\n\n\n==== Dispersal of modern Homo sapiens ====\nUp until the genetic evidence became available, there were two dominant models for the dispersal of modern humans. The multiregional hypothesis proposed that the genus Homo contained only a single interconnected population as it does today (not separate species), and that its evolution took place worldwide continuously over the last couple of million years. This model was proposed in 1988 by Milford H. Wolpoff. In contrast, the \"out of Africa\" model proposed that modern H. sapiens speciated in Africa recently (that is, approximately 200,000 years ago) and the subsequent migration through Eurasia resulted in the nearly complete replacement of other Homo species. This model has been developed by Chris Stringer and Peter Andrews.\n\nSequencing mtDNA and Y-DNA sampled from a wide range of indigenous populations revealed ancestral information relating to both male and female genetic heritage, and strengthened the \"out of Africa\" theory and weakened the views of multiregional evolutionism. Aligned in genetic tree differences were interpreted as supportive of a recent single origin.\"Out of Africa\" has thus gained much support from research using female mitochondrial DNA and the male Y chromosome. After analysing genealogy trees constructed using 133 types of mtDNA, researchers concluded that all were descended from a female African progenitor, dubbed Mitochondrial Eve. \"Out of Africa\" is also supported by the fact that mitochondrial genetic diversity is highest among African populations.A broad study of African genetic diversity, headed by Sarah Tishkoff, found the San people had the greatest genetic diversity among the 113 distinct populations sampled, making them one of 14 \"ancestral population clusters\". The research also located a possible origin of modern human migration in southwestern Africa, near the coastal border of Namibia and Angola. The fossil evidence was insufficient for archaeologist Richard Leakey to resolve the debate about exactly where in Africa modern humans first appeared. Studies of haplogroups in Y-chromosomal DNA and mitochondrial DNA have largely supported a recent African origin. All the evidence from autosomal DNA also predominantly supports a Recent African origin. However, evidence for archaic admixture in modern humans, both in Africa and later, throughout Eurasia has recently been suggested by a number of studies.Recent sequencing of Neanderthal and Denisovan genomes shows that some admixture with these populations has occurred. All modern human groups outside Africa have 1–4% or (according to more recent research) about 1.5–2.6% Neanderthal alleles in their genome, and some Melanesians have an additional 4–6% of Denisovan alleles. These new results do not contradict the \"out of Africa\" model, except in its strictest interpretation, although they make the situation more complex. After recovery from a genetic bottleneck that some researchers speculate might be linked to the Toba supervolcano catastrophe, a fairly small group left Africa and interbred with Neanderthals, probably in the Middle East, on the Eurasian steppe or even in North Africa before their departure. Their still predominantly African descendants spread to populate the world. A fraction in turn interbred with Denisovans, probably in southeastern Asia, before populating Melanesia. HLA haplotypes of Neanderthal and Denisova origin have been identified in modern Eurasian and Oceanian populations. The Denisovan EPAS1 gene has also been found in Tibetan populations. Studies of the human genome using machine learning have identified additional genetic contributions in Eurasians from an \"unknown\" ancestral population potentially related to the Neanderthal-Denisovan lineage.\n\nThere are still differing theories on whether there was a single exodus from Africa or several. A multiple dispersal model involves the Southern Dispersal theory, which has gained support in recent years from genetic, linguistic and archaeological evidence. In this theory, there was a coastal dispersal of modern humans from the Horn of Africa crossing the Bab el Mandib to Yemen at a lower sea level around 70,000 years ago. This group helped to populate Southeast Asia and Oceania, explaining the discovery of early human sites in these areas much earlier than those in the Levant. This group seems to have been dependent upon marine resources for their survival.\nStephen Oppenheimer has proposed a second wave of humans may have later dispersed through the Persian Gulf oases, and the Zagros mountains into the Middle East. Alternatively it may have come across the Sinai Peninsula into Asia, from shortly after 50,000 yrs BP, resulting in the bulk of the human populations of Eurasia. It has been suggested that this second group possibly possessed a more sophisticated \"big game hunting\" tool technology and was less dependent on coastal food sources than the original group. Much of the evidence for the first group's expansion would have been destroyed by the rising sea levels at the end of each glacial maximum. The multiple dispersal model is contradicted by studies indicating that the populations of Eurasia and the populations of Southeast Asia and Oceania are all descended from the same mitochondrial DNA L3 lineages, which support a single migration out of Africa that gave rise to all non-African populations.On the basis of the early date of Badoshan Iranian Aurignacian, Oppenheimer suggests that this second dispersal may have occurred with a pluvial period about 50,000 years before the present, with modern human big-game hunting cultures spreading up the Zagros Mountains, carrying modern human genomes from Oman, throughout the Persian Gulf, northward into Armenia and Anatolia, with a variant travelling south into Israel and to Cyrenicia.Recent genetic evidence suggests that all modern non-African populations, including those of Eurasia and Oceania, are descended from a single wave that left Africa between 65,000 and 50,000 years ago.\n\n\n== Evidence ==\nThe evidence on which scientific accounts of human evolution are based comes from many fields of natural science. The main source of knowledge about the evolutionary process has traditionally been the fossil record, but since the development of genetics beginning in the 1970s, DNA analysis has come to occupy a place of comparable importance. The studies of ontogeny, phylogeny and especially evolutionary developmental biology of both vertebrates and invertebrates offer considerable insight into the evolution of all life, including how humans evolved. The specific study of the origin and life of humans is anthropology, particularly paleoanthropology which focuses on the study of human prehistory.\n\n\n=== Evidence from genetics ===\n\nThe closest living relatives of humans are bonobos and chimpanzees (both genus Pan) and gorillas (genus Gorilla). With the sequencing of both the human and chimpanzee genome, as of 2012 estimates of the similarity between their DNA sequences range between 95% and 99%. By using the technique called the molecular clock which estimates the time required for the number of divergent mutations to accumulate between two lineages, the approximate date for the split between lineages can be calculated.\nThe gibbons (family Hylobatidae) and then the orangutans (genus Pongo) were the first groups to split from the line leading to the hominins, including humans—followed by gorillas (genus Gorilla), and, ultimately, by the chimpanzees (genus Pan). The splitting date between hominin and chimpanzee lineages is placed by some between 4 to 8 million years ago, that is, during the Late Miocene. Speciation, however, appears to have been unusually drawn out. Initial divergence occurred sometime between 7 to 13 million years ago, but ongoing hybridization blurred the separation and delayed complete separation during several millions of years. Patterson (2006) dated the final divergence at 5 to 6 million years ago.Genetic evidence has also been employed to compare species within the genus Homo, investigating gene flow between early modern humans and Neanderthals, and to enhance the understanding of the early human migration patterns and splitting dates. By comparing the parts of the genome that are not under natural selection and which therefore accumulate mutations at a fairly steady rate, it is possible to reconstruct a genetic tree incorporating the entire human species since the last shared ancestor.\nEach time a certain mutation (single-nucleotide polymorphism) appears in an individual and is passed on to his or her descendants, a haplogroup is formed including all of the descendants of the individual who will also carry that mutation. By comparing mitochondrial DNA which is inherited only from the mother, geneticists have concluded that the last female common ancestor whose genetic marker is found in all modern humans, the so-called mitochondrial Eve, must have lived around 200,000 years ago.\nHuman evolutionary genetics studies how human genomes differ among individuals, the evolutionary past that gave rise to them, and their current effects. Differences between genomes have anthropological, medical and forensic implications and applications. Genetic data can provide important insight into human evolution.\n\n\n=== Evidence from the fossil record ===\n\nThere is little fossil evidence for the divergence of the gorilla, chimpanzee and hominin lineages. The earliest fossils that have been proposed as members of the hominin lineage are Sahelanthropus tchadensis dating from 7 million years ago, Orrorin tugenensis dating from 5.7 million years ago, and Ardipithecus kadabba dating to 5.6 million years ago. Each of these have been argued to be a bipedal ancestor of later hominins but, in each case, the claims have been contested. It is also possible that one or more of these species are ancestors of another branch of African apes, or that they represent a shared ancestor between hominins and other apes.\nThe question then of the relationship between these early fossil species and the hominin lineage is still to be resolved. From these early species, the australopithecines arose around 4 million years ago and diverged into robust (also called Paranthropus) and gracile branches, one of which (possibly A. garhi) probably went on to become ancestors of the genus Homo. The australopithecine species that is best represented in the fossil record is Australopithecus afarensis with more than 100 fossil individuals represented, found from Northern Ethiopia (such as the famous \"Lucy\"), to Kenya, and South Africa. Fossils of robust australopithecines such as Au. robustus (or alternatively Paranthropus robustus) and Au./P. boisei are particularly abundant in South Africa at sites such as Kromdraai and Swartkrans, and around Lake Turkana in Kenya.\nThe earliest member of the genus Homo is Homo habilis which evolved around 2.8 million years ago. H. habilis is the first species for which we have positive evidence of the use of stone tools. They developed the Oldowan lithic technology, named after the Olduvai Gorge in which the first specimens were found. Some scientists consider Homo rudolfensis, a larger bodied group of fossils with similar morphology to the original H. habilis fossils, to be a separate species, while others consider them to be part of H. habilis—simply representing intraspecies variation, or perhaps even sexual dimorphism. The brains of these early hominins were about the same size as that of a chimpanzee, and their main adaptation was bipedalism as an adaptation to terrestrial living.\nDuring the next million years, a process of encephalization began and, by the arrival (about 1.9 million years ago) of H. erectus in the fossil record, cranial capacity had doubled. H. erectus were the first of the hominins to emigrate from Africa, and, from 1.8 to 1.3 million years ago, this species spread through Africa, Asia, and Europe. One population of H. erectus, also sometimes classified as separate species H. ergaster, remained in Africa and evolved into H. sapiens. It is believed that H. erectus and H. ergaster were the first to use fire and complex tools. In Eurasia, H. erectus evolved into species such as H. antecessor, H. heidelbergensis and H. neanderthalensis. The earliest fossils of anatomically modern humans are from the Middle Paleolithic, about 300–200,000 years ago such as the Herto and Omo remains of Ethiopia, Jebel Irhoud remains of Morocco, and Florisbad remains of South Africa; later fossils from the Skhul Cave in Israel and Southern Europe begin around 90,000 years ago (0.09 million years ago).\nAs modern humans spread out from Africa, they encountered other hominins such as H. neanderthalensis and the Denisovans, who may have evolved from populations of H. erectus that had left Africa around 2 million years ago. The nature of interaction between early humans and these sister species has been a long-standing source of controversy, the question being whether humans replaced these earlier species or whether they were in fact similar enough to interbreed, in which case these earlier populations may have contributed genetic material to modern humans.This migration out of Africa is estimated to have begun about 70–50,000 years BP and modern humans subsequently spread globally, replacing earlier hominins either through competition or hybridization. They inhabited Eurasia and Oceania by 40,000 years BP, and the Americas by at least 14,500 years BP.\n\n\n=== Inter-species breeding ===\n\nThe hypothesis of interbreeding, also known as hybridization, admixture or hybrid-origin theory, has been discussed ever since the discovery of Neanderthal remains in the 19th century. The linear view of human evolution began to be abandoned in the 1970s as different species of humans were discovered that made the linear concept increasingly unlikely. In the 21st century with the advent of molecular biology techniques and computerization, whole-genome sequencing of Neanderthal and human genome were performed, confirming recent admixture between different human species. In 2010, evidence based on molecular biology was published, revealing unambiguous examples of interbreeding between archaic and modern humans during the Middle Paleolithic and early Upper Paleolithic. It has been demonstrated that interbreeding happened in several independent events that included Neanderthals and Denisovans, as well as several unidentified hominins. Today, approximately 2% of DNA from all non-African populations (including Europeans, Asians, and Oceanians) is Neanderthal, with traces of Denisovan heritage. Also, 4–6% of modern Melanesian genetics are Denisovan. Comparisons of the human genome to the genomes of Neandertals, Denisovans and apes can help identify features that set modern humans apart from other hominin species. In a 2016 comparative genomics study, a Harvard Medical School/UCLA research team made a world map on the distribution and made some predictions about where Denisovan and Neanderthal genes may be impacting modern human biology.For example, comparative studies in the mid-2010s found several traits related to neurological, immunological, developmental, and metabolic phenotypes, that were developed by archaic humans to European and Asian environments and inherited to modern humans through admixture with local hominins.Although the narratives of human evolution are often contentious, several discoveries since 2010 show that human evolution should not be seen as a simple linear or branched progression, but a mix of related species. In fact, genomic research has shown that hybridization between substantially diverged lineages is the rule, not the exception, in human evolution. Furthermore, it is argued that hybridization was an essential creative force in the emergence of modern humans.\n\n\n=== Stone tools ===\n\nStone tools are first attested around 2.6 million years ago, when hominins in Eastern Africa used so-called core tools, choppers made out of round cores that had been split by simple strikes. This marks the beginning of the Paleolithic, or Old Stone Age; its end is taken to be the end of the last Ice Age, around 10,000 years ago. The Paleolithic is subdivided into the Lower Paleolithic (Early Stone Age), ending around 350,000–300,000 years ago, the Middle Paleolithic (Middle Stone Age), until 50,000–30,000 years ago, and the Upper Paleolithic, (Late Stone Age), 50,000–10,000 years ago.\nArchaeologists working in the Great Rift Valley in Kenya have discovered the oldest known stone tools in the world. Dated to around 3.3 million years ago, the implements are some 700,000 years older than stone tools from Ethiopia that previously held this distinction.The period from 700,000 to 300,000 years ago is also known as the Acheulean, when H. ergaster (or erectus) made large stone hand axes out of flint and quartzite, at first quite rough (Early Acheulian), later \"retouched\" by additional, more-subtle strikes at the sides of the flakes. After 350,000 BP the more refined so-called Levallois technique was developed, a series of consecutive strikes, by which scrapers, slicers (\"racloirs\"), needles, and flattened needles were made. Finally, after about 50,000 BP, ever more refined and specialized flint tools were made by the Neanderthals and the immigrant Cro-Magnons (knives, blades, skimmers). Bone tools were also made by H. sapiens in Africa by 90–70,000 years ago and are also known from early H. sapiens sites in Eurasia by about 50,000 years ago.\n\n\n== Species list ==\n\nThis list is in chronological order across the table by genus. Some species/subspecies names are well-established, and some are less established – especially in genus Homo. Please see articles for more information.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n=== Sources ===\n\n\n== Further reading ==\n\n\n== External links ==\n\n\"Race, Evolution and the Science of Human Origins\" by Allison Hopper, Scientific American (July 5, 2021).\n\"The evolution of man\". BBC Science & Nature. Retrieved May 6, 2015.\n\"Becoming Human\". Arizona State University's Institute of Human Origins. Retrieved May 6, 2015.\n\"Bones, Stones and Genes: The Origin of Modern Humans\" (Video lecture series). Howard Hughes Medical Institute. Archived from the original on April 24, 2015. Retrieved May 6, 2015.\n\"Evolution Figures: Chapter 25\". Cold Spring Harbor Laboratory Press. Retrieved May 6, 2015. – Illustrations from the book Evolution (2007)\n\"Human Evolution\". Smithsonian Institution's Human Origins Program. Retrieved June 24, 2013.\n\"Human Evolution Timeline\". ArchaeologyInfo.com. Retrieved June 24, 2013.\n\"Human Trace\" video (2015) Normandy University UNIHAVRE, CNRS, IDEES, E.Laboratory on Human Trace Unitwin Complex System Digital Campus UNESCO.\nLambert, Tim (Producer) (June 24, 2015). First Peoples. London: Wall to Wall Television. OCLC 910115743. Retrieved July 18, 2015.\nShaping Humanity Video 2013 Yale University\nHuman Timeline (Interactive) – Smithsonian, National Museum of Natural History (August 2016).\nHuman Evolution, BBC Radio 4 discussion with Steve Jones, Fred Spoor & Margaret Clegg (In Our Time, February 16, 2006)\nEvolutionary Timeline of Home Sapiens − Smithsonian (February 2021)\nHistory of Human Evolution in the United States – Salon (August 24, 2021)"
    },
    "black hole": {
        "url": "https://en.wikipedia.org/wiki/Black_hole",
        "summary": "A black hole is a region of spacetime where gravity is so strong that nothing, including light or other electromagnetic waves, has enough energy to escape its event horizon. The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole. The boundary of no escape is called the event horizon.",
        "content": "A black hole is a region of spacetime where gravity is so strong that nothing, including light or other electromagnetic waves, has enough energy to escape its event horizon. The theory of general relativity predicts that a sufficiently compact mass can deform spacetime to form a black hole. The boundary of no escape is called the event horizon. Although it has a great effect on the fate and circumstances of an object crossing it, it has no locally detectable features according to general relativity. In many ways, a black hole acts like an ideal black body, as it reflects no light. Moreover, quantum field theory in curved spacetime predicts that event horizons emit Hawking radiation, with the same spectrum as a black body of a temperature inversely proportional to its mass. This temperature is of the order of billionths of a kelvin for stellar black holes, making it essentially impossible to observe directly.\nObjects whose gravitational fields are too strong for light to escape were first considered in the 18th century by John Michell and Pierre-Simon Laplace. In 1916, Karl Schwarzschild found the first modern solution of general relativity that would characterize a black hole. David Finkelstein, in 1958, first published the interpretation of \"black hole\" as a region of space from which nothing can escape. Black holes were long considered a mathematical curiosity; it was not until the 1960s that theoretical work showed they were a generic prediction of general relativity. The discovery of neutron stars by Jocelyn Bell Burnell in 1967 sparked interest in gravitationally collapsed compact objects as a possible astrophysical reality. The first black hole known was Cygnus X-1, identified by several researchers independently in 1971.Black holes of stellar mass form when massive stars collapse at the end of their life cycle. After a black hole has formed, it can grow by absorbing mass from its surroundings. Supermassive black holes of millions of solar masses (M☉) may form by absorbing other stars and merging with other black holes. There is consensus that supermassive black holes exist in the centres of most galaxies.\nThe presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. Any matter that falls onto a black hole can form an external accretion disk heated by friction, forming quasars, some of the brightest objects in the universe. Stars passing too close to a supermassive black hole can be shredded into streamers that shine very brightly before being \"swallowed.\" If other stars are orbiting a black hole, their orbits can determine the black hole's mass and location. Such observations can be used to exclude possible alternatives such as neutron stars. In this way, astronomers have identified numerous stellar black hole candidates in binary systems and established that the radio source known as Sagittarius A*, at the core of the Milky Way galaxy, contains a supermassive black hole of about 4.3 million solar masses.\n\n\n== History ==\n\nThe idea of a body so big that even light could not escape was briefly proposed by English astronomical pioneer and clergyman John Michell in a letter published in November 1784. Michell's simplistic calculations assumed such a body might have the same density as the Sun, and concluded that one would form when a star's diameter exceeds the Sun's by a factor of 500, and its surface escape velocity exceeds the usual speed of light. Michell referred to these bodies as dark stars. He correctly noted that such supermassive but non-radiating bodies might be detectable through their gravitational effects on nearby visible bodies. Scholars of the time were initially excited by the proposal that giant but invisible 'dark stars' might be hiding in plain view, but enthusiasm dampened when the wavelike nature of light became apparent in the early nineteenth century, as if light were a wave rather than a particle, it was unclear what, if any, influence gravity would have on escaping light waves.Modern physics discredits Michell's notion of a light ray shooting directly from the surface of a supermassive star, being slowed down by the star's gravity, stopping, and then free-falling back to the star's surface.\n\n\n=== General relativity ===\n\nIn 1915, Albert Einstein developed his theory of general relativity, having earlier shown that gravity does influence light's motion. Only a few months later, Karl Schwarzschild found a solution to the Einstein field equations that describes the gravitational field of a point mass and a spherical mass. A few months after Schwarzschild, Johannes Droste, a student of Hendrik Lorentz, independently gave the same solution for the point mass and wrote more extensively about its properties. This solution had a peculiar behaviour at what is now called the Schwarzschild radius, where it became singular, meaning that some of the terms in the Einstein equations became infinite. The nature of this surface was not quite understood at the time. In 1924, Arthur Eddington showed that the singularity disappeared after a change of coordinates, although it took until 1933 for Georges Lemaître to realize that this meant the singularity at the Schwarzschild radius was a non-physical coordinate singularity. Arthur Eddington did however comment on the possibility of a star with mass compressed to the Schwarzschild radius in a 1926 book, noting that Einstein's theory allows us to rule out overly large densities for visible stars like Betelgeuse because \"a star of 250 million km radius could not possibly have so high a density as the Sun. Firstly, the force of gravitation would be so great that light would be unable to escape from it, the rays falling back to the star like a stone to the earth. Secondly, the red shift of the spectral lines would be so great that the spectrum would be shifted out of existence. Thirdly, the mass would produce so much curvature of the spacetime metric that space would close up around the star, leaving us outside (i.e., nowhere).\"In 1931, Subrahmanyan Chandrasekhar calculated, using special relativity, that a non-rotating body of electron-degenerate matter above a certain limiting mass (now called the Chandrasekhar limit at 1.4 M☉) has no stable solutions. His arguments were opposed by many of his contemporaries like Eddington and Lev Landau, who argued that some yet unknown mechanism would stop the collapse. They were partly correct: a white dwarf slightly more massive than the Chandrasekhar limit will collapse into a neutron star, which is itself stable. But in 1939, Robert Oppenheimer and others predicted that neutron stars above another limit (the Tolman–Oppenheimer–Volkoff limit) would collapse further for the reasons presented by Chandrasekhar, and concluded that no law of physics was likely to intervene and stop at least some stars from collapsing to black holes. Their original calculations, based on the Pauli exclusion principle, gave it as 0.7 M☉; subsequent consideration of neutron-neutron repulsion mediated by the strong force raised the estimate to approximately 1.5 M☉ to 3.0 M☉. Observations of the neutron star merger GW170817, which is thought to have generated a black hole shortly afterward, have refined the TOV limit estimate to ~2.17 M☉.Oppenheimer and his co-authors interpreted the singularity at the boundary of the Schwarzschild radius as indicating that this was the boundary of a bubble in which time stopped. This is a valid point of view for external observers, but not for infalling observers. Because of this property, the collapsed stars were called \"frozen stars\", because an outside observer would see the surface of the star frozen in time at the instant where its collapse takes it to the Schwarzschild radius.\n\n\n==== Golden age ====\nIn 1958, David Finkelstein identified the Schwarzschild surface as an event horizon, \"a perfect unidirectional membrane: causal influences can cross it in only one direction\". This did not strictly contradict Oppenheimer's results, but extended them to include the point of view of infalling observers. Finkelstein's solution extended the Schwarzschild solution for the future of observers falling into a black hole. A complete extension had already been found by Martin Kruskal, who was urged to publish it.These results came at the beginning of the golden age of general relativity, which was marked by general relativity and black holes becoming mainstream subjects of research. This process was helped by the discovery of pulsars by Jocelyn Bell Burnell in 1967, which, by 1969, were shown to be rapidly rotating neutron stars. Until that time, neutron stars, like black holes, were regarded as just theoretical curiosities; but the discovery of pulsars showed their physical relevance and spurred a further interest in all types of compact objects that might be formed by gravitational collapse.In this period more general black hole solutions were found. In 1963, Roy Kerr found the exact solution for a rotating black hole. Two years later, Ezra Newman found the axisymmetric solution for a black hole that is both rotating and electrically charged. Through the work of Werner Israel, Brandon Carter, and David Robinson the no-hair theorem emerged, stating that a stationary black hole solution is completely described by the three parameters of the Kerr–Newman metric: mass, angular momentum, and electric charge.At first, it was suspected that the strange features of the black hole solutions were pathological artifacts from the symmetry conditions imposed, and that the singularities would not appear in generic situations. This view was held in particular by Vladimir Belinsky, Isaak Khalatnikov, and Evgeny Lifshitz, who tried to prove that no singularities appear in generic solutions. However, in the late 1960s Roger Penrose and Stephen Hawking used global techniques to prove that singularities appear generically. For this work, Penrose received half of the 2020 Nobel Prize in Physics, Hawking having died in 2018. Based on observations in Greenwich and Toronto in the early 1970s, Cygnus X-1, a galactic X-ray source discovered in 1964, became the first astronomical object commonly accepted to be a black hole.Work by James Bardeen, Jacob Bekenstein, Carter, and Hawking in the early 1970s led to the formulation of black hole thermodynamics. These laws describe the behaviour of a black hole in close analogy to the laws of thermodynamics by relating mass to energy, area to entropy, and surface gravity to temperature. The analogy was completed when Hawking, in 1974, showed that quantum field theory implies that black holes should radiate like a black body with a temperature proportional to the surface gravity of the black hole, predicting the effect now known as Hawking radiation.\n\n\n=== Observation ===\nOn 11 February 2016, the LIGO Scientific Collaboration and the Virgo collaboration announced the first direct detection of gravitational waves, representing the first observation of a black hole merger. On 10 April 2019, the first direct image of a black hole and its vicinity was published, following observations made by the Event Horizon Telescope (EHT) in 2017 of the supermassive black hole in Messier 87's galactic centre. As of 2021, the nearest known body thought to be a black hole is around 1,500 light-years (460 parsecs) away. Though only a couple dozen black holes have been found so far in the Milky Way, there are thought to be hundreds of millions, most of which are solitary and do not cause emission of radiation. Therefore, they would only be detectable by gravitational lensing.\n\n\n=== Etymology ===\nJohn Michell used the term \"dark star\" in a November 1783 letter to Henry Cavendish, and in the early 20th century, physicists used the term \"gravitationally collapsed object\". Science writer Marcia Bartusiak traces the term \"black hole\" to physicist Robert H. Dicke, who in the early 1960s reportedly compared the phenomenon to the Black Hole of Calcutta, notorious as a prison where people entered but never left alive.The term \"black hole\" was used in print by Life and Science News magazines in 1963, and by science journalist Ann Ewing in her article \"'Black Holes' in Space\", dated 18 January 1964, which was a report on a meeting of the American Association for the Advancement of Science held in Cleveland, Ohio.In December 1967, a student reportedly suggested the phrase \"black hole\" at a lecture by John Wheeler; Wheeler adopted the term for its brevity and \"advertising value\", and it quickly caught on, leading some to credit Wheeler with coining the phrase.\n\n\n== Properties and structure ==\n\nThe no-hair theorem postulates that, once it achieves a stable condition after formation, a black hole has only three independent physical properties: mass, electric charge, and angular momentum; the black hole is otherwise featureless. If the conjecture is true, any two black holes that share the same values for these properties, or parameters, are indistinguishable from one another. The degree to which the conjecture is true for real black holes under the laws of modern physics is currently an unsolved problem.These properties are special because they are visible from outside a black hole. For example, a charged black hole repels other like charges just like any other charged object. Similarly, the total mass inside a sphere containing a black hole can be found by using the gravitational analog of Gauss's law (through the ADM mass), far away from the black hole. Likewise, the angular momentum (or spin) can be measured from far away using frame dragging by the gravitomagnetic field, through for example the Lense–Thirring effect.When an object falls into a black hole, any information about the shape of the object or distribution of charge on it is evenly distributed along the horizon of the black hole, and is lost to outside observers. The behavior of the horizon in this situation is a dissipative system that is closely analogous to that of a conductive stretchy membrane with friction and electrical resistance—the membrane paradigm. This is different from other field theories such as electromagnetism, which do not have any friction or resistivity at the microscopic level, because they are time-reversible. Because a black hole eventually achieves a stable state with only three parameters, there is no way to avoid losing information about the initial conditions: the gravitational and electric fields of a black hole give very little information about what went in. The information that is lost includes every quantity that cannot be measured far away from the black hole horizon, including approximately conserved quantum numbers such as the total baryon number and lepton number. This behavior is so puzzling that it has been called the black hole information loss paradox.\n\n\n=== Physical properties ===\nThe simplest static black holes have mass but neither electric charge nor angular momentum. These black holes are often referred to as Schwarzschild black holes after Karl Schwarzschild who discovered this solution in 1916. According to Birkhoff's theorem, it is the only vacuum solution that is spherically symmetric. This means there is no observable difference at a distance between the gravitational field of such a black hole and that of any other spherical object of the same mass. The popular notion of a black hole \"sucking in everything\" in its surroundings is therefore correct only near a black hole's horizon; far away, the external gravitational field is identical to that of any other body of the same mass.Solutions describing more general black holes also exist. Non-rotating charged black holes are described by the Reissner–Nordström metric, while the Kerr metric describes a non-charged rotating black hole. The most general stationary black hole solution known is the Kerr–Newman metric, which describes a black hole with both charge and angular momentum.While the mass of a black hole can take any positive value, the charge and angular momentum are constrained by the mass. The total electric charge Q and the total angular momentum J are expected to satisfy the inequality\n\n  \n    \n      \n        \n          \n            \n              Q\n              \n                2\n              \n            \n            \n              4\n              π\n              \n                ϵ\n                \n                  0\n                \n              \n            \n          \n        \n        +\n        \n          \n            \n              \n                c\n                \n                  2\n                \n              \n              \n                J\n                \n                  2\n                \n              \n            \n            \n              G\n              \n                M\n                \n                  2\n                \n              \n            \n          \n        \n        ≤\n        G\n        \n          M\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\frac {Q^{2}}{4\\pi \\epsilon _{0}}}+{\\frac {c^{2}J^{2}}{GM^{2}}}\\leq GM^{2}}\n  for a black hole of mass M. Black holes with the minimum possible mass satisfying this inequality are called extremal. Solutions of Einstein's equations that violate this inequality exist, but they do not possess an event horizon. These solutions have so-called naked singularities that can be observed from the outside, and hence are deemed unphysical. The cosmic censorship hypothesis rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. This is supported by numerical simulations.Due to the relatively large strength of the electromagnetic force, black holes forming from the collapse of stars are expected to retain the nearly neutral charge of the star. Rotation, however, is expected to be a universal feature of compact astrophysical objects. The black-hole candidate binary X-ray source GRS 1915+105 appears to have an angular momentum near the maximum allowed value. That uncharged limit is\n\n  \n    \n      \n        J\n        ≤\n        \n          \n            \n              G\n              \n                M\n                \n                  2\n                \n              \n            \n            c\n          \n        \n        ,\n      \n    \n    {\\displaystyle J\\leq {\\frac {GM^{2}}{c}},}\n  allowing definition of a dimensionless spin parameter such that\n\n  \n    \n      \n        0\n        ≤\n        \n          \n            \n              c\n              J\n            \n            \n              G\n              \n                M\n                \n                  2\n                \n              \n            \n          \n        \n        ≤\n        1.\n      \n    \n    {\\displaystyle 0\\leq {\\frac {cJ}{GM^{2}}}\\leq 1.}\n  Black holes are commonly classified according to their mass, independent of angular momentum, J. The size of a black hole, as determined by the radius of the event horizon, or Schwarzschild radius, is proportional to the mass, M, through\n\n  \n    \n      \n        \n          r\n          \n            \n              s\n            \n          \n        \n        =\n        \n          \n            \n              2\n              G\n              M\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        ≈\n        2.95\n        \n        \n          \n            M\n            \n              M\n              \n                ⊙\n              \n            \n          \n        \n         \n        \n          k\n          m\n          ,\n        \n      \n    \n    {\\displaystyle r_{\\mathrm {s} }={\\frac {2GM}{c^{2}}}\\approx 2.95\\,{\\frac {M}{M_{\\odot }}}~\\mathrm {km,} }\n  where rs is the Schwarzschild radius and M☉ is the mass of the Sun. For a black hole with nonzero spin and/or electric charge, the radius is smaller, until an extremal black hole could have an event horizon close to\n\n  \n    \n      \n        \n          r\n          \n            \n              +\n            \n          \n        \n        =\n        \n          \n            \n              G\n              M\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle r_{\\mathrm {+} }={\\frac {GM}{c^{2}}}.}\n  \n\n\n=== Event horizon ===\n\nThe defining feature of a black hole is the appearance of an event horizon—a boundary in spacetime through which matter and light can pass only inward towards the mass of the black hole. Nothing, not even light, can escape from inside the event horizon. The event horizon is referred to as such because if an event occurs within the boundary, information from that event cannot reach an outside observer, making it impossible to determine whether such an event occurred.As predicted by general relativity, the presence of a mass deforms spacetime in such a way that the paths taken by particles bend towards the mass. At the event horizon of a black hole, this deformation becomes so strong that there are no paths that lead away from the black hole.To a distant observer, clocks near a black hole would appear to tick more slowly than those farther away from the black hole. Due to this effect, known as gravitational time dilation, an object falling into a black hole appears to slow as it approaches the event horizon, taking an infinite time to reach it. At the same time, all processes on this object slow down, from the viewpoint of a fixed outside observer, causing any light emitted by the object to appear redder and dimmer, an effect known as gravitational redshift. Eventually, the falling object fades away until it can no longer be seen. Typically this process happens very rapidly with an object disappearing from view within less than a second.On the other hand, indestructible observers falling into a black hole do not notice any of these effects as they cross the event horizon. According to their own clocks, which appear to them to tick normally, they cross the event horizon after a finite time without noting any singular behaviour; in classical general relativity, it is impossible to determine the location of the event horizon from local observations, due to Einstein's equivalence principle.The topology of the event horizon of a black hole at equilibrium is always spherical. For non-rotating (static) black holes the geometry of the event horizon is precisely spherical, while for rotating black holes the event horizon is oblate.\n\n\n=== Singularity ===\n\nAt the centre of a black hole, as described by general relativity, may lie a gravitational singularity, a region where the spacetime curvature becomes infinite. For a non-rotating black hole, this region takes the shape of a single point; for a rotating black hole it is smeared out to form a ring singularity that lies in the plane of rotation. In both cases, the singular region has zero volume. It can also be shown that the singular region contains all the mass of the black hole solution. The singular region can thus be thought of as having infinite density.Observers falling into a Schwarzschild black hole (i.e., non-rotating and not charged) cannot avoid being carried into the singularity once they cross the event horizon. They can prolong the experience by accelerating away to slow their descent, but only up to a limit. When they reach the singularity, they are crushed to infinite density and their mass is added to the total of the black hole. Before that happens, they will have been torn apart by the growing tidal forces in a process sometimes referred to as spaghettification or the \"noodle effect\".In the case of a charged (Reissner–Nordström) or rotating (Kerr) black hole, it is possible to avoid the singularity. Extending these solutions as far as possible reveals the hypothetical possibility of exiting the black hole into a different spacetime with the black hole acting as a wormhole. The possibility of traveling to another universe is, however, only theoretical since any perturbation would destroy this possibility. It also appears to be possible to follow closed timelike curves (returning to one's own past) around the Kerr singularity, which leads to problems with causality like the grandfather paradox. It is expected that none of these peculiar effects would survive in a proper quantum treatment of rotating and charged black holes.The appearance of singularities in general relativity is commonly perceived as signaling the breakdown of the theory. This breakdown, however, is expected; it occurs in a situation where quantum effects should describe these actions, due to the extremely high density and therefore particle interactions. To date, it has not been possible to combine quantum and gravitational effects into a single theory, although there exist attempts to formulate such a theory of quantum gravity. It is generally expected that such a theory will not feature any singularities.\n\n\n=== Photon sphere ===\n\nThe photon sphere is a spherical boundary of zero thickness in which photons that move on tangents to that sphere would be trapped in a circular orbit about the black hole. For non-rotating black holes, the photon sphere has a radius 1.5 times the Schwarzschild radius. Their orbits would be dynamically unstable, hence any small perturbation, such as a particle of infalling matter, would cause an instability that would grow over time, either setting the photon on an outward trajectory causing it to escape the black hole, or on an inward spiral where it would eventually cross the event horizon.While light can still escape from the photon sphere, any light that crosses the photon sphere on an inbound trajectory will be captured by the black hole. Hence any light that reaches an outside observer from the photon sphere must have been emitted by objects between the photon sphere and the event horizon. For a Kerr black hole the radius of the photon sphere depends on the spin parameter and on the details of the photon orbit, which can be prograde (the photon rotates in the same sense of the black hole spin) or retrograde.\n\n\n=== Ergosphere ===\n\nRotating black holes are surrounded by a region of spacetime in which it is impossible to stand still, called the ergosphere. This is the result of a process known as frame-dragging; general relativity predicts that any rotating mass will tend to slightly \"drag\" along the spacetime immediately surrounding it. Any object near the rotating mass will tend to start moving in the direction of rotation. For a rotating black hole, this effect is so strong near the event horizon that an object would have to move faster than the speed of light in the opposite direction to just stand still.The ergosphere of a black hole is a volume bounded by the black hole's event horizon and the ergosurface, which coincides with the event horizon at the poles but is at a much greater distance around the equator.Objects and radiation can escape normally from the ergosphere. Through the Penrose process, objects can emerge from the ergosphere with more energy than they entered with. The extra energy is taken from the rotational energy of the black hole. Thereby the rotation of the black hole slows down. A variation of the Penrose process in the presence of strong magnetic fields, the Blandford–Znajek process is considered a likely mechanism for the enormous luminosity and relativistic jets of quasars and other active galactic nuclei.\n\n\n=== Innermost stable circular orbit (ISCO) ===\n\nIn Newtonian gravity, test particles can stably orbit at arbitrary distances from a central object. In general relativity, however, there exists an innermost stable circular orbit (often called the ISCO), for which any infinitesimal inward perturbations to a circular orbit will lead to spiraling into the black hole, and any outward perturbations will, depending on the energy, result in spiraling in, stably orbiting between apastron and periastron, or escaping to infinity. The location of the ISCO depends on the spin of the black hole, in the case of a Schwarzschild black hole (spin zero) is:\n\n  \n    \n      \n        \n          r\n          \n            \n              I\n              S\n              C\n              O\n            \n          \n        \n        =\n        3\n        \n        \n          r\n          \n            s\n          \n        \n        =\n        \n          \n            \n              6\n              \n              G\n              M\n            \n            \n              c\n              \n                2\n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle r_{\\rm {ISCO}}=3\\,r_{s}={\\frac {6\\,GM}{c^{2}}},}\n  and decreases with increasing black hole spin for particles orbiting in the same direction as the spin.\n\n\n== Formation and evolution ==\nGiven the bizarre character of black holes, it was long questioned whether such objects could actually exist in nature or whether they were merely pathological solutions to Einstein's equations. Einstein himself wrongly thought black holes would not form, because he held that the angular momentum of collapsing particles would stabilize their motion at some radius. This led the general relativity community to dismiss all results to the contrary for many years. However, a minority of relativists continued to contend that black holes were physical objects, and by the end of the 1960s, they had persuaded the majority of researchers in the field that there is no obstacle to the formation of an event horizon.\n\nPenrose demonstrated that once an event horizon forms, general relativity without quantum mechanics requires that a singularity will form within. Shortly afterwards, Hawking showed that many cosmological solutions that describe the Big Bang have singularities without scalar fields or other exotic matter. The Kerr solution, the no-hair theorem, and the laws of black hole thermodynamics showed that the physical properties of black holes were simple and comprehensible, making them respectable subjects for research. Conventional black holes are formed by gravitational collapse of heavy objects such as stars, but they can also in theory be formed by other processes.\n\n\n=== Gravitational collapse ===\n\nGravitational collapse occurs when an object's internal pressure is insufficient to resist the object's own gravity. For stars this usually occurs either because a star has too little \"fuel\" left to maintain its temperature through stellar nucleosynthesis, or because a star that would have been stable receives extra matter in a way that does not raise its core temperature. In either case the star's temperature is no longer high enough to prevent it from collapsing under its own weight.\nThe collapse may be stopped by the degeneracy pressure of the star's constituents, allowing the condensation of matter into an exotic denser state. The result is one of the various types of compact star. Which type forms depends on the mass of the remnant of the original star left if the outer layers have been blown away (for example, in a Type II supernova). The mass of the remnant, the collapsed object that survives the explosion, can be substantially less than that of the original star. Remnants exceeding 5 M☉ are produced by stars that were over 20 M☉ before the collapse.If the mass of the remnant exceeds about 3–4 M☉ (the Tolman–Oppenheimer–Volkoff limit), either because the original star was very heavy or because the remnant collected additional mass through accretion of matter, even the degeneracy pressure of neutrons is insufficient to stop the collapse. No known mechanism (except possibly quark degeneracy pressure) is powerful enough to stop the implosion and the object will inevitably collapse to form a black hole.\n\nThe gravitational collapse of heavy stars is assumed to be responsible for the formation of stellar mass black holes. Star formation in the early universe may have resulted in very massive stars, which upon their collapse would have produced black holes of up to 103 M☉. These black holes could be the seeds of the supermassive black holes found in the centres of most galaxies. It has further been suggested that massive black holes with typical masses of ~105 M☉ could have formed from the direct collapse of gas clouds in the young universe. These massive objects have been proposed as the seeds that eventually formed the earliest quasars observed already at redshift \n  \n    \n      \n        z\n        ∼\n        7\n      \n    \n    {\\displaystyle z\\sim 7}\n  . Some candidates for such objects have been found in observations of the young universe.While most of the energy released during gravitational collapse is emitted very quickly, an outside observer does not actually see the end of this process. Even though the collapse takes a finite amount of time from the reference frame of infalling matter, a distant observer would see the infalling material slow and halt just above the event horizon, due to gravitational time dilation. Light from the collapsing material takes longer and longer to reach the observer, with the light emitted just before the event horizon forms delayed an infinite amount of time. Thus the external observer never sees the formation of the event horizon; instead, the collapsing material seems to become dimmer and increasingly red-shifted, eventually fading away.\n\n\n==== Primordial black holes and the Big Bang ====\nGravitational collapse requires great density. In the current epoch of the universe these high densities are found only in stars, but in the early universe shortly after the Big Bang densities were much greater, possibly allowing for the creation of black holes. High density alone is not enough to allow black hole formation since a uniform mass distribution will not allow the mass to bunch up. In order for primordial black holes to have formed in such a dense medium, there must have been initial density perturbations that could then grow under their own gravity. Different models for the early universe vary widely in their predictions of the scale of these fluctuations. Various models predict the creation of primordial black holes ranging in size from a Planck mass (\n  \n    \n      \n        \n          m\n          \n            P\n          \n        \n        =\n        \n          \n            ℏ\n            c\n            \n              /\n            \n            G\n          \n        \n      \n    \n    {\\displaystyle m_{P}={\\sqrt {\\hbar c/G}}}\n   ≈ 1.2×1019 GeV/c2 ≈ 2.2×10−8 kg) to hundreds of thousands of solar masses.Despite the early universe being extremely dense, it did not re-collapse into a black hole during the Big Bang, since the expansion rate was greater than the attraction. Following inflation theory there was a net repulsive gravitation in the beginning until the end of inflation. Since then the Hubble flow was slowed by the energy density of the universe.\nModels for the gravitational collapse of objects of relatively constant size, such as stars, do not necessarily apply in the same way to rapidly expanding space such as the Big Bang.\n\n\n=== High-energy collisions ===\n\nGravitational collapse is not the only process that could create black holes. In principle, black holes could be formed in high-energy collisions that achieve sufficient density. As of 2002, no such events have been detected, either directly or indirectly as a deficiency of the mass balance in particle accelerator experiments. This suggests that there must be a lower limit for the mass of black holes. Theoretically, this boundary is expected to lie around the Planck mass, where quantum effects are expected to invalidate the predictions of general relativity. This would put the creation of black holes firmly out of reach of any high-energy process occurring on or near the Earth. However, certain developments in quantum gravity suggest that the minimum black hole mass could be much lower: some braneworld scenarios for example put the boundary as low as 1 TeV/c2. This would make it conceivable for micro black holes to be created in the high-energy collisions that occur when cosmic rays hit the Earth's atmosphere, or possibly in the Large Hadron Collider at CERN. These theories are very speculative, and the creation of black holes in these processes is deemed unlikely by many specialists. Even if micro black holes could be formed, it is expected that they would evaporate in about 10−25 seconds, posing no threat to the Earth.\n\n\n=== Growth ===\nOnce a black hole has formed, it can continue to grow by absorbing additional matter. Any black hole will continually absorb gas and interstellar dust from its surroundings. This growth process is one possible way through which some supermassive black holes may have been formed, although the formation of supermassive black holes is still an open field of research. A similar process has been suggested for the formation of intermediate-mass black holes found in globular clusters. Black holes can also merge with other objects such as stars or even other black holes. This is thought to have been important, especially in the early growth of supermassive black holes, which could have formed from the aggregation of many smaller objects. The process has also been proposed as the origin of some intermediate-mass black holes.\n\n\n=== Evaporation ===\n\nIn 1974, Hawking predicted that black holes are not entirely black but emit small amounts of thermal radiation at a temperature ℏc3/(8πGMkB); this effect has become known as Hawking radiation. By applying quantum field theory to a static black hole background, he determined that a black hole should emit particles that display a perfect black body spectrum. Since Hawking's publication, many others have verified the result through various approaches. If Hawking's theory of black hole radiation is correct, then black holes are expected to shrink and evaporate over time as they lose mass by the emission of photons and other particles. The temperature of this thermal spectrum (Hawking temperature) is proportional to the surface gravity of the black hole, which, for a Schwarzschild black hole, is inversely proportional to the mass. Hence, large black holes emit less radiation than small black holes.A stellar black hole of 1 M☉ has a Hawking temperature of 62 nanokelvins. This is far less than the 2.7 K temperature of the cosmic microwave background radiation. Stellar-mass or larger black holes receive more mass from the cosmic microwave background than they emit through Hawking radiation and thus will grow instead of shrinking. To have a Hawking temperature larger than 2.7 K (and be able to evaporate), a black hole would need a mass less than the Moon. Such a black hole would have a diameter of less than a tenth of a millimeter.If a black hole is very small, the radiation effects are expected to become very strong. A black hole with the mass of a car would have a diameter of about 10−24 m and take a nanosecond to evaporate, during which time it would briefly have a luminosity of more than 200 times that of the Sun. Lower-mass black holes are expected to evaporate even faster; for example, a black hole of mass 1 TeV/c2 would take less than 10−88 seconds to evaporate completely. For such a small black hole, quantum gravity effects are expected to play an important role and could hypothetically make such a small black hole stable, although current developments in quantum gravity do not indicate this is the case.The Hawking radiation for an astrophysical black hole is predicted to be very weak and would thus be exceedingly difficult to detect from Earth. A possible exception, however, is the burst of gamma rays emitted in the last stage of the evaporation of primordial black holes. Searches for such flashes have proven unsuccessful and provide stringent limits on the possibility of existence of low mass primordial black holes. NASA's Fermi Gamma-ray Space Telescope launched in 2008 will continue the search for these flashes.If black holes evaporate via Hawking radiation, a solar mass black hole will evaporate (beginning once the temperature of the cosmic microwave background drops below that of the black hole) over a period of 1064 years. A supermassive black hole with a mass of 1011 M☉ will evaporate in around 2×10100 years. Some monster black holes in the universe are predicted to continue to grow up to perhaps 1014 M☉ during the collapse of superclusters of galaxies. Even these would evaporate over a timescale of up to 10106 years.\n\n\n== Observational evidence ==\nBy nature, black holes do not themselves emit any electromagnetic radiation other than the hypothetical Hawking radiation, so astrophysicists searching for black holes must generally rely on indirect observations. For example, a black hole's existence can sometimes be inferred by observing its gravitational influence on its surroundings.On 10 April 2019, an image was released of a black hole, which is seen magnified because the light paths near the event horizon are highly bent. The dark shadow in the middle results from light paths absorbed by the black hole. The image is in false color, as the detected light halo in this image is not in the visible spectrum, but radio waves.\n\nThe Event Horizon Telescope (EHT) is an active program that directly observes the immediate environment of black holes' event horizons, such as the black hole at the centre of the Milky Way. In April 2017, EHT began observing the black hole at the centre of Messier 87. \"In all, eight radio observatories on six mountains and four continents observed the galaxy in Virgo on and off for 10 days in April 2017\" to provide the data yielding the image in April 2019. After two years of data processing, EHT released the first direct image of a black hole; specifically, the supermassive black hole that lies in the centre of the aforementioned galaxy. What is visible is not the black hole—which shows as black because of the loss of all light within this dark region. Instead, it is the gases at the edge of the event horizon (displayed as orange or red) that define the black hole.On 12 May 2022, the EHT released the first image of Sagittarius A*, the supermassive black hole at the centre of the Milky Way galaxy. The published image displayed the same ring-like structure and circular shadow as seen in the M87* black hole, and the image was created using the same techniques as for the M87 black hole. However, the imaging process for Sagittarius A*, which is more than a thousand times smaller and less massive than M87*, was significantly more complex because of the instability of its surroundings. The image of Sagittarius A* was also partially blurred by turbulent plasma on the way to the galactic centre, an effect which prevents resolution of the image at longer wavelengths.The brightening of this material in the 'bottom' half of the processed EHT image is thought to be caused by Doppler beaming, whereby material approaching the viewer at relativistic speeds is perceived as brighter than material moving away. In the case of a black hole, this phenomenon implies that the visible material is rotating at relativistic speeds (>1,000 km/s [2,200,000 mph]), the only speeds at which it is possible to centrifugally balance the immense gravitational attraction of the singularity, and thereby remain in orbit above the event horizon. This configuration of bright material implies that the EHT observed M87* from a perspective catching the black hole's accretion disc nearly edge-on, as the whole system rotated clockwise. However, the extreme gravitational lensing associated with black holes produces the illusion of a perspective that sees the accretion disc from above. In reality, most of the ring in the EHT image was created when the light emitted by the far side of the accretion disc bent around the black hole's gravity well and escaped, meaning that most of the possible perspectives on M87* can see the entire disc, even that directly behind the \"shadow\".\nIn 2015, the EHT detected magnetic fields just outside the event horizon of Sagittarius A* and even discerned some of their properties. The field lines that pass through the accretion disc were a complex mixture of ordered and tangled. Theoretical studies of black holes had predicted the existence of magnetic fields.\n\n\n=== Detection of gravitational waves from merging black holes ===\nOn 14 September 2015, the LIGO gravitational wave observatory made the first-ever successful direct observation of gravitational waves. The signal was consistent with theoretical predictions for the gravitational waves produced by the merger of two black holes: one with about 36 solar masses, and the other around 29 solar masses. This observation provides the most concrete evidence for the existence of black holes to date. For instance, the gravitational wave signal suggests that the separation of the two objects before the merger was just 350 km (or roughly four times the Schwarzschild radius corresponding to the inferred masses). The objects must therefore have been extremely compact, leaving black holes as the most plausible interpretation.More importantly, the signal observed by LIGO also included the start of the post-merger ringdown, the signal produced as the newly formed compact object settles down to a stationary state. Arguably, the ringdown is the most direct way of observing a black hole. From the LIGO signal, it is possible to extract the frequency and damping time of the dominant mode of the ringdown. From these, it is possible to infer the mass and angular momentum of the final object, which match independent predictions from numerical simulations of the merger. The frequency and decay time of the dominant mode are determined by the geometry of the photon sphere. Hence, observation of this mode confirms the presence of a photon sphere; however, it cannot exclude possible exotic alternatives to black holes that are compact enough to have a photon sphere.The observation also provides the first observational evidence for the existence of stellar-mass black hole binaries. Furthermore, it is the first observational evidence of stellar-mass black holes weighing 25 solar masses or more.Since then, many more gravitational wave events have been observed.\n\n\n=== Proper motions of stars orbiting Sagittarius A* ===\nThe proper motions of stars near the centre of our own Milky Way provide strong observational evidence that these stars are orbiting a supermassive black hole. Since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source Sagittarius A*. By fitting their motions to Keplerian orbits, the astronomers were able to infer, in 1998, that a 2.6×106 M☉ object must be contained in a volume with a radius of 0.02 light-years to cause the motions of those stars. Since then, one of the stars—called S2—has completed a full orbit. From the orbital data, astronomers were able to refine the calculations of the mass to 4.3×106 M☉ and a radius of less than 0.002 light-years for the object causing the orbital motion of those stars. The upper limit on the object's size is still too large to test whether it is smaller than its Schwarzschild radius; nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume. Additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes.\n\n\n=== Accretion of matter ===\n\nDue to conservation of angular momentum, gas falling into the gravitational well created by a massive object will typically form a disk-like structure around the object. Artists' impressions such as the accompanying representation of a black hole with corona commonly depict the black hole as if it were a flat-space body hiding the part of the disk just behind it, but in reality gravitational lensing would greatly distort the image of the accretion disk.\n\nWithin such a disk, friction would cause angular momentum to be transported outward, allowing matter to fall farther inward, thus releasing potential energy and increasing the temperature of the gas.\n\nWhen the accreting object is a neutron star or a black hole, the gas in the inner accretion disk orbits at very high speeds because of its proximity to the compact object. The resulting friction is so significant that it heats the inner disk to temperatures at which it emits vast amounts of electromagnetic radiation (mainly X-rays). These bright X-ray sources may be detected by telescopes. This process of accretion is one of the most efficient energy-producing processes known; up to 40% of the rest mass of the accreted material can be emitted as radiation. (In nuclear fusion only about 0.7% of the rest mass will be emitted as energy.) In many cases, accretion disks are accompanied by relativistic jets that are emitted along the poles, which carry away much of the energy. The mechanism for the creation of these jets is currently not well understood, in part due to insufficient data.As such, many of the universe's more energetic phenomena have been attributed to the accretion of matter on black holes. In particular, active galactic nuclei and quasars are believed to be the accretion disks of supermassive black holes. Similarly, X-ray binaries are generally accepted to be binary star systems in which one of the two stars is a compact object accreting matter from its companion. It has also been suggested that some ultraluminous X-ray sources may be the accretion disks of intermediate-mass black holes.In November 2011 the first direct observation of a quasar accretion disk around a supermassive black hole was reported.\n\n\n==== X-ray binaries ====\n\nX-ray binaries are binary star systems that emit a majority of their radiation in the X-ray part of the spectrum. These X-ray emissions are generally thought to result when one of the stars (compact object) accretes matter from another (regular) star. The presence of an ordinary star in such a system provides an opportunity for studying the central object and to determine if it might be a black hole.If such a system emits signals that can be directly traced back to the compact object, it cannot be a black hole. The absence of such a signal does, however, not exclude the possibility that the compact object is a neutron star. By studying the companion star it is often possible to obtain the orbital parameters of the system and to obtain an estimate for the mass of the compact object. If this is much larger than the Tolman–Oppenheimer–Volkoff limit (the maximum mass a star can have without collapsing) then the object cannot be a neutron star and is generally expected to be a black hole.The first strong candidate for a black hole, Cygnus X-1, was discovered in this way by Charles Thomas Bolton, Louise Webster, and Paul Murdin in 1972. Some doubt, however, remained due to the uncertainties that result from the companion star being much heavier than the candidate black hole. Currently, better candidates for black holes are found in a class of X-ray binaries called soft X-ray transients. In this class of system, the companion star is of relatively low mass allowing for more accurate estimates of the black hole mass. Moreover, these systems actively emit X-rays for only several months once every 10–50 years. During the period of low X-ray emission (called quiescence), the accretion disk is extremely faint allowing detailed observation of the companion star during this period. One of the best such candidates is V404 Cygni.\n\n\n===== Quasi-periodic oscillations =====\n\nThe X-ray emissions from accretion disks sometimes flicker at certain frequencies. These signals are called quasi-periodic oscillations and are thought to be caused by material moving along the inner edge of the accretion disk (the innermost stable circular orbit). As such their frequency is linked to the mass of the compact object. They can thus be used as an alternative way to determine the mass of candidate black holes.\n\n\n==== Galactic nuclei ====\n\nAstronomers use the term \"active galaxy\" to describe galaxies with unusual characteristics, such as unusual spectral line emission and very strong radio emission. Theoretical and observational studies have shown that the activity in these active galactic nuclei (AGN) may be explained by the presence of supermassive black holes, which can be millions of times more massive than stellar ones. The models of these AGN consist of a central black hole that may be millions or billions of times more massive than the Sun; a disk of interstellar gas and dust called an accretion disk; and two jets perpendicular to the accretion disk.\n\nAlthough supermassive black holes are expected to be found in most AGN, only some galaxies' nuclei have been more carefully studied in attempts to both identify and measure the actual masses of the central supermassive black hole candidates. Some of the most notable galaxies with supermassive black hole candidates include the Andromeda Galaxy, M32, M87, NGC 3115, NGC 3377, NGC 4258, NGC 4889, NGC 1277, OJ 287, APM 08279+5255 and the Sombrero Galaxy.It is now widely accepted that the centre of nearly every galaxy, not just active ones, contains a supermassive black hole. The close observational correlation between the mass of this hole and the velocity dispersion of the host galaxy's bulge, known as the M–sigma relation, strongly suggests a connection between the formation of the black hole and that of the galaxy itself.\n\n\n=== Microlensing ===\nAnother way the black hole nature of an object may be tested is through observation of effects caused by a strong gravitational field in their vicinity. One such effect is gravitational lensing: The deformation of spacetime around a massive object causes light rays to be deflected, such as light passing through an optic lens. Observations have been made of weak gravitational lensing, in which light rays are deflected by only a few arcseconds. Microlensing occurs when the sources are unresolved and the observer sees a small brightening. In January 2022, astronomers reported the first possible detection of a microlensing event from an isolated black hole.Another possibility for observing gravitational lensing by a black hole would be to observe stars orbiting the black hole. There are several candidates for such an observation in orbit around Sagittarius A*.\n\n\n== Alternatives ==\n\nThe evidence for stellar black holes strongly relies on the existence of an upper limit for the mass of a neutron star. The size of this limit heavily depends on the assumptions made about the properties of dense matter. New exotic phases of matter could push up this bound. A phase of free quarks at high density might allow the existence of dense quark stars, and some supersymmetric models predict the existence of Q stars. Some extensions of the standard model posit the existence of preons as fundamental building blocks of quarks and leptons, which could hypothetically form preon stars. These hypothetical models could potentially explain a number of observations of stellar black hole candidates. However, it can be shown from arguments in general relativity that any such object will have a maximum mass.Since the average density of a black hole inside its Schwarzschild radius is inversely proportional to the square of its mass, supermassive black holes are much less dense than stellar black holes (the average density of a 108 M☉ black hole is comparable to that of water). Consequently, the physics of matter forming a supermassive black hole is much better understood and the possible alternative explanations for supermassive black hole observations are much more mundane. For example, a supermassive black hole could be modelled by a large cluster of very dark objects. However, such alternatives are typically not stable enough to explain the supermassive black hole candidates.The evidence for the existence of stellar and supermassive black holes implies that in order for black holes to not form, general relativity must fail as a theory of gravity, perhaps due to the onset of quantum mechanical corrections. A much anticipated feature of a theory of quantum gravity is that it will not feature singularities or event horizons and thus black holes would not be real artifacts. For example, in the fuzzball model based on string theory, the individual states of a black hole solution do not generally have an event horizon or singularity, but for a classical/semi-classical observer the statistical average of such states appears just as an ordinary black hole as deduced from general relativity.A few theoretical objects have been conjectured to match observations of astronomical black hole candidates identically or near-identically, but which function via a different mechanism. These include the gravastar, the black star, and the dark-energy star.\n\n\n== Open questions ==\n\n\n=== Entropy and thermodynamics ===\n\nIn 1971, Hawking showed under general conditions that the total area of the event horizons of any collection of classical black holes can never decrease, even if they collide and merge. This result, now known as the second law of black hole mechanics, is remarkably similar to the second law of thermodynamics, which states that the total entropy of an isolated system can never decrease. As with classical objects at absolute zero temperature, it was assumed that black holes had zero entropy. If this were the case, the second law of thermodynamics would be violated by entropy-laden matter entering a black hole, resulting in a decrease in the total entropy of the universe. Therefore, Bekenstein proposed that a black hole should have an entropy, and that it should be proportional to its horizon area.The link with the laws of thermodynamics was further strengthened by Hawking's discovery in 1974 that quantum field theory predicts that a black hole radiates blackbody radiation at a constant temperature. This seemingly causes a violation of the second law of black hole mechanics, since the radiation will carry away energy from the black hole causing it to shrink. The radiation, however also carries away entropy, and it can be proven under general assumptions that the sum of the entropy of the matter surrounding a black hole and one quarter of the area of the horizon as measured in Planck units is in fact always increasing. This allows the formulation of the first law of black hole mechanics as an analogue of the first law of thermodynamics, with the mass acting as energy, the surface gravity as temperature and the area as entropy.One puzzling feature is that the entropy of a black hole scales with its area rather than with its volume, since entropy is normally an extensive quantity that scales linearly with the volume of the system. This odd property led Gerard 't Hooft and Leonard Susskind to propose the holographic principle, which suggests that anything that happens in a volume of spacetime can be described by data on the boundary of that volume.Although general relativity can be used to perform a semi-classical calculation of black hole entropy, this situation is theoretically unsatisfying. In statistical mechanics, entropy is understood as counting the number of microscopic configurations of a system that have the same macroscopic qualities (such as mass, charge, pressure, etc.). Without a satisfactory theory of quantum gravity, one cannot perform such a computation for black holes. Some progress has been made in various approaches to quantum gravity. In 1995, Andrew Strominger and Cumrun Vafa showed that counting the microstates of a specific supersymmetric black hole in string theory reproduced the Bekenstein–Hawking entropy. Since then, similar results have been reported for different black holes both in string theory and in other approaches to quantum gravity like loop quantum gravity.Another promising approach is constituted by treating gravity as an effective field theory. One first computes the quantum gravitational corrections to the radius of the event horizon of the black hole, then integrates over it to find the quantum gravitational corrections to the entropy as given by the Wald formula. The method was applied for Schwarzschild black holes by Calmet and Kuipers, then successfully generalised for charged black holes by Campos Delgado.\n\n\n=== Information loss paradox ===\n\nBecause a black hole has only a few internal parameters, most of the information about the matter that went into forming the black hole is lost. Regardless of the type of matter which goes into a black hole, it appears that only information concerning the total mass, charge, and angular momentum are conserved. As long as black holes were thought to persist forever this information loss is not that problematic, as the information can be thought of as existing inside the black hole, inaccessible from the outside, but represented on the event horizon in accordance with the holographic principle. However, black holes slowly evaporate by emitting Hawking radiation. This radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information appears to be gone forever.The question whether information is truly lost in black holes (the black hole information paradox) has divided the theoretical physics community. In quantum mechanics, loss of information corresponds to the violation of a property called unitarity, and it has been argued that loss of unitarity would also imply violation of conservation of energy, though this has also been disputed. Over recent years evidence has been building that indeed information and unitarity are preserved in a full quantum gravitational treatment of the problem.One attempt to resolve the black hole information paradox is known as black hole complementarity. In 2012, the \"firewall paradox\" was introduced with the goal of demonstrating that black hole complementarity fails to solve the information paradox. According to quantum field theory in curved spacetime, a single emission of Hawking radiation involves two mutually entangled particles. The outgoing particle escapes and is emitted as a quantum of Hawking radiation; the infalling particle is swallowed by the black hole. Assume a black hole formed a finite time in the past and will fully evaporate away in some finite time in the future. Then, it will emit only a finite amount of information encoded within its Hawking radiation. According to research by physicists like Don Page and Leonard Susskind, there will eventually be a time by which an outgoing particle must be entangled with all the Hawking radiation the black hole has previously emitted. This seemingly creates a paradox: a principle called \"monogamy of entanglement\" requires that, like any quantum system, the outgoing particle cannot be fully entangled with two other systems at the same time; yet here the outgoing particle appears to be entangled both with the infalling particle and, independently, with past Hawking radiation. In order to resolve this contradiction, physicists may eventually be forced to give up one of three time-tested principles: Einstein's equivalence principle, unitarity, or local quantum field theory. One possible solution, which violates the equivalence principle, is that a \"firewall\" destroys incoming particles at the event horizon. In general, which—if any—of these assumptions should be abandoned remains a topic of debate.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n=== Popular reading ===\n\n\n=== University textbooks and monographs ===\n\n\n=== Review papers ===\n\n\n== External links ==\n\nBlack Holes on In Our Time at the BBC\nStanford Encyclopedia of Philosophy: \"Singularities and Black Holes\" by Erik Curiel and Peter Bokulich.\nBlack Holes: Gravity's Relentless Pull – Interactive multimedia Web site about the physics and astronomy of black holes from the Space Telescope Science Institute (HubbleSite)\nESA's Black Hole Visualization Archived 3 May 2019 at the Wayback Machine\nFrequently Asked Questions (FAQs) on Black Holes\nSchwarzschild Geometry\nBlack holes - basic (NYT; April 2021)\n\n\n=== Videos ===\n16-year-long study tracks stars orbiting Sagittarius A*\nMovie of Black Hole Candidate from Max Planck Institute\nCowen, Ron (20 April 2015). \"3D simulations of colliding black holes hailed as most realistic yet\". Nature. doi:10.1038/nature.2015.17360.\nComputer visualisation of the signal detected by LIGO\nTwo Black Holes Merge into One (based upon the signal GW150914)"
    },
    "impressionism": {
        "url": "https://en.wikipedia.org/wiki/Impressionism",
        "summary": "Impressionism was a 19th-century art movement characterized by relatively small, thin, yet visible brush strokes, open composition, emphasis on accurate depiction of light in its changing qualities (often accentuating the effects of the passage of time), ordinary subject matter, unusual visual angles, and inclusion of movement as a crucial element of human perception and experience. Impressionism originated with a group of Paris-based artists whose independent exhibitions brought them to prominence during the 1870s and 1880s.\nThe Impressionists faced harsh opposition from the conventional art community in France.",
        "content": "Impressionism was a 19th-century art movement characterized by relatively small, thin, yet visible brush strokes, open composition, emphasis on accurate depiction of light in its changing qualities (often accentuating the effects of the passage of time), ordinary subject matter, unusual visual angles, and inclusion of movement as a crucial element of human perception and experience. Impressionism originated with a group of Paris-based artists whose independent exhibitions brought them to prominence during the 1870s and 1880s.\nThe Impressionists faced harsh opposition from the conventional art community in France. The name of the style derives from the title of a Claude Monet work, Impression, soleil levant (Impression, Sunrise), which provoked the critic Louis Leroy to coin the term in a satirical review published in the Parisian newspaper Le Charivari. The development of Impressionism in the visual arts was soon followed by analogous styles in other media that became known as impressionist music and impressionist literature.\n\n\n== Overview ==\n\nRadicals in their time, early Impressionists violated the rules of academic painting. They constructed their pictures from freely brushed colours that took precedence over lines and contours, following the example of painters such as Eugène Delacroix and J. M. W. Turner. They also painted realistic scenes of modern life, and often painted outdoors. Previously, still lifes and portraits as well as landscapes were usually painted in a studio. The Impressionists found that they could capture the momentary and transient effects of sunlight by painting outdoors or en plein air. They portrayed overall visual effects instead of details, and used short \"broken\" brush strokes of mixed and pure unmixed colour—not blended smoothly or shaded, as was customary—to achieve an effect of intense colour vibration.\n\nImpressionism emerged in France at the same time that a number of other painters, including the Italian artists known as the Macchiaioli, and Winslow Homer in the United States, were also exploring plein-air painting. The Impressionists, however, developed new techniques specific to the style. Encompassing what its adherents argued was a different way of seeing, it is an art of immediacy and movement, of candid poses and compositions, of the play of light expressed in a bright and varied use of colour.\nThe public, at first hostile, gradually came to believe that the Impressionists had captured a fresh and original vision, even if the art critics and art establishment disapproved of the new style. By recreating the sensation in the eye that views the subject, rather than delineating the details of the subject, and by creating a welter of techniques and forms, Impressionism is a precursor of various painting styles, including Neo-Impressionism, Post-Impressionism, Fauvism, and Cubism.\n\n\n== Beginnings ==\nIn the middle of the 19th century—a time of change, as Emperor Napoleon III rebuilt Paris and waged war—the Académie des Beaux-Arts dominated French art. The Académie was the preserver of traditional French painting standards of content and style. Historical subjects, religious themes, and portraits were valued; landscape and still life were not. The Académie preferred carefully finished images that looked realistic when examined closely. Paintings in this style were made up of precise brush strokes carefully blended to hide the artist's hand in the work. Colour was restrained and often toned down further by the application of a golden varnish.The Académie had an annual, juried art show, the Salon de Paris, and artists whose work was displayed in the show won prizes, garnered commissions, and enhanced their prestige. The standards of the juries represented the values of the Académie, represented by the works of such artists as Jean-Léon Gérôme and Alexandre Cabanel.\nIn the early 1860s, four young painters—Claude Monet, Pierre-Auguste Renoir, Alfred Sisley, and Frédéric Bazille—met while studying under the academic artist Charles Gleyre. They discovered that they shared an interest in painting landscape and contemporary life rather than historical or mythological scenes. Following a practice—pioneered by artists such as the Englishman John Constable— that had become increasingly popular by mid-century, they often ventured into the countryside together to paint in the open air. Their purpose was not to make sketches to be developed into carefully finished works in the studio, as was the usual custom, but to complete their paintings out-of-doors. By painting in sunlight directly from nature, and making bold use of the vivid synthetic pigments that had become available since the beginning of the century, they began to develop a lighter and brighter manner of painting that extended further the Realism of Gustave Courbet and the Barbizon school. A favourite meeting place for the artists was the Café Guerbois on Avenue de Clichy in Paris, where the discussions were often led by Édouard Manet, whom the younger artists greatly admired. They were soon joined by Camille Pissarro, Paul Cézanne, and Armand Guillaumin.\n\nDuring the 1860s, the Salon jury routinely rejected about half of the works submitted by Monet and his friends in favour of works by artists faithful to the approved style. In 1863, the Salon jury rejected Manet's The Luncheon on the Grass (Le déjeuner sur l'herbe) primarily because it depicted a nude woman with two clothed men at a picnic. While the Salon jury routinely accepted nudes in historical and allegorical paintings, they condemned Manet for placing a realistic nude in a contemporary setting. The jury's severely worded rejection of Manet's painting appalled his admirers, and the unusually large number of rejected works that year perturbed many French artists.\nAfter Emperor Napoleon III saw the rejected works of 1863, he decreed that the public be allowed to judge the work themselves, and the Salon des Refusés (Salon of the Refused) was organized. While many viewers came only to laugh, the Salon des Refusés drew attention to the existence of a new tendency in art and attracted more visitors than the regular Salon.\n\nArtists' petitions requesting a new Salon des Refusés in 1867, and again in 1872, were denied. In December 1873, Monet, Renoir, Pissarro, Sisley, Cézanne, Berthe Morisot, Edgar Degas and several other artists founded the Société Anonyme Coopérative des Artistes Peintres, Sculpteurs, Graveurs (\"Cooperative and Anonymous Association of Painters, Sculptors, and Engravers\") to exhibit their artworks independently. Members of the association were expected to forswear participation in the Salon. The organizers invited a number of other progressive artists to join them in their inaugural exhibition, including the older Eugène Boudin, whose example had first persuaded Monet to adopt plein air painting years before. Another painter who greatly influenced Monet and his friends, Johan Jongkind, declined to participate, as did Édouard Manet. In total, thirty artists participated in their first exhibition, held in April 1874 at the studio of the photographer Nadar.\n\nThe critical response was mixed. Monet and Cézanne received the harshest attacks. Critic and humorist Louis Leroy wrote a scathing review in the newspaper Le Charivari in which, making wordplay with the title of Claude Monet's Impression, Sunrise (Impression, soleil levant), he gave the artists the name by which they became known. Derisively titling his article \"The Exhibition of the Impressionists\", Leroy declared that Monet's painting was at most, a sketch, and could hardly be termed a finished work.\nHe wrote, in the form of a dialogue between viewers,\n\n\"Impression—I was certain of it. I was just telling myself that, since I was impressed, there had to be some impression in it ... and what freedom, what ease of workmanship! Wallpaper in its embryonic state is more finished than that seascape.\"\nThe term Impressionist quickly gained favour with the public. It was also accepted by the artists themselves, even though they were a diverse group in style and temperament, unified primarily by their spirit of independence and rebellion. They exhibited together—albeit with shifting membership—eight times between 1874 and 1886. The Impressionists' style, with its loose, spontaneous brushstrokes, would soon become synonymous with modern life.Monet, Sisley, Morisot, and Pissarro may be considered the \"purest\" Impressionists, in their consistent pursuit of an art of spontaneity, sunlight, and colour. Degas rejected much of this, as he believed in the primacy of drawing over colour and belittled the practice of painting outdoors. Renoir turned away from Impressionism for a time during the 1880s, and never entirely regained his commitment to its ideas. Édouard Manet, although regarded by the Impressionists as their leader, never abandoned his liberal use of black as a colour (while Impressionists avoided its use and preferred to obtain darker colours by mixing), and never participated in the Impressionist exhibitions. He continued to submit his works to the Salon, where his painting Spanish Singer had won a 2nd class medal in 1861, and he urged the others to do likewise, arguing that \"the Salon is the real field of battle\" where a reputation could be made.\n\nAmong the artists of the core group (minus Bazille, who had died in the Franco-Prussian War in 1870), defections occurred as Cézanne, followed later by Renoir, Sisley, and Monet, abstained from the group exhibitions so they could submit their works to the Salon. Disagreements arose from issues such as Guillaumin's membership in the group, championed by Pissarro and Cézanne against opposition from Monet and Degas, who thought him unworthy. Degas invited Mary Cassatt to display her work in the 1879 exhibition, but also insisted on the inclusion of Jean-François Raffaëlli, Ludovic Lepic, and other realists who did not represent Impressionist practices, causing Monet in 1880 to accuse the Impressionists of \"opening doors to first-come daubers\". In this regard, the seventh Paris Impressionist exhibition in 1882 was the most selective of all including the works of only nine \"true\" impressionists, namely Gustave Caillebotte, Paul Gauguin, Armand Guillaumin, Claude Monet, Berthe Morisot, Camille Pissarro, Pierre-Auguste Renoir, Alfred Sisley, and Victor Vignon. The group then divided again over the invitations to Paul Signac and Georges Seurat to exhibit with them at the 8th Impressionist exhibition in 1886. Pissarro was the only artist to show at all eight Paris Impressionist exhibitions.\nThe individual artists achieved few financial rewards from the Impressionist exhibitions, but their art gradually won a degree of public acceptance and support. Their dealer, Durand-Ruel, played a major role in this as he kept their work before the public and arranged shows for them in London and New York. Although Sisley died in poverty in 1899, Renoir had a great Salon success in 1879. Monet became secure financially during the early 1880s and so did Pissarro by the early 1890s. By this time the methods of Impressionist painting, in a diluted form, had become commonplace in Salon art.\n\n\n== Impressionist techniques ==\n\nFrench painters who prepared the way for Impressionism include the Romantic colourist Eugène Delacroix, the leader of the realists Gustave Courbet, and painters of the Barbizon school such as Théodore Rousseau. The Impressionists learned much from the work of Johan Barthold Jongkind, Jean-Baptiste-Camille Corot and Eugène Boudin, who painted from nature in a direct and spontaneous style that prefigured Impressionism, and who befriended and advised the younger artists.\nA number of identifiable techniques and working habits contributed to the innovative style of the Impressionists. Although these methods had been used by previous artists—and are often conspicuous in the work of artists such as Frans Hals, Diego Velázquez, Peter Paul Rubens, John Constable, and J. M. W. Turner—the Impressionists were the first to use them all together, and with such consistency. These techniques include:\n\nShort, thick strokes of paint quickly capture the essence of the subject, rather than its details. The paint is often applied impasto.\nColours are applied side by side with as little mixing as possible, a technique that exploits the principle of simultaneous contrast to make the colour appear more vivid to the viewer.\nGreys and dark tones are produced by mixing complementary colours. Pure impressionism avoids the use of black paint.\nWet paint is placed into wet paint without waiting for successive applications to dry, producing softer edges and intermingling of colour.\nImpressionist paintings do not exploit the transparency of thin paint films (glazes), which earlier artists manipulated carefully to produce effects. The impressionist painting surface is typically opaque.\nThe paint is applied to a white or light-coloured ground. Previously, painters often used dark grey or strongly coloured grounds.\nThe play of natural light is emphasized. Close attention is paid to the reflection of colours from object to object. Painters often worked in the evening to produce effets de soir—the shadowy effects of evening or twilight.\nIn paintings made en plein air (outdoors), shadows are boldly painted with the blue of the sky as it is reflected onto surfaces, giving a sense of freshness previously not represented in painting. (Blue shadows on snow inspired the technique.)New technology played a role in the development of the style. Impressionists took advantage of the mid-century introduction of premixed paints in tin tubes (resembling modern toothpaste tubes), which allowed artists to work more spontaneously, both outdoors and indoors. Previously, painters made their own paints individually, by grinding and mixing dry pigment powders with linseed oil, which were then stored in animal bladders.Many vivid synthetic pigments became commercially available to artists for the first time during the 19th century. These included cobalt blue, viridian, cadmium yellow, and synthetic ultramarine blue, all of which were in use by the 1840s, before Impressionism. The Impressionists' manner of painting made bold use of these pigments, and of even newer colours such as cerulean blue, which became commercially available to artists in the 1860s.The Impressionists' progress toward a brighter style of painting was gradual. During the 1860s, Monet and Renoir sometimes painted on canvases prepared with the traditional red-brown or grey ground. By the 1870s, Monet, Renoir, and Pissarro usually chose to paint on grounds of a lighter grey or beige colour, which functioned as a middle tone in the finished painting. By the 1880s, some of the Impressionists had come to prefer white or slightly off-white grounds, and no longer allowed the ground colour a significant role in the finished painting.\n\n\n== Content and composition ==\n\nPrior to the Impressionists, other painters, notably such 17th-century Dutch painters as Jan Steen, had emphasized common subjects, but their methods of composition were traditional. They arranged their compositions so that the main subject commanded the viewer's attention. J. M. W. Turner, while an artist of the Romantic era, anticipated the style of impressionism with his artwork. The Impressionists relaxed the boundary between subject and background so that the effect of an Impressionist painting often resembles a snapshot, a part of a larger reality captured as if by chance. Photography was gaining popularity, and as cameras became more portable, photographs became more candid. Photography inspired Impressionists to represent momentary action, not only in the fleeting lights of a landscape, but in the day-to-day lives of people.\n\nThe development of Impressionism can be considered partly as a reaction by artists to the challenge presented by photography, which seemed to devalue the artist's skill in reproducing reality. Both portrait and landscape paintings were deemed somewhat deficient and lacking in truth as photography \"produced lifelike images much more efficiently and reliably\".In spite of this, photography actually inspired artists to pursue other means of creative expression, and rather than compete with photography to emulate reality, artists focused \"on the one thing they could inevitably do better than the photograph—by further developing into an art form its very subjectivity in the conception of the image, the very subjectivity that photography eliminated\". The Impressionists sought to express their perceptions of nature, rather than create exact representations. This allowed artists to depict subjectively what they saw with their \"tacit imperatives of taste and conscience\". Photography encouraged painters to exploit aspects of the painting medium, like colour, which photography then lacked: \"The Impressionists were the first to consciously offer a subjective alternative to the photograph\".\n\nAnother major influence was Japanese ukiyo-e art prints (Japonism). The art of these prints contributed significantly to the \"snapshot\" angles and unconventional compositions that became characteristic of Impressionism. An example is Monet's Jardin à Sainte-Adresse, 1867, with its bold blocks of colour and composition on a strong diagonal slant showing the influence of Japanese prints.Edgar Degas was both an avid photographer and a collector of Japanese prints. His The Dance Class (La classe de danse) of 1874 shows both influences in its asymmetrical composition. The dancers are seemingly caught off guard in various awkward poses, leaving an expanse of empty floor space in the lower right quadrant. He also captured his dancers in sculpture, such as the Little Dancer of Fourteen Years.\n\n\n== Female Impressionists ==\n\nImpressionists, in varying degrees, were looking for ways to depict visual experience and contemporary subjects. Female Impressionists were interested in these same ideals but had many social and career limitations compared to male Impressionists. They were particularly excluded from the imagery of the bourgeois social sphere of the boulevard, cafe, and dance hall. As well as imagery, women were excluded from the formative discussions that resulted in meetings in those places; that was where male Impressionists were able to form and share ideas about Impressionism. In the academic realm, women were believed to be incapable of handling complex subjects which led teachers to restrict what they taught female students. It was also considered unladylike to excel in art since women's true talents were then believed to center on homemaking and mothering.Yet several women were able to find success during their lifetime, even though their careers were affected by personal circumstances – Bracquemond, for example, had a husband who was resentful of her work which caused her to give up painting. The four most well known, namely, Mary Cassatt, Eva Gonzalès, Marie Bracquemond, and Berthe Morisot, are, and were, often referred to as the 'Women Impressionists'. Their participation in the series of eight Impressionist exhibitions that took place in Paris from 1874 to 1886 varied: Morisot participated in seven, Cassatt in four, Bracquemond in three, and Gonzalès did not participate.\n\nThe critics of the time lumped these four together without regard to their personal styles, techniques, or subject matter. Critics viewing their works at the exhibitions often attempted to acknowledge the women artists' talents but circumscribed them within a limited notion of femininity. Arguing for the suitability of Impressionist technique to women's manner of perception, Parisian critic S.C. de Soissons wrote:One can understand that women have no originality of thought, and that literature and music have no feminine character; but surely women know how to observe, and what they see is quite different from that which men see, and the art which they put in their gestures, in their toilet, in the decoration of their environment is sufficient to give is the idea of an instinctive, of a peculiar genius which resides in each one of them.While Impressionism legitimized the domestic social life as subject matter, of which women had intimate knowledge, it also tended to limit them to that subject matter. Portrayals of often-identifiable sitters in domestic settings (which could offer commissions) were dominant in the exhibitions. The subjects of the paintings were often women interacting with their environment by either their gaze or movement. Cassatt, in particular, was aware of her placement of subjects: she kept her predominantly female figures from objectification and cliche; when they are not reading, they converse, sew, drink tea, and when they are inactive, they seem lost in thought.The women Impressionists, like their male counterparts, were striving for \"truth,\" for new ways of seeing and new painting techniques; each artist had an individual painting style. Women Impressionists (particularly Morisot and Cassatt) were conscious of the balance of power between women and objects in their paintings – the bourgeois women depicted are not defined by decorative objects, but instead, interact with and dominate the things with which they live. There are many similarities in their depictions of women who seem both at ease and subtly confined. Gonzalès' Box at the Italian Opera depicts a woman staring into the distance, at ease in a social sphere but confined by the box and the man standing next to her. Cassatt's painting Young Girl at a Window is brighter in color but remains constrained by the canvas edge as she looks out the window.\n\nDespite their success in their ability to have a career and Impressionism's demise attributed to its allegedly feminine characteristics (its sensuality, dependence on sensation, physicality, and fluidity) the four women artists (and other, lesser-known women Impressionists) were largely omitted from art historical textbooks covering Impressionist artists until Tamar Garb's Women Impressionists published in 1986. For example, Impressionism by Jean Leymarie, published in 1955 included no information on any women Impressionists.\nPainter Androniqi Zengo Antoniu is co-credited with the introduction of impressionism to Albania.\n\n\n== Prominent Impressionists ==\nThe central figures in the development of Impressionism in France, listed alphabetically, were:\n\nFrédéric Bazille (1841–1870), who only posthumously participated in the Impressionist exhibitions\nGustave Caillebotte (1848–1894), who, younger than the others, joined forces with them in the mid-1870s\nMary Cassatt (1844–1926), American-born, she lived in Paris and participated in four Impressionist exhibitions\nPaul Cézanne (1839–1906), although he later broke away from the Impressionists\nEdgar Degas (1834–1917), who despised the term Impressionist\nArmand Guillaumin (1841–1927)\nÉdouard Manet (1832–1883), who did not participate in any of the Impressionist exhibitions\nClaude Monet (1840–1926), the most prolific of the Impressionists and the one who embodies their aesthetic most obviously\nBerthe Morisot (1841–1895) who participated in all Impressionist exhibitions except in 1879\nCamille Pissarro (1830–1903)\nPierre-Auguste Renoir (1841–1919), who participated in Impressionist exhibitions in 1874, 1876, 1877 and 1882\nAlfred Sisley (1839–1899)\n\n\n== Gallery ==\n\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\n\n== Timeline: lives of the Impressionists ==\n\n\n== Associates and influenced artists ==\n\nAmong the close associates of the Impressionists, Victor Vignon is the only artist outside the group of prominent names who participated to the most exclusive Seventh Paris Impressionist Exhibition in 1882, which was indeed a rejection to the previous less restricted exhibitions chiefly organized by Degas. Originally from the school of Corot, Vignon was a friend of Camille Pissarro, whose influence is evident in his impressionist style after the late 1870s, and a friend of post-impressionist Vincent van Gogh.\nThere were several other close associates of the Impressionists who adopted their methods to some degree. These include Jean-Louis Forain (who participated in Impressionist exhibitions in 1879, 1880, 1881 and 1886) and Giuseppe De Nittis, an Italian artist living in Paris who participated in the first Impressionist exhibit at the invitation of Degas, although the other Impressionists disparaged his work. Federico Zandomeneghi was another Italian friend of Degas who showed with the Impressionists. Eva Gonzalès was a follower of Manet who did not exhibit with the group. James Abbott McNeill Whistler was an American-born painter who played a part in Impressionism although he did not join the group and preferred grayed colours. Walter Sickert, an English artist, was initially a follower of Whistler, and later an important disciple of Degas; he did not exhibit with the Impressionists. In 1904 the artist and writer Wynford Dewhurst wrote the first important study of the French painters published in English, Impressionist Painting: its genesis and development, which did much to popularize Impressionism in Great Britain.\nBy the early 1880s, Impressionist methods were affecting, at least superficially, the art of the Salon. Fashionable painters such as Jean Béraud and Henri Gervex found critical and financial success by brightening their palettes while retaining the smooth finish expected of Salon art. Works by these artists are sometimes casually referred to as Impressionism, despite their remoteness from Impressionist practice.\nThe influence of the French Impressionists lasted long after most of them had died. Artists like J.D. Kirszenbaum were borrowing Impressionist techniques throughout the twentieth century.\n\n\n== Beyond France ==\n\nAs the influence of Impressionism spread beyond France, artists, too numerous to list, became identified as practitioners of the new style. Some of the more important examples are:\n\nThe American Impressionists, including Mary Cassatt, William Merritt Chase, Frederick Carl Frieseke, Childe Hassam, Willard Metcalf, Lilla Cabot Perry, Theodore Robinson, Edmund Charles Tarbell, John Henry Twachtman, Catherine Wiley and J. Alden Weir.\nThe Australian Impressionists, including Tom Roberts, Arthur Streeton, Walter Withers, Charles Conder and Frederick McCubbin (who were prominent members of the Heidelberg School), and John Russell, a friend of Van Gogh, Rodin, Monet and Matisse.\nThe Amsterdam Impressionists in the Netherlands, including George Hendrik Breitner, Isaac Israëls, Willem Bastiaan Tholen, Willem de Zwart, Willem Witsen and Jan Toorop.\nThe California Impressionists, including William Wendt, Guy Rose, Alson Clark, Donna N. Schuster, and Sam Hyde Harris.\nAnna Boch, Vincent van Gogh's friend Eugène Boch, Georges Lemmen and Théo van Rysselberghe, Impressionist painters from Belgium.\nIvan Grohar, Rihard Jakopič, Matija Jama, and Matej Sternen, Impressionists from Slovenia. Their beginning was in the school of Anton Ažbe in Munich and they were influenced by Jurij Šubic and Ivana Kobilca, Slovenian painters working in Paris.\nWynford Dewhurst, Walter Richard Sickert, and Philip Wilson Steer were well known Impressionist painters from the United Kingdom. Pierre Adolphe Valette, who was born in France but who worked in Manchester, was the tutor of L. S. Lowry.\nThe German Impressionists, including Max Liebermann, Lovis Corinth, Ernst Oppler, Max Slevogt and August von Brandis.\nLászló Mednyánszky and Pál Szinyei-Merse in Hungary\nTheodor von Ehrmanns and Hugo Charlemont who were rare Impressionists among the more dominant Vienna Secessionist painters in Austria.\nWilliam John Leech, Roderic O'Conor, and Walter Osborne in Ireland\nKonstantin Korovin and Valentin Serov in Russia\nFrancisco Oller y Cestero, a native of Puerto Rico and a friend of Pissarro and Cézanne\nJames Nairn in New Zealand\nWilliam McTaggart in Scotland\nLaura Muntz Lyall, a Canadian artist\nWładysław Podkowiński, a Polish Impressionist and symbolist\nNicolae Grigorescu in Romania\nNazmi Ziya Güran, who brought Impressionism to Turkey\nChafik Charobim in Egypt\nEliseu Visconti in Brazil\nJoaquín Sorolla in Spain\nFaustino Brughetti, Fernando Fader, Candido Lopez, Martín Malharro, Walter de Navazio, Ramón Silva in Argentina\nSkagen Painters a group of Scandinavian artists who painted in a small Danish fishing village\nNadežda Petrović, Milo Milunović, Kosta Miličević, Milan Milovanovi and Mališa Glišić in Serbia\nÁsgrímur Jónsson in Iceland\nFujishima Takeji in Japan\nFrits Thaulow in Norway and later France\n\n\n== Sculpture, photography and film ==\nThe sculptor Auguste Rodin is sometimes called an Impressionist for the way he used roughly modeled surfaces to suggest transient light effects.Pictorialist photographers whose work is characterized by soft focus and atmospheric effects have also been called Impressionists.\nFrench Impressionist Cinema is a term applied to a loosely defined group of films and filmmakers in France from 1919 to 1929, although these years are debatable. French Impressionist filmmakers include Abel Gance, Jean Epstein, Germaine Dulac, Marcel L’Herbier, Louis Delluc, and Dmitry Kirsanoff.\n\n\n== Music and literature ==\n\nMusical Impressionism is the name given to a movement in European classical music that arose in the late 19th century and continued into the middle of the 20th century. Originating in France, musical Impressionism is characterized by suggestion and atmosphere, and eschews the emotional excesses of the Romantic era. Impressionist composers favoured short forms such as the nocturne, arabesque, and prelude, and often explored uncommon scales such as the whole tone scale. Perhaps the most notable innovations of Impressionist composers were the introduction of major 7th chords and the extension of chord structures in 3rds to five- and six-part harmonies.\nThe influence of visual Impressionism on its musical counterpart is debatable. Claude Debussy and Maurice Ravel are generally considered the greatest Impressionist composers, but Debussy disavowed the term, calling it the invention of critics. Erik Satie was also considered in this category, though his approach was regarded as less serious, more musical novelty in nature. Paul Dukas is another French composer sometimes considered an Impressionist, but his style is perhaps more closely aligned to the late Romanticists. Musical Impressionism beyond France includes the work of such composers as Ottorino Respighi (Italy), Ralph Vaughan Williams, Cyril Scott, and John Ireland (England), Manuel De Falla and Isaac Albeniz (Spain), and Charles Griffes (America).\nThe term Impressionism has also been used to describe works of literature in which a few select details suffice to convey the sensory impressions of an incident or scene. Impressionist literature is closely related to Symbolism, with its major exemplars being Baudelaire, Mallarmé, Rimbaud, and Verlaine. Authors such as Virginia Woolf, D.H. Lawrence, Henry James, and Joseph Conrad have written works that are Impressionistic in the way that they describe, rather than interpret, the impressions, sensations and emotions that constitute a character's mental life.\n\n\n== Post-Impressionism ==\n\nDuring the 1880s several artists began to develop different precepts for the use of colour, pattern, form, and line, derived from the Impressionist example: Vincent van Gogh, Paul Gauguin, Georges Seurat, and Henri de Toulouse-Lautrec. These artists were slightly younger than the Impressionists, and their work is known as post-Impressionism. Some of the original Impressionist artists also ventured into this new territory; Camille Pissarro briefly painted in a pointillist manner, and even Monet abandoned strict plein air painting. Paul Cézanne, who participated in the first and third Impressionist exhibitions, developed a highly individual vision emphasising pictorial structure, and he is more often called a post-Impressionist. Although these cases illustrate the difficulty of assigning labels, the work of the original Impressionist painters may, by definition, be categorised as Impressionism.\n\n\t\t\n\t\t\n\t\t\n\n\n== See also ==\nArt periods\nCantonese school of painting\nExpressionism (as a reaction to Impressionism)\nLes XX\nLuminism (Impressionism)\nHistory of Painting\nWestern Painting\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\n\nHecht Museum\n\n The French Impressionists (1860–1900) at Project Gutenberg\nMuseumsportal Schleswig-Holstein\nImpressionism : A Centenary Exhibition, the Metropolitan Museum of Art, December 12, 1974 – February 10, 1975, fully digitized text from The Metropolitan Museum of Art libraries\nSuburban Pastoral The Guardian, 24 February 2007\nImpressionism: Paintings collected by European Museums (1999) was an art exhibition co-organized by the High Museum of Art, Atlanta, the Seattle Art Museum, and the Denver Art Museum, touring from May through December 1999. Online guided tour\nMonet's Years at Giverny: Beyond Impressionism, 1978 exhibition catalogue fully online as PDF from The Metropolitan Museum of Art, which discusses Monet's role in this movement\nDegas: The Artist's Mind, 1976 exhibition catalogue fully online as PDF from The Metropolitan Museum of Art, which discusses Degas's role in this movement\nDefinition of impressionism on the Tate Art Glossary"
    },
    "ottoman empire": {
        "url": "https://en.wikipedia.org/wiki/Ottoman_Empire",
        "summary": "The Ottoman Empire, historically and colloquially the Turkish Empire, was an empire that controlled much of Southeast Europe, Western Asia, and Northern Africa between the 14th and early 20th centuries. It was founded at the end of the 13th century in northwestern Anatolia in the town of Söğüt (modern-day Bilecik Province) by the Turkoman tribal leader Osman I. After 1354, the Ottomans crossed into Europe and, with the conquest of the Balkans, the Ottoman beylik was transformed into a transcontinental empire. The Ottomans ended the Byzantine Empire with the conquest of Constantinople in 1453 by Mehmed the Conqueror.Under the reign of Suleiman the Magnificent, the Ottoman Empire marked the peak of its power and prosperity, as well as the highest development of its governmental, social, and economic systems.",
        "content": "The Ottoman Empire, historically and colloquially the Turkish Empire, was an empire that controlled much of Southeast Europe, Western Asia, and Northern Africa between the 14th and early 20th centuries. It was founded at the end of the 13th century in northwestern Anatolia in the town of Söğüt (modern-day Bilecik Province) by the Turkoman tribal leader Osman I. After 1354, the Ottomans crossed into Europe and, with the conquest of the Balkans, the Ottoman beylik was transformed into a transcontinental empire. The Ottomans ended the Byzantine Empire with the conquest of Constantinople in 1453 by Mehmed the Conqueror.Under the reign of Suleiman the Magnificent, the Ottoman Empire marked the peak of its power and prosperity, as well as the highest development of its governmental, social, and economic systems. At the beginning of the 17th century, the empire contained 32 provinces and numerous vassal states. Some of these were later absorbed into the Ottoman Empire, while others were granted various types of autonomy over the course of centuries. With Constantinople (modern-day Istanbul) as its capital and control of lands around the Mediterranean Basin, the Ottoman Empire was at the centre of interactions between the Middle East and Europe for six centuries.\nWhile the empire was once thought to have entered a period of decline following the death of Suleiman the Magnificent, this view is no longer supported by the majority of academic historians. The newer academic consensus posits that the empire continued to maintain a flexible and strong economy, society and military throughout the 17th and for much of the 18th century. However, during a long period of peace from 1740 to 1768, the Ottoman military system fell behind that of its European rivals, the Habsburg and Russian empires. The Ottomans consequently suffered severe military defeats in the late 18th and early 19th centuries. The successful Greek War of Independence concluded with decolonization of Greece following the London Protocol (1830) and Treaty of Constantinople (1832). This and other defeats prompted the Ottoman state to initiate a comprehensive process of reform and modernization known as the Tanzimat. Thus, over the course of the 19th century, the Ottoman state became vastly more powerful and organized internally, despite suffering further territorial losses, especially in the Balkans, where a number of new states emerged.The Committee of Union and Progress (CUP) established the Second Constitutional Era in the Young Turk Revolution in 1908, turning the Empire into a constitutional monarchy, which conducted competitive multi-party elections. However, after the disastrous Balkan Wars, the now radicalized and nationalistic CUP took over the government in the 1913 coup d'état, creating a one-party regime. The CUP allied the Empire with Germany, hoping to escape from the diplomatic isolation which had contributed to its recent territorial losses, and thus joined World War I on the side of the Central Powers. While the Empire was able to largely hold its own during the conflict, it was struggling with internal dissent, especially with the Arab Revolt in its Arabian holdings. During this time, the Ottoman government engaged in genocide against the Armenians, Assyrians, and Greeks. The Empire's defeat and the occupation of part of its territory by the Allied Powers in the aftermath of World War I resulted in its partitioning and the loss of its southern territories, which were divided between the United Kingdom and France. The successful Turkish War of Independence, led by Mustafa Kemal Atatürk against the occupying Allies, led to the emergence of the Republic of Turkey in the Anatolian heartland and the abolition of the Ottoman monarchy.\n\n\n== Name ==\n\nThe word Ottoman is a historical anglicisation of the name of Osman I, the founder of the Empire and of the ruling House of Osman (also known as the Ottoman dynasty). Osman's name in turn was the Turkish form of the Arabic name ʿUthmān (عثمان). In Ottoman Turkish, the empire was referred to as Devlet-i ʿAlīye-yi ʿOsmānīye (دولت عليه عثمانیه), literally \"The Supreme Ottoman State\", or alternatively ʿOsmānlı Devleti (عثمانلى دولتى). In Modern Turkish, it is known as Osmanlı İmparatorluğu (\"The Ottoman Empire\") or Osmanlı Devleti (\"The Ottoman State\").The Turkish word for \"Ottoman\" (Osmanlı) originally referred to the tribal followers of Osman in the fourteenth century. The word subsequently came to be used to refer to the empire's military-administrative elite. In contrast, the term \"Turk\" (Türk) was used to refer to the Anatolian peasant and tribal population and was seen as a disparaging term when applied to urban, educated individuals.: 26  In the early modern period, an educated, urban-dwelling Turkish-speaker who was not a member of the military-administrative class would often refer to himself neither as an Osmanlı nor as a Türk, but rather as a Rūmī (رومى), or \"Roman\", meaning an inhabitant of the territory of the former Byzantine Empire in the Balkans and Anatolia. The term Rūmī was also used to refer to Turkish speakers by the other Muslim peoples of the empire and beyond.: 11  As applied to Ottoman Turkish-speakers, this term began to fall out of use at the end of the seventeenth century, and instead of the word increasingly became associated with the Greek population of the empire, a meaning that it still bears in Turkey today.: 51 In Western Europe, the names Ottoman Empire, Turkish Empire and Turkey were often used interchangeably, with Turkey being increasingly favoured both in formal and informal situations. This dichotomy was officially ended in 1920–1923, when the newly established Ankara-based Turkish government chose Turkey as the sole official name. At present, most scholarly historians avoid the terms \"Turkey\", \"Turks\", and \"Turkish\" when referring to the Ottomans, due to the empire's multinational character.\n\n\n== History ==\n\n\n=== Rise (c. 1299–1453) ===\n\nAs the Rum Sultanate declined well into the 13th century, Anatolia was divided into a patchwork of independent Turkish principalities known as the Anatolian Beyliks. One of these beyliks, in the region of Bithynia on the frontier of the Byzantine Empire, was led by the Turkish tribal leader Osman I (d. 1323/4), a figure of obscure origins from whom the name Ottoman is derived.: 444  Osman's early followers consisted both of Turkish tribal groups and Byzantine renegades, with many but not all converts to Islam.: 59 : 127  Osman extended the control of his principality by conquering Byzantine towns along the Sakarya River. A Byzantine defeat at the Battle of Bapheus in 1302 contributed to Osman's rise as well. It is not well understood how the early Ottomans came to dominate their neighbors, due to the lack of sources surviving from this period. The Ghaza thesis popular during the twentieth century credited their success to their rallying of religious warriors to fight for them in the name of Islam, but it is no longer generally accepted. No other hypothesis has attracted broad acceptance.: 5, 10 : 104 \n\nIn the century after the death of Osman I, Ottoman rule had begun to extend over Anatolia and the Balkans. The earliest conflicts began during the Byzantine–Ottoman wars, waged in Anatolia in the late 13th century before entering Europe in the mid-14th century, followed by the Bulgarian–Ottoman wars and the Serbian–Ottoman wars waged beginning in the mid 14th century. Much of this period was characterised by Ottoman expansion into the Balkans. Osman's son, Orhan, captured the northwestern Anatolian city of Bursa in 1326, making it the new capital of the Ottoman state and supplanting Byzantine control in the region. The important port city of Thessaloniki was captured from the Venetians in 1387 and sacked. The Ottoman victory in Kosovo in 1389 effectively marked the end of Serbian power in the region, paving the way for Ottoman expansion into Europe.: 95–96  The Battle of Nicopolis for the Bulgarian Tsardom of Vidin in 1396, widely regarded as the last large-scale crusade of the Middle Ages, failed to stop the advance of the victorious Ottoman Turks.\nAs the Turks expanded into the Balkans, the conquest of Constantinople became a crucial objective. The Ottomans had already wrested control of nearly all former Byzantine lands surrounding the city, but the strong defense of Constantinople's strategic position on the Bosporus Strait made it difficult to conquer. In 1402, the Byzantines were temporarily relieved when the Turco-Mongol leader Timur, founder of the Timurid Empire, invaded Ottoman Anatolia from the east. In the Battle of Ankara in 1402, Timur defeated the Ottoman forces and took Sultan Bayezid I as a prisoner, throwing the empire into disorder. The ensuing civil war, also known as the Fetret Devri, lasted from 1402 to 1413 as Bayezid's sons fought over succession. It ended when Mehmed I emerged as the sultan and restored Ottoman power.: 363 The Balkan territories lost by the Ottomans after 1402, including Thessaloniki, Macedonia, and Kosovo, were later recovered by Murad II between the 1430s and 1450s. On 10 November 1444, Murad repelled the Crusade of Varna by defeating the Hungarian, Polish, and Wallachian armies under Władysław III of Poland (also King of Hungary) and John Hunyadi at the Battle of Varna, although Albanians under Skanderbeg continued to resist. Four years later, John Hunyadi prepared another army of Hungarian and Wallachian forces to attack the Turks, but was again defeated at the Second Battle of Kosovo in 1448.: 29 According to modern historiography, there is a direct connection between the fast Ottoman military advance and the consequences of the Black Death from the mid-fourteenth century onwards. Byzantine territories, where the initial Ottoman conquests were carried out, were exhausted demographically and militarily due to the plague outbreaks, which facilitated the Ottoman expansion. In addition, the slave hunting - executed at first by akinci  irregulars expediting before the Ottoman army - was the main economic driving force behind the Ottoman conquest. Some modern (21st c.) authors re-periodize the Ottoman conquest of the Balkans into the akıncı phase, which spanned 8 to 13 decades, characterized by continuous slave hunting and destruction, followed by the phase of administrative integration into the Ottoman Empire. This theory presumes that the demographical impact of the Black Death was more devastating in Byzantine territories and the Balkans compared to Anatolia, where the bubonic plague pandemic occurred between 1347 and 1349.\n\n\n=== Expansion and peak (1453–1566) ===\n\nThe son of Murad II, Mehmed the Conqueror, reorganized both state and military, and on 29 May 1453 conquered Constantinople, ending the Byzantine Empire. Mehmed allowed the Eastern Orthodox Church to maintain its autonomy and land in exchange for accepting Ottoman authority. Due to tension between the states of western Europe and the later Byzantine Empire, the majority of the Orthodox population accepted Ottoman rule as preferable to Venetian rule. Albanian resistance was a major obstacle to Ottoman expansion on the Italian peninsula. According to modern historiography, there is a direct connection between the fast Ottoman military advance and the consequences of the Black Death from the mid-fourteenth century onwards. Byzantine territories, where the initial Ottoman conquests were carried out, were exhausted demographically and militarily due to the plague outbreaks, which facilitated the Ottoman expansion.In the 15th and 16th centuries, the Ottoman Empire entered a period of expansion. The Empire prospered under the rule of a line of committed and effective Sultans. It also flourished economically due to its control of the major overland trade routes between Europe and Asia.: 111 Sultan Selim I (1512–1520) dramatically expanded the Empire's eastern and southern frontiers by defeating Shah Ismail of Safavid Iran, in the Battle of Chaldiran.: 91–105  Selim I established Ottoman rule in Egypt by defeating and annexing the Mamluk Sultanate of Egypt and created a naval presence on the Red Sea. After this Ottoman expansion, competition began between the Portuguese Empire and the Ottoman Empire to become the dominant power in the region.: 55–76 Suleiman the Magnificent (1520–1566) captured Belgrade in 1521, conquered the southern and central parts of the Kingdom of Hungary as part of the Ottoman–Hungarian Wars, and, after his historic victory in the Battle of Mohács in 1526, he established Ottoman rule in the territory of present-day Hungary (except the western part) and other Central European territories. He then laid siege to Vienna in 1529, but failed to take the city.: 50  In 1532, he made another attack on Vienna, but was repulsed in the siege of Güns. Transylvania, Wallachia and, intermittently, Moldavia, became tributary principalities of the Ottoman Empire. In the east, the Ottoman Turks took Baghdad from the Persians in 1535, gaining control of Mesopotamia and naval access to the Persian Gulf. In 1555, the Caucasus became officially partitioned for the first time between the Safavids and the Ottomans, a status quo that would remain until the end of the Russo-Turkish War (1768–1774). By this partitioning of the Caucasus as signed in the Peace of Amasya, Western Armenia, western Kurdistan, and Western Georgia (including western Samtskhe) fell into Ottoman hands, while southern Dagestan, Eastern Armenia, Eastern Georgia, and Azerbaijan remained Persian.\n\nIn 1539, a 60,000-strong Ottoman army besieged the Spanish garrison of Castelnuovo on the Adriatic coast; the successful siege cost the Ottomans 8,000 casualties, but Venice agreed to terms in 1540, surrendering most of its empire in the Aegean and the Morea. France and the Ottoman Empire, united by mutual opposition to Habsburg rule, became strong allies. The French conquests of Nice (1543) and Corsica (1553) occurred as a joint venture between the forces of the French king Francis I and Suleiman, and were commanded by the Ottoman admirals Hayreddin Barbarossa and Dragut. A month before the siege of Nice, France supported the Ottomans with an artillery unit during the 1543 Ottoman conquest of Esztergom in northern Hungary. After further advances by the Turks, the Habsburg ruler Ferdinand officially recognized Ottoman ascendancy in Hungary in 1547. Suleiman I died of natural causes in his tent during the siege of Szigetvár in 1566.\nBy the end of Suleiman's reign, the Empire spanned approximately 877,888 sq mi (2,273,720 km2), extending over three continents.: 545 \n\nIn addition, the Empire became a dominant naval force, controlling much of the Mediterranean Sea.: 61  By this time, the Ottoman Empire was a major part of the European political sphere. The Ottomans became involved in multi-continental religious wars when Spain and Portugal were united under the Iberian Union. The Ottomans were holders of the Caliph title, meaning they were the leaders of all Muslims worldwide. The Iberians were leaders of the Christian crusaders, and so the two were locked in a worldwide conflict. There were zones of operations in the Mediterranean Sea and Indian Ocean, where Iberians circumnavigated Africa to reach India and, on their way, wage war upon the Ottomans and their local Muslim allies. Likewise, the Iberians passed through newly-Christianized Latin America and had sent expeditions that traversed the Pacific in order to Christianize the formerly Muslim Philippines and use it as a base to further attack the Muslims in the Far East. In this case, the Ottomans sent armies to aid its easternmost vassal and territory, the Sultanate of Aceh in Southeast Asia.: 84 \n\nDuring the 1600s, the worldwide conflict between the Ottoman Caliphate and Iberian Union was a stalemate since both powers were at similar population, technology and economic levels. Nevertheless, the success of the Ottoman political and military establishment was compared to the Roman Empire, despite the difference in the size of their respective territories, by the likes of the contemporary Italian scholar Francesco Sansovino and the French political philosopher Jean Bodin.\n\n\n=== Stagnation and reform (1566–1827) ===\n\n\n==== Revolts, reversals, and revivals (1566–1683) ====\n\nIn the second half of the sixteenth century, the Ottoman Empire came under increasing strain from inflation and the rapidly rising costs of warfare that were impacting both Europe and the Middle East. These pressures led to a series of crises around the year 1600, placing great strain upon the Ottoman system of government.: 413–414  The empire underwent a series of transformations of its political and military institutions in response to these challenges, enabling it to successfully adapt to the new conditions of the seventeenth century and remain powerful, both militarily and economically.: 10  Historians of the mid-twentieth century once characterised this period as one of stagnation and decline, but this view is now rejected by the majority of academics.The discovery of new maritime trade routes by Western European states allowed them to avoid the Ottoman trade monopoly. The Portuguese discovery of the Cape of Good Hope in 1488 initiated a series of Ottoman-Portuguese naval wars in the Indian Ocean throughout the 16th century. Despite the growing European presence in the Indian Ocean, Ottoman trade with the east continued to flourish. Cairo, in particular, benefitted from the rise of Yemeni coffee as a popular consumer commodity. As coffeehouses appeared in cities and towns across the empire, Cairo developed into a major center for its trade, contributing to its continued prosperity throughout the seventeenth and much of the eighteenth century.: 507–508 Under Ivan IV (1533–1584), the Tsardom of Russia expanded into the Volga and Caspian regions at the expense of the Tatar khanates. In 1571, the Crimean khan Devlet I Giray, commanded by the Ottomans, burned Moscow. The next year, the invasion was repeated but repelled at the Battle of Molodi. The Ottoman Empire continued to invade Eastern Europe in a series of slave raids, and remained a significant power in Eastern Europe until the end of the 17th century.\n\nThe Ottomans decided to conquer Venetian Cyprus and on 22 July 1570, Nicosia was besieged; 50,000 Christians died, and 180,000 were enslaved.: 67  On 15 September 1570, the Ottoman cavalry appeared before the last Venetian stronghold in Cyprus, Famagusta. The Venetian defenders would hold out for 11 months against a force that would come to number 200,000 men with 145 cannons; 163,000 cannonballs struck the walls of Famagusta before it fell to the Ottomans in August 1571. The Siege of Famagusta claimed 50,000 Ottoman casualties.: 328  Meanwhile, the Holy League consisting of mostly Spanish and Venetian fleets won a victory over the Ottoman fleet at the Battle of Lepanto (1571), off southwestern Greece; Catholic forces killed over 30,000 Turks and destroyed 200 of their ships.: 24  It was a startling, if mostly symbolic, blow to the image of Ottoman invincibility, an image which the victory of the Knights of Malta over the Ottoman invaders in the 1565 siege of Malta had recently set about eroding. The battle was far more damaging to the Ottoman navy in sapping experienced manpower than the loss of ships, which were rapidly replaced.: 53  The Ottoman navy recovered quickly, persuading Venice to sign a peace treaty in 1573, allowing the Ottomans to expand and consolidate their position in North Africa.\n\nBy contrast, the Habsburg frontier had settled somewhat, a stalemate caused by a stiffening of the Habsburg defenses. The Long Turkish War against Habsburg Austria (1593–1606) created the need for greater numbers of Ottoman infantry equipped with firearms, resulting in a relaxation of recruitment policy. This contributed to problems of indiscipline and outright rebelliousness within the corps, which were never fully solved. Irregular sharpshooters (Sekban) were also recruited, and on demobilisation turned to brigandage in the Celali rebellions (1590–1610), which engendered widespread anarchy in Anatolia in the late 16th and early 17th centuries.: 24  With the Empire's population reaching 30 million people by 1600, the shortage of land placed further pressure on the government. In spite of these problems, the Ottoman state remained strong, and its army did not collapse or suffer crushing defeats. The only exceptions were campaigns against the Safavid dynasty of Persia, where many of the Ottoman eastern provinces were lost, some permanently. This 1603–1618 war eventually resulted in the Treaty of Nasuh Pasha, which ceded the entire Caucasus, except westernmost Georgia, back into the possession of Safavid Iran. The treaty ending the Cretan War cost Venice much of Dalmatia, its Aegean island possessions, and Crete. (Losses from the war totalled 30,985 Venetian soldiers and 118,754 Turkish soldiers.): 33 During his brief majority reign, Murad IV (1623–1640) reasserted central authority and recaptured Iraq (1639) from the Safavids. The resulting Treaty of Zuhab of that same year decisively divided the Caucasus and adjacent regions between the two neighbouring empires as it had already been defined in the 1555 Peace of Amasya.The Sultanate of Women (1533–1656) was a period in which the mothers of young sultans exercised power on behalf of their sons. The most prominent women of this period were Kösem Sultan and her daughter-in-law Turhan Hatice, whose political rivalry culminated in Kösem's murder in 1651. During the Köprülü era (1656–1703), effective control of the Empire was exercised by a sequence of grand viziers from the Köprülü family. The Köprülü Vizierate saw renewed military success with authority restored in Transylvania, the conquest of Crete completed in 1669, and expansion into Polish southern Ukraine, with the strongholds of Khotyn, and Kamianets-Podilskyi and the territory of Podolia ceding to Ottoman control in 1676.\n\nThis period of renewed assertiveness came to a calamitous end in 1683 when Grand Vizier Kara Mustafa Pasha led a huge army to attempt a second Ottoman siege of Vienna in the Great Turkish War of 1683–1699. The final assault being fatally delayed, the Ottoman forces were swept away by allied Habsburg, German, and Polish forces spearheaded by the Polish king John III Sobieski at the Battle of Vienna. The alliance of the Holy League pressed home the advantage of the defeat at Vienna, culminating in the Treaty of Karlowitz (26 January 1699), which ended the Great Turkish War. The Ottomans surrendered control of significant territories, many permanently. Mustafa II (1695–1703) led the counterattack of 1695–1696 against the Habsburgs in Hungary, but was undone at the disastrous defeat at Zenta (in modern Serbia), 11 September 1697.\n\n\n==== Military defeats ====\nAside from the loss of the Banat and the temporary loss of Belgrade (1717–1739), the Ottoman border on the Danube and Sava remained stable during the eighteenth century. Russian expansion, however, presented a large and growing threat.  Accordingly, King Charles XII of Sweden was welcomed as an ally in the Ottoman Empire following his defeat by the Russians at the Battle of Poltava of 1709 in central Ukraine (part of the Great Northern War of 1700–1721). Charles XII persuaded the Ottoman Sultan Ahmed III to declare war on Russia, which resulted in an Ottoman victory in the Pruth River Campaign of 1710–1711, in Moldavia. \n\nAfter the Austro-Turkish War, the Treaty of Passarowitz confirmed the loss of the Banat, Serbia, and \"Little Walachia\" (Oltenia) to Austria. The Treaty also revealed that the Ottoman Empire was on the defensive and unlikely to present any further aggression in Europe. The Austro-Russian–Turkish War (1735–1739), which was ended by the Treaty of Belgrade in 1739, resulted in the Ottoman recovery of northern Bosnia, Habsburg Serbia (including Belgrade), Oltenia and the southern parts of the Banat of Temeswar; but the Empire lost the port of Azov, north of the Crimean Peninsula, to the Russians. After this treaty the Ottoman Empire was able to enjoy a generation of peace, as Austria and Russia were forced to deal with the rise of Prussia.Educational and technological reforms came about, including the establishment of higher education institutions such as the Istanbul Technical University. In 1734 an artillery school was established to impart Western-style artillery methods, but the Islamic clergy successfully objected under the grounds of theodicy. In 1754 the artillery school was reopened on a semi-secret basis. In 1726, Ibrahim Muteferrika convinced the Grand Vizier Nevşehirli Damat Ibrahim Pasha, the Grand Mufti, and the clergy on the efficiency of the printing press, and Muteferrika was later granted by Sultan Ahmed III permission to publish non-religious books (despite opposition from some calligraphers and religious leaders). Muteferrika's press published its first book in 1729 and, by 1743, issued 17 works in 23 volumes, each having between 500 and 1,000 copies.\n\nIn North Africa, Spain conquered Oran from the autonomous Deylik of Algiers. The Bey of Oran received an army from Algiers, but it failed to recapture Oran; the siege caused the deaths of 1,500 Spaniards, and even more Algerians. The Spanish also massacred many Muslim soldiers. In 1792, Spain abandoned Oran, selling it to the Deylik of Algiers.\nIn 1768 Russian-backed Ukrainian Haidamakas, pursuing Polish confederates, entered Balta, an Ottoman-controlled town on the border of Bessarabia in Ukraine, massacred its citizens, and burned the town to the ground. This action provoked the Ottoman Empire into the Russo-Turkish War of 1768–1774. The Treaty of Küçük Kaynarca of 1774 ended the war and provided freedom of worship for the Christian citizens of the Ottoman-controlled provinces of Wallachia and Moldavia. By the late 18th century, after a number of defeats in the wars with Russia, some people in the Ottoman Empire began to conclude that the reforms of Peter the Great had given the Russians an edge, and the Ottomans would have to keep up with Western technology in order to avoid further defeats.Selim III (1789–1807) made the first major attempts to modernise the army, but his reforms were hampered by the religious leadership and the Janissary corps. Jealous of their privileges and firmly opposed to change, the Janissary revolted. Selim's efforts cost him his throne and his life, but were resolved in spectacular and bloody fashion by his successor, the dynamic Mahmud II, who eliminated the Janissary corps in 1826.\n\nThe Serbian revolution (1804–1815) marked the beginning of an era of national awakening in the Balkans during the Eastern Question. In 1811, the fundamentalist Wahhabis of Arabia, led by the al-Saud family, revolted against the Ottomans. Unable to defeat the Wahhabi rebels, the Sublime Porte had Muhammad Ali Pasha of Kavala, the vali (governor) of the Eyalet of Egypt, tasked with retaking Arabia, which ended with the destruction of the Emirate of Diriyah in 1818. The suzerainty of Serbia as a hereditary monarchy under its own dynasty was acknowledged de jure in 1830. In 1821, the Greeks declared war on the Sultan. A rebellion that originated in Moldavia as a diversion was followed by the main revolution in the Peloponnese, which, along with the northern part of the Gulf of Corinth, became the first parts of the Ottoman Empire to achieve independence (in 1829). In 1830, the French invaded the Deylik of Algiers. The campaign that took 21 days, resulted in over 5,000 Algerian military casualties, and about 2,600 French ones. Before the French invasion the total population of Algeria was most likely between 3,000,000 and 5,000,000. By 1873, the population of Algeria (excluding several hundred thousand newly arrived French settlers) decreased to a drastic 2,172,000. In 1831, Muhammad Ali Pasha revolted against Sultan Mahmud II due to the latter's refusal to grant him the governorships of Greater Syria and Crete, which the Sultan had promised him in exchange for sending military assistance to put down the Greek revolt (1821–1829) that ultimately ended with the formal independence of Greece in 1830. It was a costly enterprise for Muhammad Ali Pasha, who had lost his fleet at the Battle of Navarino in 1827. Thus began the first Egyptian–Ottoman War (1831–1833), during which the French-trained army of Muhammad Ali Pasha, under the command of his son Ibrahim Pasha, defeated the Ottoman Army as it marched into Anatolia, reaching the city of Kütahya within 320 km (200 mi) of the capital, Constantinople.: 95  In desperation, Sultan Mahmud II appealed to the empire's traditional arch-rival Russia for help, asking Emperor Nicholas I to send an expeditionary force to assist him.: 96  In return for signing the Treaty of Hünkâr İskelesi, the Russians sent the expeditionary force which deterred Ibrahim Pasha from marching any further towards Constantinople.: 96  Under the terms of the Convention of Kütahya, signed on 5 May 1833, Muhammad Ali Pasha agreed to abandon his campaign against the Sultan, in exchange for which he was made the vali (governor) of the vilayets (provinces) of Crete, Aleppo, Tripoli, Damascus and Sidon (the latter four comprising modern Syria and Lebanon), and given the right to collect taxes in Adana.: 96  Had it not been for the Russian intervention, Sultan Mahmud II could have faced the risk of being overthrown and Muhammad Ali Pasha could have even become the new Sultan. These events marked the beginning of a recurring pattern where the Sublime Porte needed the help of foreign powers to protect itself.: 95–96 \n\nIn 1839, the Sublime Porte attempted to take back what it lost to the de facto autonomous, but de jure still Ottoman Eyalet of Egypt, but its forces were initially defeated, which led to the Oriental Crisis of 1840. Muhammad Ali Pasha had close relations with France, and the prospect of him becoming the Sultan of Egypt was widely viewed as putting the entire Levant into the French sphere of influence.: 96  As the Sublime Porte had proved itself incapable of defeating Muhammad Ali Pasha, the British Empire and Austrian Empire provided military assistance, and the second Egyptian–Ottoman War (1839–1841) ended with Ottoman victory and the restoration of Ottoman suzerainty over Egypt Eyalet and the Levant.: 96 By the mid-19th century, the Ottoman Empire was called the \"sick man of Europe\". Three suzerain states – the Principality of Serbia, Wallachia and Moldavia – moved towards de jure independence during the 1860s and 1870s.\n\n\n=== Decline and modernisation (1828–1908) ===\n\nDuring the Tanzimat period (1839–1876), the government's series of constitutional reforms led to a fairly modern conscripted army, banking system reforms, the decriminalization of homosexuality, the replacement of religious law with secular law and guilds with modern factories. The Ottoman Ministry of Post was established in Istanbul in 1840. American inventor Samuel Morse received an Ottoman patent for the telegraph in 1847, which was issued by Sultan Abdülmecid who personally tested the new invention. The reformist period peaked with the Constitution, called the Kanûn-u Esâsî. The empire's First Constitutional era was short-lived. The parliament survived for only two years before the sultan suspended it.\nThe Christian population of the empire, owing to their higher educational levels, started to pull ahead of the Muslim majority, leading to much resentment on the part of the latter. In 1861, there were 571 primary and 94 secondary schools for Ottoman Christians with 140,000 pupils in total, a figure that vastly exceeded the number of Muslim children in school at the same time, who were further hindered by the amount of time spent learning Arabic and Islamic theology. Author Norman Stone further suggests that the Arabic alphabet, in which Turkish was written until 1928, was very ill-suited to reflect the sounds of the Turkish language (which is a Turkic as opposed to Semitic language), which imposed a further difficulty on Turkish children. In turn, the higher educational levels of the Christians allowed them to play a larger role in the economy, with the rise in prominence of groups such as the Sursock family indicative of this shift in influence. In 1911, of the 654 wholesale companies in Istanbul, 528 were owned by ethnic Greeks. In many cases, Christians and also Jews were able to gain protection from European consuls and citizenship, meaning they were protected from Ottoman law and not subject to the same economic regulations as their Muslim counterparts.\n\nThe Crimean War (1853–1856) was part of a long-running contest between the major European powers for influence over territories of the declining Ottoman Empire. The financial burden of the war led the Ottoman state to issue foreign loans amounting to 5 million pounds sterling on 4 August 1854.: 32 : 71  The war caused an exodus of the Crimean Tatars, about 200,000 of whom moved to the Ottoman Empire in continuing waves of emigration.: 79–108  Toward the end of the Caucasian Wars, 90% of the Circassians were ethnically cleansed and exiled from their homelands in the Caucasus and fled to the Ottoman Empire, resulting in the settlement of 500,000 to 700,000 Circassians in Turkey. Some Circassian organisations give much higher numbers, totalling 1–1.5 million deported or killed. Crimean Tatar refugees in the late 19th century played an especially notable role in seeking to modernise Ottoman education and in first promoting both Pan-Turkism and a sense of Turkish nationalism.\n\nIn this period, the Ottoman Empire spent only small amounts of public funds on education; for example in 1860–1861 only 0.2 percent of the total budget was invested in education.: 50  As the Ottoman state attempted to modernize its infrastructure and army in response to threats from the outside, it also opened itself up to a different kind of threat: that of creditors. Indeed, as the historian Eugene Rogan has written, \"the single greatest threat to the independence of the Middle East\" in the nineteenth century \"was not the armies of Europe but its banks\". The Ottoman state, which had begun taking on debt with the Crimean War, was forced to declare bankruptcy in 1875. By 1881, the Ottoman Empire agreed to have its debt controlled by an institution known as the Ottoman Public Debt Administration, a council of European men with presidency alternating between France and Britain. The body controlled swaths of the Ottoman economy, and used its position to ensure that European capital continued to penetrate the empire, often to the detriment of local Ottoman interests.\n\nThe Ottoman bashi-bazouks brutally suppressed the Bulgarian uprising of 1876, massacring up to 100,000 people in the process.: 139  The Russo-Turkish War (1877–1878) ended with a decisive victory for Russia. As a result, Ottoman holdings in Europe declined sharply: Bulgaria was established as an independent principality inside the Ottoman Empire; Romania achieved full independence; and Serbia and Montenegro finally gained complete independence, but with smaller territories. In 1878, Austria-Hungary unilaterally occupied the Ottoman provinces of Bosnia-Herzegovina and Novi Pazar.\nBritish Prime Minister Benjamin Disraeli advocated for restoring the Ottoman territories on the Balkan Peninsula during the Congress of Berlin, and in return, Britain assumed the administration of Cyprus in 1878.: 228–254  Britain later sent troops to Egypt in 1882 to put down the Urabi Revolt – Sultan Abdul Hamid II was too paranoid to mobilize his own army, fearing this would result in a coup d'état – effectively gaining control in both territories. Abdul Hamid II, popularly known as \"Abdul Hamid the Damned\" on account of his cruelty and paranoia, was so fearful of the threat of a coup that he did not allow his army to conduct war games, lest this serves as the cover for a coup, but he did see the need for military mobilization. In 1883, a German military mission under General Baron Colmar von der Goltz arrived to train the Ottoman Army, leading to the so-called \"Goltz generation\" of German-trained officers who were to play a notable role in the politics of the last years of the empire.: 24 From 1894 to 1896, between 100,000 and 300,000 Armenians living throughout the empire were killed in what became known as the Hamidian massacres.: 42 In 1897 the population was 19 million, of whom 14 million (74%) were Muslim. An additional 20 million lived in provinces that remained under the sultan's nominal suzerainty but were entirely outside his actual power. One by one the Porte lost nominal authority. They included Egypt, Tunisia, Bulgaria, Cyprus, Bosnia-Herzegovina, and Lebanon.As the Ottoman Empire gradually shrank in size, some 7–9 million Muslims from its former territories in the Caucasus, Crimea, Balkans, and the Mediterranean islands migrated to Anatolia and Eastern Thrace. After the Empire lost the First Balkan War (1912–1913), it lost all its Balkan territories except East Thrace (European Turkey). This resulted in around 400,000 Muslims fleeing with the retreating Ottoman armies (with many dying from cholera brought by the soldiers), and with some 400,000 non-Muslims fleeing territory still under Ottoman rule. Justin McCarthy estimates that during the period 1821 to 1922, 5.5 million Muslims died in southeastern Europe, with the expulsion of 5 million.\n\n\n=== Defeat and dissolution (1908–1922) ===\n\n\n==== Young Turk movement ====\n\nThe defeat and dissolution of the Ottoman Empire (1908—1922) began with the Second Constitutional Era, a moment of hope and promise established with the Young Turk Revolution. It restored the Constitution of the Ottoman Empire and brought in multi-party politics with a two-stage electoral system (electoral law) under the Ottoman parliament. The constitution offered hope by freeing the empire's citizens to modernise the state's institutions, rejuvenate its strength, and enable it to hold its own against outside powers. Its guarantee of liberties promised to dissolve inter-communal tensions and transform the empire into a more harmonious place. Instead, this period became the story of the twilight struggle of the Empire.\nMembers of Young Turks movement who had once gone underground now established their parties. Among them \"Committee of Union and Progress\", and \"Freedom and Accord Party\" were major parties. On the other end of the spectrum were ethnic parties, which included Poale Zion, Al-Fatat, and Armenian national movement organised under Armenian Revolutionary Federation. Profiting from the civil strife, Austria-Hungary officially annexed Bosnia and Herzegovina in 1908. The last of the Ottoman censuses was performed in 1914. Despite military reforms which reconstituted the Ottoman Modern Army, the Empire lost its North African territories and the Dodecanese in the Italo-Turkish War (1911) and almost all of its European territories in the Balkan Wars (1912–1913). The Empire faced continuous unrest in the years leading up to World War I, including the 31 March Incident and two further coups in 1912 and 1913.\n\n\n==== World War I ====\n\nThe Ottoman Empire entered World War I on the side of the Central Powers and was ultimately defeated. The Ottoman participation in the war began with the combined German-Ottoman surprise attack on the Black Sea coast of the Russian Empire on 29 October 1914. Following the attack, the Russian Empire (2 November 1914) and its allies France (5 November 1914) and the British Empire (5 November 1914) declared war on the Ottoman Empire (also on 5 November 1914, the British government changed the status of the Khedivate of Egypt and Cyprus, which were de jure Ottoman territories prior to the war, as British protectorates.)\nThe Ottomans successfully defended the Dardanelles strait during the Gallipoli campaign (1915–1916) and achieved initial victories against British forces in the first two years of the Mesopotamian campaign, such as the Siege of Kut (1915–1916); but the Arab Revolt (1916–1918) turned the tide against the Ottomans in the Middle East. In the Caucasus campaign, however, the Russian forces had the upper hand from the beginning, especially after the Battle of Sarikamish (1914–1915). Russian forces advanced into northeastern Anatolia and controlled the major cities there until retreating from World War I with the Treaty of Brest-Litovsk following the Russian Revolution in 1917.\n\n\n===== Genocides =====\n\nIn 1915 the Ottoman government and Kurdish tribes in the region started the extermination of its ethnic Armenian population, resulting in the deaths of up to 1.5 million Armenians in the Armenian genocide. The genocide was carried out during and after World War I and implemented in two phases: the wholesale killing of the able-bodied male population through massacre and subjection of army conscripts to forced labour, followed by the deportation of women, children, the elderly and infirm on death marches leading to the Syrian desert. Driven forward by military escorts, the deportees were deprived of food and water and subjected to periodic robbery, rape, and systematic massacre. Large-scale massacres were also committed against the Empire's Greek and Assyrian minorities as part of the same campaign of ethnic cleansing.\n\n\n===== Arab Revolt =====\n\nThe Arab Revolt began in 1916 with British support. It turned the tide against the Ottomans on the Middle Eastern front, where they seemed to have the upper hand during the first two years of the war. On the basis of the McMahon–Hussein Correspondence, an agreement between the British government and Hussein bin Ali, Sharif of Mecca, the revolt was officially initiated at Mecca on 10 June 1916. The Arab nationalist goal was to create a single unified and independent Arab state stretching from Aleppo in Syria to Aden in Yemen, which the British had promised to recognise.\nThe Sharifian Army led by Hussein and the Hashemites, with military backing from the British Egyptian Expeditionary Force, successfully fought and expelled the Ottoman military presence from much of the Hejaz and Transjordan. The rebellion eventually took Damascus and set up a short-lived monarchy led by Faisal, a son of Hussein.\nFollowing the Sykes–Picot Agreement, the Middle East was later partitioned by the British and French into mandate territories. There was no unified Arab state, much to the anger of Arab nationalists.\n\n\n===== Treaty of Sèvres and Turkish War of Independence =====\n\nDefeated in World War I, the Ottoman Empire signed the Armistice of Mudros on 30 October 1918. Istanbul was occupied by combined British, French, Italian, and Greek forces. In May 1919, Greece also took control of the area around Smyrna (now İzmir).\nThe partition of the Ottoman Empire was finalized under the terms of the 1920 Treaty of Sèvres. This treaty, as designed in the Conference of London, allowed the Sultan to retain his position and title. The status of Anatolia was problematic given the occupied forces.\nThere arose a nationalist opposition in the Turkish national movement. It won the Turkish War of Independence (1919–1923) under the leadership of Mustafa Kemal (later given the surname \"Atatürk\"). The sultanate was abolished on 1 November 1922, and the last sultan, Mehmed VI (reigned 1918–1922), left the country on 17 November 1922. The Republic of Turkey was established in its place on 29 October 1923, in the new capital city of Ankara. The caliphate was abolished on 3 March 1924.\n\n\n== Historiographical debate on the Ottoman state ==\n\nSeveral historians such as British historian Edward Gibbon and the Greek historian Dimitri Kitsikis have argued that after the fall of Constantinople, the Ottoman state took over the machinery of the Byzantine (Roman) state and that in essence, the Ottoman Empire was a continuation of the Eastern Roman Empire under a Turkish Muslim guise. The American historian Speros Vryonis wrote that the Ottoman state was centered on \"a Byzantine-Balkan base with a veneer of the Turkish language and the Islamic religion\". The American historian Heath Lowry and Kitsikis posit that the early Ottoman state was a predatory confederacy open to both Byzantine Christians and Turkish Muslims, whose primary goal was attaining booty and slaves, rather than spreading Islam, and that only later Islam became the primary characteristic of the empire. Other historians have followed the lead of the Austrian historian Paul Wittek who emphasized the Islamic character of the early Ottoman state, seeing the Ottoman state as a \"Jihad state\" dedicated to expanding the Muslim world. Many historians led in 1937 by the Turkish historian Mehmet Fuat Köprülü championed the Ghaza thesis that saw the early Ottoman state as a continuation of the way of life of the nomadic Turkic tribes who had come from East Asia to Anatolia via Central Asia and the Middle East on a much larger scale. They argued that the most important cultural influences on the Ottoman state came from Persia.The British historian Norman Stone suggested many continuities between the Eastern Roman and Ottoman empires such as the zeugarion tax of Byzantium becoming the Ottoman Resm-i çift tax, the pronoia land-holding system that linked the amount of land one owned with one's ability to raise cavalry becoming the Ottoman timar system, and the Ottoman measurement for land the dönüm was the same as the Byzantine stremma. Stone also pointed out that despite the fact that Sunni Islam was the state religion, the Eastern Orthodox Church was supported and controlled by the Ottoman state, and in return to accepting that control became the largest land-holder in the Ottoman Empire. Despite the similarities, Stone argued that a crucial difference was that the land grants under the timar system were not hereditary at first. Even after land grants under the timar system became inheritable, land ownership in the Ottoman Empire remained highly insecure, and the sultan could and did revoke land grants whenever he wished. Stone argued this insecurity in land tenure strongly discouraged Timariots from seeking long-term development of their land, and instead led the timariots to adopt a strategy of short-term exploitation, which ultimately had deleterious effects on the Ottoman economy.\n\n\n== Government ==\n\nBefore the reforms of the 19th and 20th centuries, the state organisation of the Ottoman Empire was a system with two main dimensions, the military administration, and the civil administration. The Sultan was in the highest position in the system. The civil system was based on local administrative units based on the region's characteristics. The state had control over the clergy. Certain pre-Islamic Turkish traditions that had survived the adoption of administrative and legal practices from Islamic Iran remained important in Ottoman administrative circles. According to Ottoman understanding, the state's primary responsibility was to defend and extend the land of the Muslims and to ensure security and harmony within its borders in the overarching context of orthodox Islamic practice and dynastic sovereignty.\n\nThe Ottoman Empire, or as a dynastic institution, the House of Osman, was unprecedented and unequaled in the Islamic world for its size and duration. In Europe, only the House of Habsburg had a similarly unbroken line of sovereigns (kings/emperors) from the same family who ruled for so long, and during the same period, between the late 13th and early 20th centuries. The Ottoman dynasty was Turkish in origin. On eleven occasions, the sultan was deposed (replaced by another sultan of the Ottoman dynasty, who were either the former sultan's brother, son or nephew) because he was perceived by his enemies as a threat to the state. There were only two attempts in Ottoman history to unseat the ruling Ottoman dynasty, both failures, which suggests a political system that for an extended period was able to manage its revolutions without unnecessary instability. As such, the last Ottoman sultan Mehmed VI (r. 1918–1922) was a direct patrilineal (male-line) descendant of the first Ottoman sultan Osman I (d. 1323/4), which was unparalleled in both Europe (e.g., the male line of the House of Habsburg became extinct in 1740) and in the Islamic world. The primary purpose of the Imperial Harem was to ensure the birth of male heirs to the Ottoman throne and secure the continuation of the direct patrilineal (male-line) power of the Ottoman sultans in the future generations.\nThe highest position in Islam, caliph, was claimed by the sultans starting with Murad I, which was established as the Ottoman Caliphate. The Ottoman sultan, pâdişâh or \"lord of kings\", served as the Empire's sole regent and was considered to be the embodiment of its government, though he did not always exercise complete control. The Imperial Harem was one of the most important powers of the Ottoman court. It was ruled by the valide sultan. On occasion, the valide sultan would become involved in state politics. For a time, the women of the Harem effectively controlled the state in what was termed the \"Sultanate of Women\". New sultans were always chosen from the sons of the previous sultan. The strong educational system of the palace school was geared towards eliminating the unfit potential heirs and establishing support among the ruling elite for a successor. The palace schools, which would also educate the future administrators of the state, were not a single track. First, the Madrasa (Medrese) was designated for the Muslims, and educated scholars and state officials according to Islamic tradition. The financial burden of the Medrese was supported by vakifs, allowing children of poor families to move to higher social levels and income. The second track was a free boarding school for the Christians, the Enderûn, which recruited 3,000 students annually from Christian boys between eight and twenty years old from one in forty families among the communities settled in Rumelia or the Balkans, a process known as Devshirme (Devşirme).Though the sultan was the supreme monarch, the sultan's political and executive authority was delegated. The politics of the state had a number of advisors and ministers gathered around a council known as Divan. The Divan, in the years when the Ottoman state was still a Beylik, was composed of the elders of the tribe. Its composition was later modified to include military officers and local elites (such as religious and political advisors). Later still, beginning in 1320, a Grand Vizier was appointed to assume certain of the sultan's responsibilities. The Grand Vizier had considerable independence from the sultan with almost unlimited powers of appointment, dismissal, and supervision. Beginning with the late 16th century, sultans withdrew from politics and the Grand Vizier became the de facto head of state.\n\nThroughout Ottoman history, there were many instances in which local governors acted independently, and even in opposition to the ruler. After the Young Turk Revolution of 1908, the Ottoman state became a constitutional monarchy. The sultan no longer had executive powers. A parliament was formed, with representatives chosen from the provinces. The representatives formed the Imperial Government of the Ottoman Empire.\nThis eclectic administration was apparent even in the diplomatic correspondence of the Empire, which was initially undertaken in the Greek language to the west.The Tughra were calligraphic monograms, or signatures, of the Ottoman Sultans, of which there were 35. Carved on the Sultan's seal, they bore the names of the Sultan and his father. The statement and prayer, \"ever victorious\", was also present in most. The earliest belonged to Orhan Gazi. The ornately stylized Tughra spawned a branch of Ottoman-Turkish calligraphy.\n\n\n=== Law ===\n\nThe Ottoman legal system accepted the religious law over its subjects. At the same time the Qanun (or Kanun), dynastic law, co-existed with religious law or Sharia. The Ottoman Empire was always organized around a system of local jurisprudence. Legal administration in the Ottoman Empire was part of a larger scheme of balancing central and local authority. Ottoman power revolved crucially around the administration of the rights to land, which gave a space for the local authority to develop the needs of the local millet. The jurisdictional complexity of the Ottoman Empire was aimed to permit the integration of culturally and religiously different groups. The Ottoman system had three court systems: one for Muslims, one for non-Muslims, involving appointed Jews and Christians ruling over their respective religious communities, and the \"trade court\". The entire system was regulated from above by means of the administrative Qanun, i.e., laws, a system based upon the Turkic Yassa and Töre, which were developed in the pre-Islamic era.\n\nThese court categories were not, however, wholly exclusive; for instance, the Islamic courts, which were the Empire's primary courts, could also be used to settle a trade conflict or disputes between litigants of differing religions, and Jews and Christians often went to them to obtain a more forceful ruling on an issue. The Ottoman state tended not to interfere with non-Muslim religious law systems, despite legally having a voice to do so through local governors. The Islamic Sharia law system had been developed from a combination of the Qur'an; the Hadīth, or words of the prophet Muhammad; ijmā', or consensus of the members of the Muslim community; qiyas, a system of analogical reasoning from earlier precedents; and local customs. Both systems were taught at the Empire's law schools, which were in Istanbul and Bursa.\n\nThe Ottoman Islamic legal system was set up differently from traditional European courts. Presiding over Islamic courts would be a Qadi, or judge. Since the closing of the ijtihad, or 'Gate of Interpretation', Qadis throughout the Ottoman Empire focused less on legal precedent, and more with local customs and traditions in the areas that they administered. However, the Ottoman court system lacked an appellate structure, leading to jurisdictional case strategies where plaintiffs could take their disputes from one court system to another until they achieved a ruling that was in their favour.\nIn the late 19th century, the Ottoman legal system saw substantial reform. This process of legal modernisation began with the Edict of Gülhane of 1839. These reforms included the \"fair and public trial[s] of all accused regardless of religion\", the creation of a system of \"separate competences, religious and civil\", and the validation of testimony on non-Muslims. Specific land codes (1858), civil codes (1869–1876), and a code of civil procedure also were enacted.These reforms were based heavily on French models, as indicated by the adoption of a three-tiered court system. Referred to as Nizamiye, this system was extended to the local magistrate level with the final promulgation of the Mecelle, a civil code that regulated marriage, divorce, alimony, will, and other matters of personal status. In an attempt to clarify the division of judicial competences, an administrative council laid down that religious matters were to be handled by religious courts, and statute matters were to be handled by the Nizamiye courts.\n\n\n=== Military ===\n\nThe first military unit of the Ottoman State was an army that was organized by Osman I from the tribesmen inhabiting the hills of western Anatolia in the late 13th century. The military system became an intricate organization with the advance of the Empire. The Ottoman military was a complex system of recruiting and fief-holding. The main corps of the Ottoman Army included Janissary, Sipahi, Akıncı and Mehterân. The Ottoman army was once among the most advanced fighting forces in the world, being one of the first to use muskets and cannons. The Ottoman Turks began using falconets, which were short but wide cannons, during the Siege of Constantinople. The Ottoman cavalry depended on high speed and mobility rather than heavy armor, using bows and short swords on fast Turkoman and Arabian horses (progenitors of the Thoroughbred racing horse), and often applied tactics similar to those of the Mongol Empire, such as pretending to retreat while surrounding the enemy forces inside a crescent-shaped formation and then making the real attack. The Ottoman army continued to be an effective fighting force throughout the seventeenth and early eighteenth centuries, falling behind the empire's European rivals only during a long period of peace from 1740 to 1768.\n\nThe modernization of the Ottoman Empire in the 19th century started with the military. In 1826 Sultan Mahmud II abolished the Janissary corps and established the modern Ottoman army. He named them as the Nizam-ı Cedid (New Order). The Ottoman army was also the first institution to hire foreign experts and send its officers for training in western European countries. Consequently, the Young Turks movement began when these relatively young and newly trained men returned with their education.\n\nThe Ottoman Navy vastly contributed to the expansion of the Empire's territories on the European continent. It initiated the conquest of North Africa, with the addition of Algeria and Egypt to the Ottoman Empire in 1517. Starting with the loss of Greece in 1821 and Algeria in 1830, Ottoman naval power and control over the Empire's distant overseas territories began to decline. Sultan Abdülaziz (reigned 1861–1876) attempted to reestablish a strong Ottoman navy, building the largest fleet after those of Britain and France. The shipyard at Barrow, England, built its first submarine in 1886 for the Ottoman Empire.However, the collapsing Ottoman economy could not sustain the fleet's strength for long. Sultan Abdülhamid II distrusted the admirals who sided with the reformist Midhat Pasha and claimed that the large and expensive fleet was of no use against the Russians during the Russo-Turkish War. He locked most of the fleet inside the Golden Horn, where the ships decayed for the next 30 years. Following the Young Turk Revolution in 1908, the Committee of Union and Progress sought to develop a strong Ottoman naval force. The Ottoman Navy Foundation was established in 1910 to buy new ships through public donations.\n\nThe establishment of Ottoman military aviation dates back to between June 1909 and July 1911. The Ottoman Empire started preparing its first pilots and planes, and with the founding of the Aviation School (Tayyare Mektebi) in Yeşilköy on 3 July 1912, the Empire began to tutor its own flight officers. The founding of the Aviation School quickened advancement in the military aviation program, increased the number of enlisted persons within it, and gave the new pilots an active role in the Ottoman Army and Navy. In May 1913, the world's first specialized Reconnaissance Training Program was started by the Aviation School, and the first separate reconnaissance division was established. In June 1914 a new military academy, the Naval Aviation School (Bahriye Tayyare Mektebi) was founded. With the outbreak of World War I, the modernization process stopped abruptly. The Ottoman Aviation Squadrons fought on many fronts during World War I, from Galicia in the west to the Caucasus in the east and Yemen in the south.\n\n\n== Administrative divisions ==\n\nThe Ottoman Empire was first subdivided into provinces, in the sense of fixed territorial units with governors appointed by the sultan, in the late 14th century.The Eyalet (also Pashalik or Beylerbeylik) was the territory of office of a Beylerbey (\"lord of lords\" or governor), and was further subdivided in Sanjaks.\nThe Vilayets were introduced with the promulgation of the \"Vilayet Law\" (Teskil-i Vilayet Nizamnamesi) in 1864, as part of the Tanzimat reforms. Unlike the previous eyalet system, the 1864 law established a hierarchy of administrative units: the vilayet, liva/sanjak/mutasarrifate, kaza and village council, to which the 1871 Vilayet Law added the nahiye.\n\n\n== Economy ==\n\nOttoman government deliberately pursued a policy for the development of Bursa, Edirne, and Istanbul, successive Ottoman capitals, into major commercial and industrial centers, considering that merchants and artisans were indispensable in creating a new metropolis. To this end, Mehmed and his successor Bayezid, also encouraged and welcomed migration of the Jews from different parts of Europe, who were settled in Istanbul and other port cities like Salonica. In many places in Europe, Jews were suffering persecution at the hands of their Christian counterparts, such as in Spain, after the conclusion of Reconquista. The tolerance displayed by the Turks was welcomed by the immigrants.\n\nThe Ottoman economic mind was closely related to the basic concepts of state and society in the Middle East in which the ultimate goal of a state was consolidation and extension of the ruler's power, and the way to reach it was to get rich resources of revenues by making the productive classes prosperous. The ultimate aim was to increase the state revenues without damaging the prosperity of subjects to prevent the emergence of social disorder and to keep the traditional organization of the society intact. The Ottoman economy greatly expanded during the early modern period, with particularly high growth rates during the first half of the eighteenth century. The empire's annual income quadrupled between 1523 and 1748, adjusted for inflation.The organization of the treasury and chancery were developed under the Ottoman Empire more than any other Islamic government and, until the 17th century, they were the leading organization among all their contemporaries. This organisation developed a scribal bureaucracy (known as \"men of the pen\") as a distinct group, partly highly trained ulama, which developed into a professional body. The effectiveness of this professional financial body stands behind the success of many great Ottoman statesmen.\n\nModern Ottoman studies indicate that the change in relations between the Ottoman Turks and central Europe was caused by the opening of the new sea routes. It is possible to see the decline in the significance of the land routes to the East as Western Europe opened the ocean routes that bypassed the Middle East and the Mediterranean as parallel to the decline of the Ottoman Empire itself. The Anglo-Ottoman Treaty, also known as the Treaty of Balta Liman that opened the Ottoman markets directly to English and French competitors, would be seen as one of the staging posts along with this development.\nBy developing commercial centers and routes, encouraging people to extend the area of cultivated land in the country and international trade through its dominions, the state performed basic economic functions in the Empire. But in all this, the financial and political interests of the state were dominant. Within the social and political system they were living in, Ottoman administrators could not see the desirability of the dynamics and principles of the capitalist and mercantile economies developing in Western Europe.Economic historian Paul Bairoch argues that free trade contributed to deindustrialisation in the Ottoman Empire. In contrast to the protectionism of China, Japan, and Spain, the Ottoman Empire had a liberal trade policy, open to foreign imports. This has origins in capitulations of the Ottoman Empire, dating back to the first commercial treaties signed with France in 1536 and taken further with capitulations in 1673 and 1740, which lowered duties to 3% for imports and exports. The liberal Ottoman policies were praised by British economists, such as John Ramsay McCulloch in his Dictionary of Commerce (1834), but later criticized by British politicians such as Prime Minister Benjamin Disraeli, who cited the Ottoman Empire as \"an instance of the injury done by unrestrained competition\" in the 1846 Corn Laws debate.\n\n\n== Demographics ==\n\nA population estimate for the empire of 11,692,480 for the 1520–1535 period was obtained by counting the households in Ottoman tithe registers, and multiplying this number by 5. For unclear reasons, the population in the 18th century was lower than that in the 16th century. An estimate of 7,230,660 for the first census held in 1831 is considered a serious undercount, as this census was meant only to register possible conscripts.Censuses of Ottoman territories only began in the early 19th century. Figures from 1831 onwards are available as official census results, but the censuses did not cover the whole population. For example, the 1831 census only counted men and did not cover the whole empire. For earlier periods estimates of size and distribution of the population are based on observed demographic patterns.\n\nHowever, it began to rise to reach 25–32 million by 1800, with around 10 million in the European provinces (primarily in the Balkans), 11 million in the Asiatic provinces, and around 3 million in the African provinces. Population densities were higher in the European provinces, double those in Anatolia, which in turn were triple the population densities of Iraq and Syria and five times the population density of Arabia.Towards the end of the empire's existence life expectancy was 49 years, compared to the mid-twenties in Serbia at the beginning of the 19th century. Epidemic diseases and famine caused major disruption and demographic changes. In 1785 around one-sixth of the Egyptian population died from the plague and Aleppo saw its population reduced by twenty percent in the 18th century. Six famines hit Egypt alone between 1687 and 1731 and the last famine to hit Anatolia was four decades later.The rise of port cities saw the clustering of populations caused by the development of steamships and railroads. Urbanization increased from 1700 to 1922, with towns and cities growing. Improvements in health and sanitation made them more attractive to live and work in. Port cities like Salonica, in Greece, saw its population rise from 55,000 in 1800 to 160,000 in 1912 and İzmir which had a population of 150,000 in 1800 grew to 300,000 by 1914. Some regions conversely had population falls—Belgrade saw its population drop from 25,000 to 8,000 mainly due to political strife.\n\nEconomic and political migrations made an impact across the empire. For example, the Russian and Austria-Habsburg annexation of the Crimean and Balkan regions respectively saw large influxes of Muslim refugees—200,000 Crimean Tartars fleeing to Dobruja. Between 1783 and 1913, approximately 5–7 million refugees flooded into the Ottoman Empire, at least 3.8 million of whom were from Russia. Some migrations left indelible marks such as political tension between parts of the empire (e.g., Turkey and Bulgaria), whereas centrifugal effects were noticed in other territories, simpler demographics emerging from diverse populations. Economies were also impacted by the loss of artisans, merchants, manufacturers, and agriculturists. Since the 19th century, a large proportion of Muslim peoples from the Balkans emigrated to present-day Turkey. These people are called Muhacir. By the time the Ottoman Empire came to an end in 1922, half of the urban population of Turkey was descended from Muslim refugees from Russia.\n\n\n=== Language ===\n\nOttoman Turkish was the official language of the Empire. It was an Oghuz Turkic language highly influenced by Persian and Arabic, though lower registries spoken by the common people had fewer influences from other languages compared to higher varieties used by upper classes and governmental authorities. Turkish, in its Ottoman variation, was a language of military and administration since the nascent days of the Ottomans. The Ottoman constitution of 1876 did officially cement the official imperial status of Turkish.The Ottomans had several influential languages: Turkish, spoken by the majority of the people in Anatolia and by the majority of Muslims of the Balkans except in Albania, Bosnia and the Megleno-Romanian-inhabited Nânti; Persian, only spoken by the educated; Arabic, spoken mainly in Egypt, the Levant, Arabia, Iraq, North Africa, Kuwait and parts of the Horn of Africa and Berber in North Africa. In the last two centuries, usage of these became limited, though, and specific: Persian served mainly as a literary language for the educated, while Arabic was used for Islamic prayers. In the post-Tanzimat period French became the common Western language among the educated.Because of a low literacy rate among the public (about 2–3% until the early 19th century and just about 15% at the end of the 19th century), ordinary people had to hire scribes as \"special request-writers\" (arzuhâlcis) to be able to communicate with the government. Some ethnic groups continued to speak within their families and neighborhoods (mahalles) with their own languages, though many non-Muslim minorities such as Greeks and Armenians only spoke Turkish. In villages where two or more populations lived together, the inhabitants would often speak each other's language. In cosmopolitan cities, people often spoke their family languages; many of those who were not ethnic Turks spoke Turkish as a second language.\n\n\n=== Religion ===\n\nSunni Islam was the prevailing Dīn (customs, legal traditions, and religion) of the Ottoman Empire; the official Madh'hab (school of Islamic jurisprudence) was Hanafi. From the early 16th century until the early 20th century, the Ottoman sultan also served as the caliph, or politico-religious leader, of the Muslim world. Most of the Ottoman Sultans adhered to Sufism and followed Sufi orders, and believed Sufism was the correct way to reach God.Non-Muslims, particularly Christians and Jews, were present throughout the empire's history. The Ottoman imperial system was charactised by an intricate combination of official Muslim hegemony over non-Muslims and a wide degree of religious tolerance. While religious minorities were never equal under the law, they were granted recognition, protection, and limited freedoms under both Islamic and Ottoman tradition.Until the second half of the 15th century, the majority of Ottoman subjects were Christian. Non-Muslims remained a significant and economically influential minority, albeit declining significantly by the 19th century, due largely to migration and secession. The proportion of Muslims amounted to 60% in the 1820s, gradually increasing to 69% in the 1870s and 76% in the 1890s. By 1914, less than a fifth of the empire's population (19.1%) was non-Muslim, mostly made up of Jews and Christian Greeks, Assyrians, and Armenians.\n\n\n==== Islam ====\n\nTurkic peoples practiced a form of shamanism before adopting Islam. The Muslim conquest of Transoxiana under the Abbasids facilitated the spread of Islam into the Turkic heartland of Central Asia. Many Turkic tribes—including the Oghuz Turks, who were the ancestors of both the Seljuks and the Ottomans—gradually converted to Islam and brought religion to Anatolia through their migrations beginning in the 11th century. From its founding, the Ottoman Empire officially supported the Maturidi school of Islamic theology, which emphasized human reason, rationality, the pursuit of science and philosophy (falsafa). The Ottomans were among the earliest and most enthusiastic adopters of the Hanafi school of Islamic jurisprudence, which was comparatively more flexible and discretionary in its rulings.\n\nThe Ottoman Empire had a wide variety of Islamic sects, including Druze, Ismailis, Alevis, and Alawites. Sufism, a diverse body of Islamic mysticism, found fertile ground in Ottoman lands; many Sufi religious orders (tariqa), such as the Bektashi and Mevlevi, were either established, or saw significant growth, throughout the empire's history. However, some heterodox Muslim groups were viewed as heretical and even ranked below Jews and Christians in terms of legal protection; Druze were frequent targets of persecution, with Ottoman authorities often citing the controversial rulings of Ibn Taymiyya, a member of the conservative Hanbali school. In 1514, Sultan Selim I ordered the massacre of 40,000 Anatolian Alevis (Qizilbash), whom he considered a fifth column for the rival Safavid Empire.\nDuring Selim's reign, the Ottoman Empire saw an unprecedented and rapid expansion into the Middle East, particularly the conquest of the entire Mamluk Sultanate of Egypt on the early 16th century. These conquests further solidified the Ottoman claim of being an Islamic caliphate, although Ottoman sultans had been claiming the title of caliph since the reign of Murad I (1362–1389). The caliphate was officially transferred from the Mamluks to the Ottoman sultanate in 1517, whose members would be recognized as caliphs until the office's abolition on 3 March 1924 by the Republic of Turkey (and the exile of the last caliph, Abdülmecid II, to France).\n\n\n==== Christianity and Judaism ====\n\nIn accordance with the Muslim dhimmi system, the Ottoman Empire guaranteed limited freedoms to Christians, Jews, and other \"people of the book\", such as the right to worship, own property, and be exempt from the obligatory alms (zakat) required of Muslims. However, non-Muslims (or dhimmi) were subject to various legal restrictions, including being forbidden to carry weapons, ride on horseback, or have their homes overlook those of Muslims; likewise, they were required to pay higher taxes than Muslim subjects, including the jizya, which was a key source of state revenue. Many Christians and Jews converted to Islam to secure full social and legal status, though most continued to practice their faith without restriction.The Ottomans developed a unique sociopolitical system known as the millet, which granted non-Muslim communities a large degree of political, legal, and religious autonomy; in essence, members of a millet were subjects of the empire but not subject to the Muslim faith or Islamic law. A millet could govern its own affairs, such as raising taxes and resolving internal legal disputes, with little or no interference from Ottoman authorities, so long as its members were loyal to the sultan and adhered to the rules concerning dhimmi. A quintessential example is the ancient Orthodox community of Mount Athos, which was permitted to retain its autonomy and was never subject to occupation or forced conversion; even special laws were enacted to protect it from outsiders.The Rum Millet, which encompassed most Eastern Orthodox Christians, was governed by the Byzantine-era Corpus Juris Civilis (Code of Justinian), with the Ecumenical Patriarch designated the highest religious and political authority (millet-bashi, or ethnarch). Likewise, Ottoman Jews came under the authority of the Haham Başı, or Ottoman Chief Rabbi, while Armenians were under the authority of the chief bishop of the Armenian Apostolic Church. As the largest group of non-Muslim subjects, the Rum Millet enjoyed several special privileges in politics and commerce; however, Jews and Armenians were also well represented among the wealthy merchant class, as well as in public administration.Some modern scholars consider the millet system to be an early example of religious pluralism, as it accorded minority religious groups official recognition and tolerance.\n\n\n=== Social-political-religious structure ===\n\nBeginning in the early 19th century, society, government, and religion were interrelated in a complex, overlapping way that was deemed inefficient by Atatürk, who systematically dismantled it after 1922. In Constantinople, the Sultan ruled two distinct domains: the secular government and the religious hierarchy. Religious officials formed the Ulama, who had control of religious teachings and theology, and also the Empire's judicial system, giving them a major voice in day-to-day affairs in communities across the Empire (but not including the non-Muslim millets). They were powerful enough to reject the military reforms proposed by Sultan Selim III. His successor Sultan Mahmud II (r. 1808–1839) first won ulama approval before proposing similar reforms. The secularisation program brought by Atatürk ended the ulema and their institutions. The caliphate was abolished, madrasas were closed down, and the sharia courts were abolished. He replaced the Arabic alphabet with Latin letters, ended the religious school system, and gave women some political rights. Many rural traditionalists never accepted this secularisation, and by the 1990s they were reasserting a demand for a larger role for Islam.\n\nThe Janissaries were a highly formidable military unit in the early years, but as Western Europe modernized its military organization technology, the Janissaries became a reactionary force that resisted all change. Steadily the Ottoman military power became outdated, but when the Janissaries felt their privileges were being threatened, or outsiders wanted to modernize them, or they might be superseded by the cavalrymen, they rose in rebellion. The rebellions were highly violent on both sides, but by the time the Janissaries were suppressed, it was far too late for Ottoman military power to catch up with the West. The political system was transformed by the destruction of the Janissaries, a very powerful military/governmental/police force, which revolted in the Auspicious Incident of 1826. Sultan Mahmud II crushed the revolt executed the leaders and disbanded the large organization. That set the stage for a slow process of modernization of government functions, as the government sought, with mixed success, to adopt the main elements of Western bureaucracy and military technology.\nThe Janissaries had been recruited from Christians and other minorities; their abolition enabled the emergence of a Turkish elite to control the Ottoman Empire. The problem was that the Turkish element was very poorly educated, lacking higher schools of any sort, and locked into the Turkish language that used the Arabic alphabet that inhibited wider learning. A large number of ethnic and religious minorities were tolerated in their own separate segregated domains called millets. They were primarily Greek, Armenian, or Jewish. In each locality, they governed themselves, spoke their own language, ran their own schools, cultural and religious institutions, and paid somewhat higher taxes. They had no power outside the millet. The Imperial government protected them and prevented major violent clashes between ethnic groups. However, the millets showed very little loyalty to the Empire. Ethnic nationalism, based on distinctive religion and language, provided a centripetal force that eventually destroyed the Ottoman Empire. In addition, Muslim ethnic groups, which were not part of the millet system, especially the Arabs and the Kurds, were outside the Turkish culture and developed their own separate nationalism. The British sponsored Arab nationalism in the First World War, promising an independent Arab state in return for Arab support. Most Arabs supported the Sultan, but those near Mecca believed in and supported the British promise.\n\nAt the local level, power was held beyond the control of the Sultan by the ayans or local notables. The ayan collected taxes, formed local armies to compete with other notables, took a reactionary attitude toward political or economic change, and often defied policies handed down by the Sultan.The economic system made little progress. Printing was forbidden until the 18th century, for fear of defiling the secret documents of Islam. The millets, however, were allowed their own presses, using Greek, Hebrew, Armenian and other languages that greatly facilitated nationalism. The religious prohibition on charging interest foreclosed most of the entrepreneurial skills among Muslims, although it did flourish among the Jews and Christians.\nAfter the 18th century, the Ottoman Empire was clearly shrinking, as Russia put on heavy pressure and expanded to its south; Egypt became effectively independent in 1805, and the British later took it over, along with Cyprus. Greece became independent, and Serbia and other Balkan areas became highly restive as the force of nationalism pushed against imperialism. The French took over Algeria and Tunisia. The Europeans all thought that the empire was a sick man in rapid decline. Only the Germans seemed helpful, and their support led to the Ottoman Empire joining the central powers in 1915, with the result that they came out as one of the heaviest losers of the First World War in 1918.\n\n\n== Culture ==\n\nThe Ottomans absorbed some of the traditions, art, and institutions of cultures in the regions they conquered and added new dimensions to them. Numerous traditions and cultural traits of previous empires (In fields such as architecture, cuisine, music, leisure, and government) were adopted by the Ottoman Turks, who developed them into new forms, resulting in a new and distinctively Ottoman cultural identity. Although the predominant literary language of the Ottoman Empire was Turkish, Persian was the preferred vehicle for the projection of an imperial image.Slavery was a part of Ottoman society, with most slaves employed as domestic servants. Agricultural slavery, such as that which was widespread in the Americas, was relatively rare. Unlike systems of chattel slavery, slaves under Islamic law were not regarded as movable property, and the children of female slaves were born legally free. Female slaves were still sold in the Empire as late as 1908. During the 19th century the Empire came under pressure from Western European countries to outlaw the practice. Policies developed by various sultans throughout the 19th century attempted to curtail the Ottoman slave trade but slavery had centuries of religious backing and sanction and so slavery was never abolished in the Empire.Plague remained a major scourge in Ottoman society until the second quarter of the 19th century. \"Between 1701 and 1750, 37 larger and smaller plague epidemics were recorded in Istanbul, and 31 between 1751 and 1801.\"Ottomans adopted Persian bureaucratic traditions and culture. The sultans also made an important contribution in the development of Persian literature.\n\n\n=== Education ===\n\nIn the Ottoman Empire, each millet established a schooling system serving its members. Education, therefore, was largely divided on ethnic and religious lines: few non-Muslims attended schools for Muslim students and vice versa. Most institutions that did serve all ethnic and religious groups taught in French or other languages.Several \"foreign schools\" (Frerler mektebleri) operated by religious clergy primarily served Christians, although some Muslim students attended. Garnett described the schools for Christians and Jews as \"organised upon European models\", with \"voluntary contributions\" supporting their operation and most of them being \"well attended\" and with \"a high standard of education\".\n\n\n=== Literature ===\n\nThe two primary streams of Ottoman written literature are poetry and prose. Poetry was by far the dominant stream. Until the 19th century, Ottoman prose did not contain any examples of fiction: there were no counterparts to, for instance, the European romance, short story, or novel. Analog genres did exist, though, in both Turkish folk literature and in Divan poetry.\nOttoman Divan poetry was a highly ritualized and symbolic art form. From the Persian poetry that largely inspired it, it inherited a wealth of symbols whose meanings and interrelationships—both of similitude (مراعات نظير mura'ât-i nazîr / تناسب tenâsüb) and opposition (تضاد tezâd) were more or less prescribed. Divan poetry was composed through the constant juxtaposition of many such images within a strict metrical framework, thus allowing numerous potential meanings to emerge. The vast majority of Divan poetry was lyric in nature: either gazels (which make up the greatest part of the repertoire of the tradition), or kasîdes. There were, however, other common genres, most particularly the mesnevî, a kind of verse romance and thus a variety of narrative poetry; the two most notable examples of this form are the Leyli and Majnun of Fuzûlî and the Hüsn ü Aşk of Şeyh Gâlib. The Seyahatnâme of Evliya Çelebi (1611–1682) is an outstanding example of travel literature.\n\nUntil the 19th century, Ottoman prose did not develop to the extent that contemporary Divan poetry did. A large part of the reason for this was that much prose was expected to adhere to the rules of sec (سجع, also transliterated as seci), or rhymed prose, a type of writing descended from the Arabic saj' and which prescribed that between each adjective and noun in a string of words, such as a sentence, there must be a rhyme. Nevertheless, there was a tradition of prose in the literature of the time, though exclusively non-fictional in nature. One apparent exception was Muhayyelât (\"Fancies\") by Giritli Ali Aziz Efendi, a collection of stories of the fantastic written in 1796, though not published until 1867. The first novel published in the Ottoman Empire was by an Armenian named Vartan Pasha. Published in 1851, the novel was entitled The Story of Akabi (Turkish: Akabi Hikyayesi) and was written in Turkish but with Armenian script.Due to historically close ties with France, French literature came to constitute the major Western influence on Ottoman literature throughout the latter half of the 19th century. As a result, many of the same movements prevalent in France during this period also had their Ottoman equivalents; in the developing Ottoman prose tradition, for instance, the influence of Romanticism can be seen during the Tanzimat period, and that of the Realist and Naturalist movements in subsequent periods; in the poetic tradition, on the other hand, it was the influence of the Symbolist and Parnassian movements that became paramount.\nMany of the writers in the Tanzimat period wrote in several different genres simultaneously; for instance, the poet Namık Kemal also wrote the important 1876 novel İntibâh (\"Awakening\"), while the journalist İbrahim Şinasi is noted for writing, in 1860, the first modern Turkish play, the one-act comedy \"Şair Evlenmesi\" (\"The Poet's Marriage\"). An earlier play, a farce entitled \"Vakâyi'-i 'Acibe ve Havâdis-i Garibe-yi Kefşger Ahmed\" (\"The Strange Events and Bizarre Occurrences of the Cobbler Ahmed\"), dates from the beginning of the 19th century, but there remains some doubt about its authenticity. In a similar vein, the novelist Ahmed Midhat Efendi wrote important novels in each of the major movements: Romanticism (Hasan Mellâh yâhud Sırr İçinde Esrâr, 1873; \"Hasan the Sailor, or The Mystery Within the Mystery\"), Realism (Henüz on Yedi Yaşında, 1881; \"Just Seventeen Years Old\"), and Naturalism (Müşâhedât, 1891; \"Observations\"). This diversity was, in part, due to the Tanzimat writers' wish to disseminate as much of the new literature as possible, in the hopes that it would contribute to a revitalization of Ottoman social structures.\n\n\n=== Media ===\n\nThe media of the Ottoman Empire was diverse, with newspapers and journals published in various languages including French, Greek, and German. Many of these publications were centered in Constantinople, but there were also French-language newspapers produced in Beirut, Salonika, and Smyrna. Non-Muslim ethnic minorities in the empire used French as a lingua franca and used French-language publications, while some provincial newspapers were published in Arabic. The use of French in the media persisted until the end of the empire in 1923 and for a few years thereafter in the Republic of Turkey.\n\n\n=== Architecture ===\n\nThe architecture of the empire developed from earlier Seljuk Turkish architecture, with influences from Byzantine and Iranian architecture and other architectural traditions in the Middle East. Early Ottoman architecture experimented with multiple building types over the course of the 13th to 15th centuries, progressively evolving into the Classical Ottoman style of the 16th and 17th centuries, which was also strongly influenced by the Hagia Sophia. The most important architect of the Classical period is Mimar Sinan, whose major works include the Şehzade Mosque, Süleymaniye Mosque, and Selimiye Mosque. The greatest of the court artists enriched the Ottoman Empire with many pluralistic artistic influences, such as mixing traditional Byzantine art with elements of Chinese art. The second half of the 16th century also saw the apogee of certain decorative arts, most notably in the use of Iznik tiles.Beginning in the 18th century, Ottoman architecture was influenced by the Baroque architecture in Western Europe, resulting in the Ottoman Baroque style. Nuruosmaniye Mosque is one of the most important examples from this period. The last Ottoman period saw more influences from Western Europe, brought in by architects such as those from the Balyan family. Empire style and Neoclassical motifs were introduced and a trend towards eclecticism was evident in many types of buildings, such as the Dolmabaçe Palace. The last decades of the Ottoman Empire also saw the development of a new architectural style called neo-Ottoman or Ottoman revivalism, also known as the First National Architectural Movement, by architects such as Mimar Kemaleddin and Vedat Tek.Ottoman dynastic patronage was concentrated in the historic capitals of Bursa, Edirne, and Istanbul (Constantinople), as well as in several other important administrative centers such as Amasya and Manisa. It was in these centers that most important developments in Ottoman architecture occurred and that the most monumental Ottoman architecture can be found. Major religious monuments were typically architectural complexes, known as a külliye, that had multiple components providing different services or amenities. In addition to a mosque, these could include a madrasa, a hammam, an imaret, a sebil, a market, a caravanserai, a primary school, or others. These complexes were governed and managed with the help of a vakif agreement (Arabic waqf). Ottoman constructions were still abundant in Anatolia and in the Balkans (Rumelia), but in the more distant Middle Eastern and North African provinces older Islamic architectural styles continued to hold strong influence and were sometimes blended with Ottoman styles.\n\n\n=== Decorative arts ===\n\nThe tradition of Ottoman miniatures, painted to illustrate manuscripts or used in dedicated albums, was heavily influenced by the Persian art form, though it also included elements of the Byzantine tradition of illumination and painting. A Greek academy of painters, the Nakkashane-i-Rum, was established in the Topkapi Palace in the 15th century, while early in the following century a similar Persian academy, the Nakkashane-i-Irani, was added. Surname-i Hümayun (Imperial Festival Books) were albums that commemorated celebrations in the Ottoman Empire in pictorial and textual detail.\nOttoman illumination covers non-figurative painted or drawn decorative art in books or on sheets in muraqqa or albums, as opposed to the figurative images of the Ottoman miniature. It was a part of the Ottoman Book Arts together with the Ottoman miniature (taswir), calligraphy (hat), Islamic calligraphy, bookbinding (cilt) and paper marbling (ebru). In the Ottoman Empire, illuminated and illustrated manuscripts were commissioned by the Sultan or the administrators of the court. In Topkapi Palace, these manuscripts were created by the artists working in Nakkashane, the atelier of the miniature and illumination artists. Both religious and non-religious books could be illuminated. Also, sheets for albums levha consisted of illuminated calligraphy (hat) of tughra, religious texts, verses from poems or proverbs, and purely decorative drawings.\nThe art of carpet weaving was particularly significant in the Ottoman Empire, carpets having an immense importance both as decorative furnishings, rich in religious and other symbolism and as a practical consideration, as it was customary to remove one's shoes in living quarters. The weaving of such carpets originated in the nomadic cultures of central Asia (carpets being an easily transportable form of furnishing), and eventually spread to the settled societies of Anatolia. Turks used carpets, rugs, and kilims not just on the floors of a room but also as a hanging on walls and doorways, where they provided additional insulation. They were also commonly donated to mosques, which often amassed large collections of them.\n\n\n=== Music and performing arts ===\n\nOttoman classical music was an important part of the education of the Ottoman elite. A number of the Ottoman sultans have accomplished musicians and composers themselves, such as Selim III, whose compositions are often still performed today. Ottoman classical music arose largely from a confluence of Byzantine music, Armenian music, Arabic music, and Persian music. Compositionally, it is organized around rhythmic units called usul, which are somewhat similar to meter in Western music, and melodic units called makam, which bear some resemblance to Western musical modes.\nThe instruments used are a mixture of Anatolian and Central Asian instruments (the saz, the bağlama, the kemence), other Middle Eastern instruments (the ud, the tanbur, the kanun, the ney), and—later in the tradition—Western instruments (the violin and the piano). Because of a geographic and cultural divide between the capital and other areas, two broadly distinct styles of music arose in the Ottoman Empire: Ottoman classical music and folk music. In the provinces, several different kinds of folk music were created. The most dominant regions with their distinguished musical styles are Balkan-Thracian Türküs, North-Eastern (Laz) Türküs, Aegean Türküs, Central Anatolian Türküs, Eastern Anatolian Türküs, and Caucasian Türküs. Some of the distinctive styles were: Janissary music, Roma music, Belly dance, Turkish folk music.\nThe traditional shadow play called Karagöz and Hacivat was widespread throughout the Ottoman Empire and featured characters representing all of the major ethnic and social groups in that culture. It was performed by a single puppet master, who voiced all of the characters, and accompanied by tambourine (def). Its origins are obscure, deriving perhaps from an older Egyptian tradition, or possibly from an Asian source.\n\n\t\t\n\t\t\n\t\t\n\t\t\n\n\n=== Cuisine ===\n\nOttoman cuisine is the cuisine of the capital, Constantinople (Istanbul), and the regional capital cities, where the melting pot of cultures created a common cuisine that most of the population regardless of ethnicity shared. This diverse cuisine was honed in the Imperial Palace's kitchens by chefs brought from certain parts of the Empire to create and experiment with different ingredients. The creations of the Ottoman Palace's kitchens filtered to the population, for instance through Ramadan events, and through the cooking at the Yalıs of the Pashas, and from there on spread to the rest of the population.\nMuch of the cuisine of former Ottoman territories today is descended from a shared Ottoman cuisine, especially Turkish, and including Greek, Balkan, Armenian, and Middle Eastern cuisines. Many common dishes in the region, descendants of the once-common Ottoman cuisine, include yogurt, döner kebab/gyro/shawarma, cacık/tzatziki, ayran, pita bread, feta cheese, baklava, lahmacun, moussaka, yuvarlak, köfte/keftés/kofta, börek/boureki, rakı/rakia/tsipouro/tsikoudia, meze, dolma, sarma, rice pilaf, Turkish coffee, sujuk, kashk, keşkek, manti, lavash, kanafeh, and more.\n\n\n=== Sports ===\n\nThe main sports Ottomans were engaged in were Turkish wrestling, hunting, Turkish archery, horseback riding, equestrian javelin throw, arm wrestling, and swimming. European model sports clubs were formed with the spreading popularity of football matches in 19th century Constantinople. The leading clubs, according to timeline, were Beşiktaş Gymnastics Club (1903), Galatasaray Sports Club (1905), Fenerbahçe Sports Club (1907), MKE Ankaragücü (formerly Turan Sanatkaragücü) (1910) in Constantinople. Football clubs were formed in other provinces too, such as Karşıyaka Sports Club (1912), Altay Sports Club (1914) and Turkish Fatherland Football Club (later Ülküspor) (1914) of İzmir.\n\n\n== Science and technology ==\n\nOver the course of Ottoman history, the Ottomans managed to build a large collection of libraries complete with translations of books from other cultures, as well as original manuscripts. A great part of this desire for local and foreign manuscripts arose in the 15th century. Sultan Mehmet II ordered Georgios Amiroutzes, a Greek scholar from Trabzon, to translate and make available to Ottoman educational institutions the geography book of Ptolemy. Another example is Ali Qushji – an astronomer, mathematician and physicist originally from Samarkand – who became a professor in two madrasas and influenced Ottoman circles as a result of his writings and the activities of his students, even though he only spent two or three years in Constantinople before his death.\nTaqi al-Din built the Constantinople observatory of Taqi al-Din in 1577, where he carried out observations until 1580. He calculated the eccentricity of the Sun's orbit and the annual motion of the apogee. However, the observatory's primary purpose was almost certainly astrological rather than astronomical, leading to its destruction in 1580 due to the rise of a clerical faction that opposed its use for that purpose. He also experimented with steam power in Ottoman Egypt in 1551, when he described a steam jack driven by a rudimentary steam turbine.\nIn 1660 the Ottoman scholar Ibrahim Efendi al-Zigetvari Tezkireci translated Noël Duret's French astronomical work (written in 1637) into Arabic.Şerafeddin Sabuncuoğlu was the author of the first surgical atlas and the last major medical encyclopaedia from the Islamic world. Though his work was largely based on Abu al-Qasim al-Zahrawi's Al-Tasrif, Sabuncuoğlu introduced many innovations of his own. Female surgeons were also illustrated for the first time. Since, the Ottoman Empire is credited with the invention of several surgical instruments in use such as forceps, catheters, scalpels and lancets as well as pincers.An example of a watch that measured time in minutes was created by an Ottoman watchmaker, Meshur Sheyh Dede, in 1702.In the early 19th century, Egypt under Muhammad Ali began using steam engines for industrial manufacturing, with industries such as ironworks, textile manufacturing, paper mills and hulling mills moving towards steam power. Economic historian Jean Batou argues that the necessary economic conditions existed in Egypt for the adoption of oil as a potential energy source for its steam engines later in the 19th century.In the 19th century, Ishak Efendi is credited with introducing the then current Western scientific ideas and developments to the Ottoman and wider Muslim world, as well as the invention of a suitable Turkish and Arabic scientific terminology, through his translations of Western works.\n\n\n== See also ==\nTurkic History\nOutline of the Ottoman Empire\nBibliography of the Ottoman Empire\nGunpowder empires\nHistoriography of the fall of the Ottoman Empire\nIndex of Ottoman Empire-related articles\nList of battles involving the Ottoman Empire\nList of Ottoman conquests, sieges and landings\nList of Turkic dynasties and countries\nList of wars involving the Ottoman Empire\nOttoman wars in Europe\nThe Inspection Board of Finance of Turkey (1879)\n16 Great Turkic Empires\n\n\n== References ==\n\n\n=== Footnotes ===\n\n\n=== Citations ===\n\n\n=== Sources ===\n\n\n== Further reading ==\n\n\n=== General surveys ===\n\n\n== External links ==\n\nOttoman Text Archive Project – University of Washington\nSearchable Ottoman texts – Wikilala\nAmerican Travelers to the Holy Land in the 19th Century Archived 23 May 2013 at the Wayback Machine Shapell Manuscript Foundation\nThe Ottoman Empire: Resources – University of Michigan\nTurkey in Asia, 1920"
    }
}